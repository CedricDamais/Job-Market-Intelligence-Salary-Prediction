{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d74624",
   "metadata": {},
   "source": [
    "# Generation of job descriptions with tranformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bb751",
   "metadata": {},
   "source": [
    "### Checking for GPU availability to run the computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dd86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/.cache/pypoetry/virtualenvs/nlp-linkedin-offers-VrYHBMh4-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-10 01:51:28.357835: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-10 01:51:28.563067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746834688.635543  210453 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746834688.656145  210453 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746834688.828193  210453 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746834688.828216  210453 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746834688.828217  210453 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746834688.828219  210453 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-10 01:51:28.847351: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3cbbf",
   "metadata": {},
   "source": [
    "### Setting root path to project path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ded5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(\n",
    "    os.path.join(os.getcwd(), '../../../..')\n",
    ")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "LOAD_FINE_TUNED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e108844",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f2ca506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a733769",
   "metadata": {},
   "source": [
    "### Load fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9240fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_path = os.path.join(project_root, 'models', 'src', 'generation', 'transformers', 'fine_tuned_model_gpt2')\n",
    "if not os.path.exists(fine_tuned_model_path):\n",
    "    raise FileNotFoundError(f\"Model directory not found: {fine_tuned_model_path}. Please ensure the path is correct.\")\n",
    "\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d0f11",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5f5b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: gpt2\n",
      "Tokenizer pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"gpt2\"  # Or choose another model like \"gpt2\", \"t5-small\", etc.\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token if it doesn't exist (common for GPT-2 models)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "print(f\"Tokenizer pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754baadb",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971d3483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame from /home/gabriel/dev/SCIA/NLP_Linkedin_offers/data/processed/cleaned_postings_modeling.parquet. Shape: (122124, 4)\n",
      "                company_name  \\\n",
      "0      Corcoran Sawyer Smith   \n",
      "1     The National Exemplar    \n",
      "2     Abrams Fensterman, LLP   \n",
      "3  Downtown Raleigh Alliance   \n",
      "4                 Raw Cereal   \n",
      "\n",
      "                                               title  \\\n",
      "0                              Marketing Coordinator   \n",
      "1                        Assitant Restaurant Manager   \n",
      "2  Senior Elder Law / Trusts and Estates Associat...   \n",
      "3           Economic Development and Planning Intern   \n",
      "4                                           Producer   \n",
      "\n",
      "                                         description           location  \n",
      "0  Job description A leading real estate firm in ...      Princeton, NJ  \n",
      "1  The National Exemplar is accepting application...     Cincinnati, OH  \n",
      "2  Senior Associate Attorney Elder Law Trusts and...  New Hyde Park, NY  \n",
      "3  Job summary The Economic Development Planning ...        Raleigh, NC  \n",
      "4  Company Description Raw Cereal is a creative d...      United States  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 122124 entries, 0 to 122123\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   company_name  122124 non-null  object\n",
      " 1   title         122124 non-null  object\n",
      " 2   description   122124 non-null  object\n",
      " 3   location      122124 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.7+ MB\n",
      "None\n",
      "Filtered DataFrame. New shape: (122055, 4)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 122055 entries, 0 to 122123\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   company_name  122055 non-null  object\n",
      " 1   title         122055 non-null  object\n",
      " 2   description   122055 non-null  object\n",
      " 3   location      122055 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 4.7+ MB\n",
      "None\n",
      "\n",
      "Converted DataFrame to Dataset. Size: 122055\n",
      "\n",
      "Split dataset into training and testing sets:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['company_name', 'title', 'description', 'location', '__index_level_0__'],\n",
      "        num_rows: 115952\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['company_name', 'title', 'description', 'location', '__index_level_0__'],\n",
      "        num_rows: 6103\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115952/115952 [00:50<00:00, 2277.76 examples/s]\n",
      "Map: 100%|██████████| 6103/6103 [00:02<00:00, 2492.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 115952\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 6103\n",
      "    })\n",
      "})\n",
      "\n",
      "Tokenized training dataset sample:\n",
      "{'input_ids': tensor([   36,  1731,   486,  ...,  4645,  1085, 22901]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]), 'labels': tensor([   36,  1731,   486,  ...,  4645,  1085, 22901])}\n",
      "\n",
      "Setting train_dataset to None to skip training and only perform evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "parquet_file_path = os.path.join(project_root, \"data\", \"processed\", \"cleaned_postings_modeling.parquet\")\n",
    "text_column = \"description\" # Your column name\n",
    "block_size = 1024 # Max sequence length for the model\n",
    "test_size = 0.05 \n",
    "random_seed = 42\n",
    "\n",
    "model = fine_tuned_model\n",
    "tokenizer = fine_tuned_tokenizer\n",
    "\n",
    "# --- Load Dataframe from Parquet ---\n",
    "try:\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "    print(f\"Loaded DataFrame from {parquet_file_path}. Shape: {df.shape}\")\n",
    "    print(df.head()) # Optional\n",
    "    print(df.info()) # Optional\n",
    "    # filter out any rows with a description shorter than 100 characters\n",
    "    df = df[df[text_column].str.len() > 100]\n",
    "    print(f\"Filtered DataFrame. New shape: {df.shape}\")\n",
    "    print (df.info())\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in the DataFrame.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Parquet file not found at {parquet_file_path}\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Parquet file: {e}\")\n",
    "    df = None\n",
    "\n",
    "# --- Convert Pandas DataFrame to Hugging Face Dataset ---\n",
    "if df is not None:\n",
    "    # Convert the DataFrame to a single Dataset object first\n",
    "    full_dataset = Dataset.from_pandas(df)\n",
    "    print(f\"\\nConverted DataFrame to Dataset. Size: {len(full_dataset)}\")\n",
    "\n",
    "    # --- Split the Dataset ---\n",
    "    # Use train_test_split on the Dataset object\n",
    "    split_datasets = full_dataset.train_test_split(test_size=test_size, seed=random_seed)\n",
    "\n",
    "    # Rename the default 'test' split to 'validation' if preferred for Trainer, or keep as 'test'\n",
    "    # Trainer uses 'eval_dataset', so 'validation' or 'test' are common keys. Let's use 'test'.\n",
    "    # split_datasets['validation'] = split_datasets.pop('test') # Optional rename\n",
    "\n",
    "    print(\"\\nSplit dataset into training and testing sets:\")\n",
    "    print(split_datasets)\n",
    "\n",
    "    # Assign to raw_datasets (which is now a DatasetDict with 'train' and 'test')\n",
    "    raw_datasets = split_datasets\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping dataset conversion and splitting due to loading error.\")\n",
    "    raw_datasets = None\n",
    "\n",
    "# --- Tokenization Function (remains the same) ---\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples[text_column], truncation=True, padding=\"max_length\", max_length=block_size)\n",
    "    # For Causal LM, labels are usually the same as inputs\n",
    "    tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"].copy()\n",
    "    return tokenized_output\n",
    "\n",
    "# --- Apply Tokenization ---\n",
    "if raw_datasets:\n",
    "    # Apply tokenization to both splits ('train' and 'test')\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names # Remove original columns from both splits\n",
    "    )\n",
    "    # Set format for PyTorch\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "    print(\"\\nTokenized dataset structure:\")\n",
    "    print(tokenized_datasets)\n",
    "\n",
    "    print(\"\\nTokenized training dataset sample:\")\n",
    "    if len(tokenized_datasets[\"train\"]) > 0:\n",
    "         print(tokenized_datasets[\"train\"][0])\n",
    "    else:\n",
    "        print(\"Tokenized training dataset is empty.\")\n",
    "\n",
    "    # Assign the splits\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"test\"] # Use the 'test' split for evaluation\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping tokenization due to dataset loading/conversion error.\")\n",
    "    train_dataset = None\n",
    "    eval_dataset = None\n",
    "\n",
    "if (LOAD_FINE_TUNED):\n",
    "    print(\"\\nSetting train_dataset to None to skip training and only perform evaluation.\")\n",
    "    train_dataset = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c10c90",
   "metadata": {},
   "source": [
    "### Setup for the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e00aaa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210453/3807418757.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments and Trainer initialized.\n",
      "Evaluation dataset size: 6103\n",
      "FP16 enabled: True\n",
      "Evaluation strategy: IntervalStrategy.STEPS\n"
     ]
    }
   ],
   "source": [
    "output_directory = os.path.join(project_root, \"models\", \"src\", \"generation\", \"transformers\")\n",
    "use_fp16 = torch.cuda.is_available() # Enable FP16 only if GPU is available\n",
    "\n",
    "if (LOAD_FINE_TUNED):\n",
    "    model = fine_tuned_model\n",
    "    tokenizer = fine_tuned_tokenizer\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"inductor\",\n",
    "    torch_compile_mode=\"default\", \n",
    "    torch_empty_cache_steps=4,\n",
    "    output_dir=output_directory,\n",
    "    num_train_epochs=1,  # Start with 1 epoch for testing\n",
    "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=4, # Batch size for evaluation (can often be larger)\n",
    "    gradient_accumulation_steps=4, # Increase effective batch size\n",
    "    learning_rate=5e-5, # Learning rate\n",
    "    fp16=use_fp16, # Enable mixed precision training if GPU available\n",
    "    logging_dir=f\"{output_directory}/logs\",\n",
    "    logging_strategy=\"steps\", # Log metrics periodically\n",
    "    logging_steps=100,        # Log every 100 steps\n",
    "    eval_strategy=\"steps\", # Evaluate periodically\n",
    "    eval_steps=500,              # Evaluate every 500 steps\n",
    "    save_strategy=\"steps\",       # Save checkpoints periodically\n",
    "    save_steps=500,              # Save every 500 steps\n",
    "    load_best_model_at_end=True, # Load the best model found during evaluation at the end\n",
    "    metric_for_best_model=\"loss\", # Use evaluation loss to determine the best model (lower is better)\n",
    "    greater_is_better=False,     # Lower loss is better\n",
    "    save_total_limit=1,          # Keep only the last 2 checkpoints + the best one\n",
    "    report_to=\"none\",          # Disable external reporting (like wandb) for now\n",
    "    weight_decay=0.01,         # Regularization\n",
    "    dataloader_pin_memory=True, # Pin memory for faster data transfer to GPU\n",
    "    dataloader_num_workers=12, # Number of workers for data loading, leverage multiple CPU cores for faster data loading to GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if train_dataset else None,\n",
    "    eval_dataset=eval_dataset if eval_dataset else None, # Pass the evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    # data_collator=data_collator # Usually not needed for CausalLM unless custom padding\n",
    ")\n",
    "\n",
    "print(\"TrainingArguments and Trainer initialized.\")\n",
    "if eval_dataset:\n",
    "    print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "print(f\"FP16 enabled: {use_fp16}\")\n",
    "print(f\"Evaluation strategy: {training_args.eval_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443f1b9",
   "metadata": {},
   "source": [
    "### Fine-tuning the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b46457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14494' max='14494' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14494/14494 3:10:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.158700</td>\n",
       "      <td>2.055996</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.179600</td>\n",
       "      <td>2.050528</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.129400</td>\n",
       "      <td>2.046983</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.044600</td>\n",
       "      <td>2.044590</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.176400</td>\n",
       "      <td>2.042842</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.127000</td>\n",
       "      <td>2.041235</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.089900</td>\n",
       "      <td>2.039516</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.114300</td>\n",
       "      <td>2.037973</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.123200</td>\n",
       "      <td>2.036798</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.093000</td>\n",
       "      <td>2.035658</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.129700</td>\n",
       "      <td>2.034386</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.146000</td>\n",
       "      <td>2.033240</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.136700</td>\n",
       "      <td>2.032328</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.099900</td>\n",
       "      <td>2.031451</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.155300</td>\n",
       "      <td>2.030328</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.075500</td>\n",
       "      <td>2.029330</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.149500</td>\n",
       "      <td>2.028664</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.082200</td>\n",
       "      <td>2.027956</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.159100</td>\n",
       "      <td>2.027033</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.134300</td>\n",
       "      <td>2.026230</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.146400</td>\n",
       "      <td>2.025720</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.090600</td>\n",
       "      <td>2.025155</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.099000</td>\n",
       "      <td>2.024573</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.144700</td>\n",
       "      <td>2.023586</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.091800</td>\n",
       "      <td>2.022485</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.035100</td>\n",
       "      <td>2.021432</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.083900</td>\n",
       "      <td>2.020565</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.087600</td>\n",
       "      <td>2.020196</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0508 11:43:40.051000 277504 torch/_inductor/utils.py:1250] [2/1_1] Not enough SMs to use max_autotune_gemm mode\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Train Output Metrics: {'train_runtime': 11423.8779, 'train_samples_per_second': 10.15, 'train_steps_per_second': 1.269, 'total_flos': 6.0965702074368e+16, 'train_loss': 2.123645397684764, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               = 56778734GF\n",
      "  train_loss               =     2.1236\n",
      "  train_runtime            = 3:10:23.87\n",
      "  train_samples_per_second =      10.15\n",
      "  train_steps_per_second   =      1.269\n"
     ]
    }
   ],
   "source": [
    "if train_dataset:\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"Training finished.\")\n",
    "        # You can print some metrics from train_result if needed\n",
    "        metrics = train_result.metrics\n",
    "        print(f\"Train Output Metrics: {metrics}\")\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "else:\n",
    "    print(\"Skipping training because the dataset was not loaded properly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c403105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add4cd8",
   "metadata": {},
   "source": [
    "### Perform Inference with Pipeline and store predictions for test dataset\n",
    "Using the fine-tuned model to generate text samples based on prompts from the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32550188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing text-generation pipeline with model: /home/gabriel/dev/SCIA/NLP_Linkedin_offers/models/src/generation/transformers/fine_tuned_model_gpt2\n",
      "Pipeline initialized.\n",
      "\n",
      "--- Sample 1 ---\n",
      "Prompt: \"As a Publix Pharmacy Associate, you ll see how very satisfying it is to work for a company that is passionately devoted to its customers, to its associates, and to the wellness of the communities it serves. What sets our pharmacy departments apart is a genuine patient centric environment with...\"\n",
      "Generated Text: \"As a Publix Pharmacy Associate, you ll see how very satisfying it is to work for a company that is passionately devoted to its customers, to its associates, and to the wellness of the communities it serves. What sets our pharmacy departments apart is a genuine patient centric environment with a wide variety of products and experiences available, as well as an emphasis on quality control and patient satisfaction, and that ensures that everyone feels safe and well. We are committed to ensuring our patients are safe and well, and our associates are well served. At our company, there is always something you can do to keep them from getting their hands dirty. This includes making sure your associates receive the highest level of care, training, and support in the workplace, ensuring they receive appropriate training, and helping ensure their patients get the best care possible. What you ll do As a Pharmacy Associate, you ll be responsible for performing the following duties. The following activities include performing the following tasks as assigned. Performs the following duties as assigned, including but not limited to: Reasonable accommodation to patients in an emergency room or other hospital. Assist the patient with medication. Receive medication and administer it to patients in an emergency room or other hospital. Assist patients in a routine medication administration program. Assist patients with a medical exam including medical history and medical history records. Assist patients in receiving and storing personal information. Assist patients with a medical history record including medical history records. Assist patients with a computer program. Assist patients with a telemedicine system. Receive patient information as requested by the department, including the date of death, name, phone number or other identifying information. Performs all other duties as assigned as assigned. Receive and maintain accurate personal and business records, including but not limited to phone records, bank and credit card records, medical records and personal health information. Manage and maintain a comprehensive list of all medications and supplies required to perform the duties. Manage the department and the department s equipment to provide quality care. Manage the department s inventory, including inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory and inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory, inventory,\"\n",
      "Appended sample 1 to golden dataset.\n",
      "\n",
      "--- Sample 2 ---\n",
      "Prompt: \"Job Title Release Manager CLT Duration 12 18 months Location Charlotte NC Required Pay Scale 65 HOUR ON W2 Due to client requirements this role is only open to USC or GC candidates Job Summary Project Details Release Manager supporting Commercial Credit Technology and their ongoing releases. Must Haves Must...\"\n",
      "Generated Text: \"Job Title Release Manager CLT Duration 12 18 months Location Charlotte NC Required Pay Scale 65 HOUR ON W2 Due to client requirements this role is only open to USC or GC candidates Job Summary Project Details Release Manager supporting Commercial Credit Technology and their ongoing releases. Must Haves Must have a minimum of 5 years of experience in the commercial banking industry.Must be an active member of an industry based organization, as well as be able to work independently and professionally.Must have strong communication skills, and be able to provide feedback to customers.Experience in developing, testing, and executing commercial software and services.Strong interpersonal skills and ability to work independently.Must be able to communicate effectively with customers and provide constructive and helpful feedback.Excellent analytical and problem solving skills.Knowledge of financial products such as credit monitoring, credit protection, and regulatory compliance.Must have strong interpersonal skills, and be able to work independently and professionally.Ability to work collaboratively as well as with other team members.Must have good communication skills and able to work independently and professionally.Experience in creating and implementing financial software applications for credit monitoring, compliance, compliance, and regulation.Experience working in a large company setting including a large retail store experience.Must also be proficient in managing multiple departments at the same time.Ability to work independently and professionally Strong communication skills.Must have good verbal and written communication skills.Excellent interpersonal skills. Ability to work independently and professionally.Experience with the following industries. Commercial Credit Technology, Credit Protection, or Regulatory Compliance. Banking Banking, Insurance, Financial Services, Retail, Insurance. Finance, Finance, Finance Education, Health, and Other Education Experience with a variety of credit monitoring services or related disciplines such as credit monitoring, credit protection, and regulatory compliance.Experience working with a team of senior management, sales, and business development professionals. Experience with an integrated team of sales, development, and financial advisors. Experience working outside of retail banking.Experience with a flexible schedule. Ability to travel, but not require travel to work in the US. <EOD>\"\n",
      "Appended sample 2 to golden dataset.\n",
      "\n",
      "--- Sample 3 ---\n",
      "Prompt: \"About This Role Wells Fargo is seeking a Senior Lead Business Execution Consultant for our Small Business Card Team in Cards Merchant Services, part of Consumer Lending. This role will be responsible for driving change across the customer lifecycle to improve how we do business and better serve our small...\"\n",
      "Generated Text: \"About This Role Wells Fargo is seeking a Senior Lead Business Execution Consultant for our Small Business Card Team in Cards Merchant Services, part of Consumer Lending. This role will be responsible for driving change across the customer lifecycle to improve how we do business and better serve our small business customers. The candidate will also oversee our internal and external Customer Experience initiatives including customer service, customer interaction, and product support. The candidate is responsible for managing the internal customer experience through onboarding, customer engagement, and strategic initiatives. This position is in a position to deliver and maintain high quality customer engagement with the customer in the digital and physical retail space and within the customer service organization. This role is highly experienced with Customer Experience in Customer Experience in both physical and digital retail. Responsibilities Prepare and oversee client interactions and onboarding processes to improve customer experience and deliver high quality customer experience. Collaborate closely with customer service and other team members to provide guidance on onboarding, customer interaction, and product development. Conduct business in an engaging environment and drive change across the customer lifecycle, including through customer service. Communicate customer feedback with internal and external stakeholders to support the development of our business strategy and deliver high quality customer experience. Conduct and implement product reviews and other initiatives to improve customer experience and drive customer engagement within the Customer Experience organization. Conduct business in an engaging environment and drive change across the Customer Experience organization. Develop and maintain a high quality customer satisfaction program. Demonstrate ability to manage and lead the execution of a team of internal and external Customer Experience professionals. Develop and maintain a high quality customer loyalty program. Demonstrate ability to manage and lead the execution of a team of Customer Experience professionals. Conduct business in an engaging environment and drive change within the Customer Experience organization. Communicate customer feedback with internal and external stakeholders to support the development of our business strategy and deliver high quality customer experience. Conduct business in an engaging environment and drive change across the customer experience organization. Conduct and implement product reviews and other initiatives to improve customer experience and drive customer engagement within the Customer Experience organization. Conduct business in an engaging environment and drive change across the customer experience organization. Conduct or support external project related activities to achieve or exceed customer satisfaction. Demonstrate ability to manage and lead the execution of a companywide team of customer experiences. Demonstrate ability to work independently within an organization. Demonstrate ability to support an internal team in developing high quality customer experiences and deliver high quality customer experiences. Demonstrate ability to perform customer service tasks to achieve or exceed customer satisfaction. Demonstrate ability to manage and lead a team of internal and external Customer Experience professionals. Communicate customer feedback to internal and external stakeholders to support the development of our business strategy and deliver high quality customer experience. Demonstrate ability to manage and lead a team of\"\n",
      "Appended sample 3 to golden dataset.\n",
      "\n",
      "--- Sample 4 ---\n",
      "Prompt: \"We ve made a lot of progress since opening the doors in 1942, but one thing has never changed our commitment to serve, heal, lead, educate, and innovate. We believe that every award earned, every record broken and every patient helped is because of the dedicated employees who fill our...\"\n",
      "Generated Text: \"We ve made a lot of progress since opening the doors in 1942, but one thing has never changed our commitment to serve, heal, lead, educate, and innovate. We believe that every award earned, every record broken and every patient helped is because of the dedicated employees who fill our offices each week. We re excited to be the leading leader in delivering high volume, reliable, quality, and cost effective medical care to patients. Our vision for our staff s well being is to provide them with the highest quality, highest value for their contributions, and the best of both worlds. We are seeking a dedicated, hard working and caring individual to join our team. Responsibilities Provide and maintain a high level of customer service, including timely, accurate and timely delivery of medical care through our team of dedicated, patient oriented doctors and nurses.Support our team as they work to ensure patient satisfaction by supporting patient care and providing patient referrals as necessary.Provide timely, accurate and timely diagnosis, treatment, and referral assistance to our patients.Support staff in the preparation of patient referrals.Provide timely medical care through the use of appropriate and appropriate medical equipment and supplies.Support staff in their efforts to maintain a clean and professional work environment.Provide quality services, including medical supplies and hygiene. Assist in the preparation of patient records and follow up appointments with our team on the latest medical information and information regarding the patient. Assist in the collection of patient information, including medical records, medical devices, records, and other information necessary to perform assigned duties as assigned.Assist our team in providing medical care to our patients.Maintain and maintain a quality environment by maintaining and maintaining a healthy, clean, and well functioning community.Assist our team with the maintenance and repair of medical equipment and equipment.Participate in and participate in the management of our community health services.Provide and maintain professional and financial support to our employees.Participate in the education and training of our staff and staff members as they prepare to work on their respective careers and professional objectives.Provide, train, and equip staff members and supervisors as needed. Assist our team in maintaining a safe and friendly work environment.Support our staff in the preparation of patient records and follow up appointments with our team on the latest medical information and information regarding the patient.Assist our team in the maintenance and repair of medical equipment and equipment.Maintain and maintain a clean and professional work environment. Assist our team in supporting our patients.Provide and maintain professional and financial support to our employees.Participate in the education and training of our staff and staff members as they prepare to work on their respective careers and professional objectives.Participate in the training and supervision of our employees, ensuring compliance with all state regulations and requirements. Assist with the\"\n",
      "Appended sample 4 to golden dataset.\n",
      "\n",
      "--- Sample 5 ---\n",
      "Prompt: \"At Whole Foods Market, we re committed to providing record setting grocery delivery services to our Prime Now customers. This is a fast growing program and candidates who are passionate about our quality products and great customer service will be a great fit. We think you ll agree that it...\"\n",
      "Generated Text: \"At Whole Foods Market, we re committed to providing record setting grocery delivery services to our Prime Now customers. This is a fast growing program and candidates who are passionate about our quality products and great customer service will be a great fit. We think you ll agree that it takes a little to get started on our exciting new grocery delivery platform. At Whole Foods Market, our customers are looking for the best products and are passionate about our customer service. We are a fast growing program and candidates who are passionate about our quality products and great customer service will be a great fit. We think you ll agree that it takes a little to start on our exciting new grocery delivery platform. At Whole Foods Market, our customers are looking for the best products and are passionate about our customer service. We are a fast growing program and candidates who are passionate about our customer service. We think you ll agree that it takes a little to start on our exciting new grocery delivery platform. At Whole Foods Market, our customers are looking for the best products and are passionate about our customer service. We are a fast growing program and candidates who are passionate about our customer service. We think you ll agree that it takes a little to start on our exciting new grocery delivery platform. At Whole Foods Market, our customers are looking for the best products and are passionate about our customer service. We are a fast growing program and candidates who are passionate about our customer service. We think you ll agree that it takes a little to start on our exciting new grocery delivery platform. We are seeking talented individuals to join our team. This position is not currently a full time position. If you have a great story to tell, you might consider joining us for a year and we will look forward to hearing about your contributions! At Whole Foods Market, our employees are committed to providing quality products and great customer service. As a Whole Foods Market, our customers are looking for the best products and are passionate about our customer service. We are a fast growing program and candidates who are passionate about our customer service. We think you ll agree that it takes a little to start on our exciting new grocery delivery platform. At Whole Foods Market, our customers are looking for the best products and are passionate about our customer service. We are a fast growing program and candidates who are passionate about our customer service. We think you ll agree that it takes a little to start on our exciting new grocery delivery platform. We are searching for people to join our team. If you have a great story to tell, you might consider joining us for a year and we will look forward to hearing about your contributions! At Whole Foods Market, our employees are committed to providing quality products and great customer service. As a Whole Foods Market, our customers are looking for the\"\n",
      "Appended sample 5 to golden dataset.\n",
      "\n",
      "--- Sample 6 ---\n",
      "Prompt: \"PHD is a global communications planning and media buying agency network delivering smart strategic thinking and creative innovation for the world s leading brands. Brilliant media thinking is in our DNA. A culture of thought leadership, creativity and innovation has seen us grow from a challenger agency in the UK,...\"\n",
      "Generated Text: \"PHD is a global communications planning and media buying agency network delivering smart strategic thinking and creative innovation for the world s leading brands. Brilliant media thinking is in our DNA. A culture of thought leadership, creativity and innovation has seen us grow from a challenger agency in the UK, to an internationally recognized player on a global stage. We re redefining how content and media are communicated and delivered by leading brands across industries and territories. What You ll Do Responsibilities Developing, developing, and delivering digital strategy strategies for content and media to drive the success of our content and media strategy. Working on strategic digital media and marketing strategies that combine strategy, data management and strategy to drive the success of our content and media. Collaborating with other content and media partners to develop strategic communications strategies for media strategy. Managing content and media partnerships to maximize content and media value for our clients. Collaborative with media and content marketing teams in the digital and social media sectors to deliver strategic digital and social media strategy. Identifying opportunities to expand digital and social media engagement across all segments of the organization. Developing and delivering strategic digital media and digital marketing strategies that leverage the digital and social media channels of the business to reach audiences across markets and markets. Engaging and cultivating audiences in the digital media sector to deliver content to clients. Working with content and media partners in the digital and social media sectors to deliver strategic digital and social media strategy. Identifying opportunities to expand digital and social media engagement across all segments of the organization. Working with content and media partners in the digital and social media sectors to deliver content to clients. Working with content and media partners in the digital and social media sectors to deliver content to clients. Understanding of and leveraging content and media to drive content and media value for our clients. Understanding of the need for a media strategy that supports a strong digital and social media presence. Understanding of the need for digital and social media marketing strategies that combine strategy, data management and strategy to drive content and media value for our clients. Managing content and media partnerships to maximize content and media value for our clients. Developing and delivering strategic digital media and digital marketing strategies that leverage the digital and social media channels of the business to reach audiences across markets and markets. Collaborating with content and media partners to develop strategic digital and social media strategies that leverage the digital and social media channels of the business to reach audiences across markets and markets. Understanding of and leveraging content and media to drive content and media value for our clients. Understanding of the need for a media strategy that supports a strong digital and social media presence. Understanding of media and content marketing partnerships that leverage social media platforms to drive digital and social media strategy. Working with content and media partners to develop and deliver strategic digital media and digital marketing strategies that leverage\"\n",
      "Appended sample 6 to golden dataset.\n",
      "\n",
      "--- Sample 7 ---\n",
      "Prompt: \"Starting hiring pay range based on location, experience, qualifications, etc. 10 12 hour As a Cook at Raising Cane s, you will set the pace of the Restaurant by ensuring all of the food we serve meets our high standards. Working in the Kitchen at Raising Cane s is a...\"\n",
      "Generated Text: \"Starting hiring pay range based on location, experience, qualifications, etc. 10 12 hour As a Cook at Raising Cane s, you will set the pace of the Restaurant by ensuring all of the food we serve meets our high standards. Working in the Kitchen at Raising Cane s is a fun job and a rewarding experience that will last your entire career! We are seeking a full time Cook to join our team, with a full time role as a Senior Chef, and a rotating team of over 50 full time cooks. We love to hear how you have impacted our Restaurant and what you are looking for, and you can be the first choice for a full time Chef in the Restaurant. We are seeking someone who will be able to make a meaningful impact in our Restaurant and our communities. Qualifications Strong analytical, organizational, and problem solving skills.Strong interpersonal skills and ability to prioritize and prioritize work well together.Ability to work in a team environment.Ability to work collaboratively, with a collaborative approach.Strong ability to collaborate collaboratively, with a collaborative approach.Ability to manage and resolve issues with respect to the restaurant and the restaurant community as a whole.Ability to communicate effectively and effectively in an informal environment.Ability to collaborate with colleagues and customers on restaurant related issues.Ability to manage and resolve issues with respect to the restaurant and the restaurant community as a whole.Experience as a Senior Chef is a plus and a must. Salary Range 20 19.00 19.25 20 21.50 20 22.00 <EOD>\"\n",
      "Appended sample 7 to golden dataset.\n",
      "\n",
      "--- Sample 8 ---\n",
      "Prompt: \"Join a strong community where all we do is care for the children and families we serve every day, as well as for our dedicated team members. Our people are our best asset. We listen and we know what you re looking for You want benefits. We support you with...\"\n",
      "Generated Text: \"Join a strong community where all we do is care for the children and families we serve every day, as well as for our dedicated team members. Our people are our best asset. We listen and we know what you re looking for You want benefits. We support you with a variety of flexible hours and are committed to providing a full range of benefits including a full time paid parental leave plan. We have a comprehensive health plan that you ll be able to choose from. Our employees can work full time, part time, paid time off, paid holidays and holidays. All Benefits Our benefits are tailored specifically for you, your family, your company, and your career. We offer an amazing benefits package including a comprehensive healthcare program, 401 k, paid parental leave, health savings accounts with company match, and a generous 401 k match. You ll find an exciting and rewarding opportunity to join our team, learn from our amazing team, and make your voice heard. Our benefits will include health coverage and dental insurance. Benefits are paid for by the employee s own company and are designed to be paid for on a per job basis. We are committed to working hard to provide you a competitive pay and benefit package. Job Description As a Registered Nurse, your work may include the following duties as a Registered Nurse, including nursing or other related work. The Nursing Nurse will provide support and guidance to the nursing unit and the unit s residents, and the unit may provide support to the residents and residents of the hospital by assisting with various tasks and procedures. The Nursing Nurse must have good working knowledge of nursing and related nursing facilities and facilities of interest to the resident or resident. The Nursing Nurse may be employed in a hospital setting, in an outpatient environment, or as an emergency room nurse with special supervision. A nurse with special supervision in the nursing unit. A nurse with special supervision in an outpatient setting, in a hospital environment, or as an emergency room nurse with special supervision. A Nurse with special supervision in an outpatient setting, as needed, must be available at least two times a week during the shift. Required Experience Required Qualifications Bachelor s Degree Bachelor s degree in Nursing, Nursing or related nursing. Bachelor s degree in Nursing, Nursing or related nursing, preferably in nursing or related nursing, preferably in nursing or related nursing. Excellent written and oral communication skills. Knowledge of nursing facilities and services. Familiarity with the patient s condition, needs, and expectations of the patient. Strong attention to detail and problem solving skills. Ability to work independently with a team of associates. Ability to work safely and effectively in a fast paced environment. Experience working as a Registered Nurse. Qualifications Required Bachelor s Degree Bachelor s degree in Nursing, Nursing or related nursing. Bachelor s degree in Nursing, Nursing or\"\n",
      "Appended sample 8 to golden dataset.\n",
      "\n",
      "--- Sample 9 ---\n",
      "Prompt: \"Senior Data Science Lead Travel Analytics Location Hybrid Role Commutable to Atlanta, GA Architect cutting edge Machine Learning solutions.Champion commercial driven Data Science.Nurture a culture of innovation within your team.Mentor emerging talent within the Data Science organization.Produce scalable machine learning algorithms. Leadership Mentorship Lead and nurture junior data analysts and...\"\n",
      "Generated Text: \"Senior Data Science Lead Travel Analytics Location Hybrid Role Commutable to Atlanta, GA Architect cutting edge Machine Learning solutions.Champion commercial driven Data Science.Nurture a culture of innovation within your team.Mentor emerging talent within the Data Science organization.Produce scalable machine learning algorithms. Leadership Mentorship Lead and nurture junior data analysts and Data Science professionals within the Data Science organization.Provide mentorship and guidance in the Data Science environment to senior leaders.Lead and develop a team of data scientists who are passionate about Data Sciences.Provide a positive environment for all data scientists at all levels of the organization.Lead a team of Senior Data Science team members who are motivated to grow as leaders and innovators.Develop and maintain a high level of team collaboration and collaboration.Lead the Data Science team at various levels of the organization.Develop and maintain a team focused and efficient data management system.Create new data science teams and maintain their data quality.Mentor Data Science with other Data Science leaders and data engineering leaders to develop a new Data Science organization that fosters leadership.Participate in the Data science organization development process.Lead and manage Data Science teams across various departments as well as within the Data Science organization.Monitor the team, communicate with senior data scientists, and support their work.Monitor and maintain data quality and maintain high quality data management tools.Support Data Science teams in their growth, development, and implementation.Mentor Data Science team members on a daily basis.Provide leadership mentorship and guidance to team members and senior leaders to help them develop and maintain high performance systems, systems, tools, and data systems.Support Data Science team members on a daily basis and participate in the team development process.Participate in the development of data analytics tools and data management processes.Provide leadership coaching and mentorship to team members as well as senior leaders.Lead and manage a high level of team collaboration and collaboration.Support Data Science teams in the development and implementation of new Data Science organization, and provide guidance and guidance to senior leaders.Provide leadership coaching and mentorship to team members and senior leaders.Lead and manage Data Science teams in the development and implementation of new Data Science organization, and provide guidance and guidance to senior leaders.Participate in the Development of data analytics tools and data management processes as well as within the Data Science organization.Lead and manage Data Science teams in the development and implementation of new Data Science organization, and provide guidance and guidance to senior leaders.Provide leadership leadership coaching and mentorship to team members and senior leaders.Participate in the development of data analytics tools and data management processes.Lead and manage teams of Data Science staff in collaboration with senior leaders and data engineers to develop high performance systems, tools, and data tools.Participate in the development of data analytics tools and data management\"\n",
      "Appended sample 9 to golden dataset.\n",
      "\n",
      "--- Sample 10 ---\n",
      "Prompt: \"Specialty Competency Assurance Industry Sector Not Applicable Time Type Full time Travel Requirements Up to 40 A career within External Audit services, will provide you with the opportunity to provide a range of Assurance services and business advice to a variety of clients from small, fast growing clients to large...\"\n",
      "Generated Text: \"Specialty Competency Assurance Industry Sector Not Applicable Time Type Full time Travel Requirements Up to 40 A career within External Audit services, will provide you with the opportunity to provide a range of Assurance services and business advice to a variety of clients from small, fast growing clients to large and dynamic multinationals. As a Senior Business Analyst, you will be responsible for the development of the business plan, ensuring that the company is in compliance with the Business and Tax Regulations, and the Financial Reporting Act. In addition to this, we will also be providing you with valuable experience developing and implementing new business plans and services for the business. You will play an active role in the team of professionals that develop the business plan and provide guidance to the business and clients. We are currently hiring and are looking for a Senior Business Analyst to join our team. You will be responsible for the development of the Business and Tax Regulations and the Financial Reporting Act. In addition to this, we will be providing you with valuable experience developing and implementing new business plans and services for the business. You will play an active role in the team of professionals that develop the business plan and provide guidance to the business and clients. We are currently hiring and are looking for a Senior Business Analyst to join our team. You will be responsible for the development of the Business and Tax Regulations and the Financial Reporting Act. Qualifications Experience in accounting is preferred, but not required Experience in Business and Tax Finance, Accounting, or Finance experience preferred Qualified for a job with our company, and preferably with a large organization. Required Requirements The Business and Tax Finance experience you will be looking for requires a strong understanding of the financial reporting laws and regulations that govern the business, business operations, and related functions. Experience in Accounting is preferred, but not required Experience with Business Finance or Accounting is preferred. Knowledge of financial instruments and accounting principles is preferred. Experience with complex business and tax issues and the accounting process is preferred. Ability to effectively interpret financial statements is preferred. Strong communication and analytical skills are essential. Ability to work independently is preferred. Ability to effectively communicate effectively in multiple languages is essential. Ability to work with colleagues, colleagues, and clients is essential. <EOD>\"\n",
      "Appended sample 10 to golden dataset.\n",
      "\n",
      "Cleaned up generator and emptied CUDA cache.\n",
      "\n",
      "Golden dataset saved to: /home/gabriel/dev/SCIA/NLP_Linkedin_offers/models/src/generation/transformers/small_golden_dataset_gpt2.csv\n"
     ]
    }
   ],
   "source": [
    "ground_truth = []\n",
    "predictions = []\n",
    "inputs = []\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'cuda' if device == 0 else 'cpu'}\")\n",
    "\n",
    "# Check if trainer and necessary data are available\n",
    "if 'trainer' in globals() and hasattr(trainer, 'model') and trainer.model and \\\n",
    "   'raw_datasets' in globals() and raw_datasets and 'test' in raw_datasets and \\\n",
    "   'text_column' in globals():\n",
    "\n",
    "    print(f\"Initializing text-generation pipeline with model: {trainer.model.config._name_or_path if hasattr(trainer.model, 'config') else 'N/A'}\")\n",
    "    \n",
    "    # Use the model and tokenizer from the trainer\n",
    "    # These should be the fine-tuned model and its corresponding tokenizer\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=trainer.model,\n",
    "        tokenizer=trainer.tokenizer,\n",
    "        device=device,\n",
    "        do_sample=True,\n",
    "        # temperature=0,\n",
    "        top_k=10,\n",
    "        # top_p=0.95,\n",
    "    )\n",
    "\n",
    "    print(\"Pipeline initialized.\")\n",
    "\n",
    "    # Get a few examples from the raw test dataset to use as prompts\n",
    "    num_samples_to_generate = 10\n",
    "    if len(raw_datasets['test']) >= num_samples_to_generate:\n",
    "        sample_indices = range(num_samples_to_generate)\n",
    "    else:\n",
    "        sample_indices = range(len(raw_datasets['test']))\n",
    "        print(f\"Warning: Requested {num_samples_to_generate} samples, but only {len(raw_datasets['test'])} available in test set.\")\n",
    "\n",
    "    for i in sample_indices:\n",
    "        original_text = raw_datasets['test'][i][text_column]\n",
    "        \n",
    "        # Create a prompt (e.g., first 50 words of the original text)\n",
    "        prompt_max_words = 50\n",
    "        prompt_text = \" \".join(original_text.split()[:prompt_max_words])\n",
    "        \n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"Prompt: \\\"{prompt_text}...\\\"\")\n",
    "        \n",
    "        try:\n",
    "            # Generate text\n",
    "            # max_new_tokens specifies how many tokens to generate after the prompt\n",
    "            # max_length would be prompt_length + max_new_tokens\n",
    "            generated_sequences = generator(\n",
    "                prompt_text,\n",
    "                max_new_tokens=500,  # Generate 100 new tokens\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=generator.tokenizer.eos_token_id # Important for open-ended generation\n",
    "            )\n",
    "            \n",
    "            generated_text = generated_sequences[0]['generated_text']\n",
    "            print(f\"Generated Text: \\\"{generated_text}\\\"\")\n",
    "            # Append to predictions and ground truth\n",
    "            predictions.append(generated_text)\n",
    "            ground_truth.append(original_text)\n",
    "            inputs.append(prompt_text)\n",
    "            print(f\"Appended sample {i+1} to golden dataset.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation for sample {i+1}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # Clean up to free GPU memory if a GPU was used\n",
    "    if device == 0:\n",
    "        del generator\n",
    "        # torch.cuda.empty_cache()\n",
    "        print(\"\\nCleaned up generator and emptied CUDA cache.\")\n",
    "        \n",
    "elif 'trainer' not in globals() or not hasattr(trainer, 'model') or not trainer.model:\n",
    "    print(\"Skipping pipeline inference: 'trainer' or 'trainer.model' is not available.\")\n",
    "elif 'raw_datasets' not in globals() or not raw_datasets or 'test' not in raw_datasets:\n",
    "    print(\"Skipping pipeline inference: 'raw_datasets' or 'raw_datasets['test']' is not available.\")\n",
    "elif 'text_column' not in globals():\n",
    "    print(\"Skipping pipeline inference: 'text_column' variable is not defined.\")\n",
    "else:\n",
    "    print(\"Skipping pipeline inference due to missing components. Ensure trainer, data, and text_column are set up.\")\n",
    "\n",
    "golden_dataset = pd.DataFrame({\n",
    "    'prediction': predictions,\n",
    "    'ground_truth': ground_truth,\n",
    "    'input': inputs\n",
    "})\n",
    "model_name = 'gpt2' # TO DELETE\n",
    "sanitized_model_name = model_name.replace('/', '_')\n",
    "golden_dataset_file_path = os.path.join(output_directory, f\"small_golden_dataset_{sanitized_model_name}.csv\")\n",
    "golden_dataset.to_csv(golden_dataset_file_path, index=False)\n",
    "print(f\"\\nGolden dataset saved to: {golden_dataset_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9bceb1",
   "metadata": {},
   "source": [
    "### Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded73d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final model and tokenizer to /home/gabriel/dev/SCIA/NLP_Linkedin_offers/models/src/generation/transformers/fine_tuned_model_gpt2...\n",
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "final_model_path = f\"{output_directory}/fine_tuned_model_{sanitized_model_name}\"\n",
    "\n",
    "if train_dataset: # Only save if training actually happened\n",
    "    print(f\"Saving final model and tokenizer to {final_model_path}...\")\n",
    "    try:\n",
    "        trainer.save_model(final_model_path)\n",
    "        tokenizer.save_pretrained(final_model_path)\n",
    "        print(\"Model and tokenizer saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model/tokenizer: {e}\")\n",
    "else:\n",
    "    print(\"Skipping final model saving as training did not run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495db382",
   "metadata": {},
   "source": [
    "### Evaluate a loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c61da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0509 16:09:36.977000 2997 torch/_inductor/utils.py:1250] [2/0_1] Not enough SMs to use max_autotune_gemm mode\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1526' max='1526' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1526/1526 02:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 2.020193576812744, 'eval_model_preparation_time': 0.9129, 'eval_runtime': 152.3496, 'eval_samples_per_second': 40.059, 'eval_steps_per_second': 10.016}\n",
      "***** eval metrics *****\n",
      "  eval_loss                   =     2.0202\n",
      "  eval_model_preparation_time =     0.9129\n",
      "  eval_runtime                = 0:02:32.34\n",
      "  eval_samples_per_second     =     40.059\n",
      "  eval_steps_per_second       =     10.016\n"
     ]
    }
   ],
   "source": [
    "if eval_dataset and trainer:\n",
    "    print(\"\\nStarting evaluation on the test set...\")\n",
    "    try:\n",
    "        eval_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "        print(f\"Evaluation results: {eval_results}\")\n",
    "        trainer.log_metrics(\"eval\", eval_results)\n",
    "        trainer.save_metrics(\"eval\", eval_results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "else:\n",
    "    print(\"Skipping evaluation as eval_dataset or trainer is not available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-linkedin-offers-VrYHBMh4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
