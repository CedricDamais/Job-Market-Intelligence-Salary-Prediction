{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "\n",
    "THis notebook will contain the Neural Network taht will do the text generation\n",
    "\n",
    "I will have a FFNN generate job descriptions, will I do so:\n",
    "\n",
    "1- First part is the tokenization part take the jb description and have the tokens\n",
    "2- Create my vocabulary:\n",
    "        - First I will try with every single toke,\n",
    "        - SecondI will try with only the top N most frequent token\n",
    "3- For the token representation I will use a Enbeddings, pre-trained embeddings wit h word2Vec\n",
    "4- How will I train the FFNN ?\n",
    "   - I will create Input, Output pairs\n",
    "        - The Input will be a context window of fixed size N and a Special token <START> and the output is the actual next token\n",
    "        it will be as followed\n",
    "        | Input (context window, N=3) | Output (next token) |\n",
    "        |--------------------------------------|---------------------|\n",
    "        | [\"<START>\", \"Analyze\", \"data\"]       |     \"and\"           |\n",
    "    \n",
    "\n",
    "\n",
    "Perhaps try dropping job descriptions that are over the thrshold to keep the repartition as gaussean bell (Normal law distribuion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../..')) # Adjust '..' if your notebook is deeper\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources checked/downloaded.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt' tokenizer...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'stopwords'...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Add this check and download for punkt_tab\n",
    "try:\n",
    "    # Check for the specific English directory within punkt_tab\n",
    "    nltk.data.find('tokenizers/punkt_tab/english/') \n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt_tab'...\")\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "print(\"NLTK resources checked/downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 08:44:02,790 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-05-07 08:44:02,791 - INFO - Load pretrained SentenceTransformer: multi-qa-MiniLM-L6-cos-v1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from models.src.preprocessing.tokenizer import tokenize_data_frame\n",
    "from data.process_data_modeling import get_processed_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import logging\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "embedding_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "from torch.utils.data import IterableDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cedric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/cedric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 08:44:03,880 - INFO - Importing data from Kaggle\n",
      "2025-05-07 08:44:03,881 - INFO - Path to dataset files: /home/cedric/.cache/kagglehub/datasets/arshkon/linkedin-job-postings/versions/13\n",
      "2025-05-07 08:44:03,882 - INFO - List of files in the dataset: ['companies', 'postings.csv', 'mappings', 'jobs']\n",
      "2025-05-07 08:44:07,002 - INFO - Rows with at least one NaN value: 1725\n",
      "2025-05-07 08:44:07,003 - INFO - Number of rows before dropping NaN values: 123849\n",
      "2025-05-07 08:44:07,026 - INFO - Number of rows after dropping NaN values: 122124\n",
      "2025-05-07 08:44:49,271 - INFO - DataFrame saved to: /home/cedric/Desktop/MAJEUR/NLP/NLP_Linkedin_offers/models/src/generation/data/processed/cleaned_postings_modeling.parquet\n",
      "2025-05-07 08:44:49,272 - INFO - Data processing completed successfully !\n",
      "2025-05-07 08:44:49,272 - INFO - Returning the processed DataFrame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Coordinator</td>\n",
       "      <td>job description a leading real estate firm in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assitant Restaurant Manager</td>\n",
       "      <td>the national exemplar is accepting application...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Elder Law / Trusts and Estates Associat...</td>\n",
       "      <td>senior associate attorney elder law trusts and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Economic Development and Planning Intern</td>\n",
       "      <td>job summary the economic development planning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Producer</td>\n",
       "      <td>company description raw cereal is a creative d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                              Marketing Coordinator   \n",
       "2                        Assitant Restaurant Manager   \n",
       "3  Senior Elder Law / Trusts and Estates Associat...   \n",
       "5           Economic Development and Planning Intern   \n",
       "6                                           Producer   \n",
       "\n",
       "                                         description  \n",
       "0  job description a leading real estate firm in ...  \n",
       "2  the national exemplar is accepting application...  \n",
       "3  senior associate attorney elder law trusts and...  \n",
       "5  job summary the economic development planning ...  \n",
       "6  company description raw cereal is a creative d...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_processed_data()\n",
    "df.head()\n",
    "df.drop(columns=[\"company_name\", \"location\"], inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Coordinator __START__ job descriptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assitant Restaurant Manager __START__ the nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Elder Law / Trusts and Estates Associat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Economic Development and Planning Intern __STA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Producer __START__ company description raw cer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              merged\n",
       "0  Marketing Coordinator __START__ job descriptio...\n",
       "2  Assitant Restaurant Manager __START__ the nati...\n",
       "3  Senior Elder Law / Trusts and Estates Associat...\n",
       "5  Economic Development and Planning Intern __STA...\n",
       "6  Producer __START__ company description raw cer..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"merged\"] = df['title'] + \" __START__ \" + df['description'] + \" __END__\"\n",
    "df.drop(columns=[\"title\", \"description\"], inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merged</th>\n",
       "      <th>merged_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Coordinator __START__ job descriptio...</td>\n",
       "      <td>[Marketing, Coordinator, __START__, job, descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assitant Restaurant Manager __START__ the nati...</td>\n",
       "      <td>[Assitant, Restaurant, Manager, __START__, nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Elder Law / Trusts and Estates Associat...</td>\n",
       "      <td>[Senior, Elder, Law, /, Trusts, Estates, Assoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Economic Development and Planning Intern __STA...</td>\n",
       "      <td>[Economic, Development, Planning, Intern, __ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Producer __START__ company description raw cer...</td>\n",
       "      <td>[Producer, __START__, company, description, ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              merged  \\\n",
       "0  Marketing Coordinator __START__ job descriptio...   \n",
       "2  Assitant Restaurant Manager __START__ the nati...   \n",
       "3  Senior Elder Law / Trusts and Estates Associat...   \n",
       "5  Economic Development and Planning Intern __STA...   \n",
       "6  Producer __START__ company description raw cer...   \n",
       "\n",
       "                                    merged_tokenized  \n",
       "0  [Marketing, Coordinator, __START__, job, descr...  \n",
       "2  [Assitant, Restaurant, Manager, __START__, nat...  \n",
       "3  [Senior, Elder, Law, /, Trusts, Estates, Assoc...  \n",
       "5  [Economic, Development, Planning, Intern, __ST...  \n",
       "6  [Producer, __START__, company, description, ra...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df = tokenize_data_frame(df, [\"merged\"], method=\"nltk\", remove_stopwords=True)\n",
    "tokenized_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merged_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Marketing, Coordinator, __START__, job, descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Assitant, Restaurant, Manager, __START__, nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Senior, Elder, Law, /, Trusts, Estates, Assoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Economic, Development, Planning, Intern, __ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Producer, __START__, company, description, ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    merged_tokenized\n",
       "0  [Marketing, Coordinator, __START__, job, descr...\n",
       "2  [Assitant, Restaurant, Manager, __START__, nat...\n",
       "3  [Senior, Elder, Law, /, Trusts, Estates, Assoc...\n",
       "5  [Economic, Development, Planning, Intern, __ST...\n",
       "6  [Producer, __START__, company, description, ra..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df.drop(columns=[\"merged\"], inplace=True)\n",
    "tokenized_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df_train, tokenized_df_test = train_test_split(tokenized_df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merged_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Marketing, Coordinator, __START__, job, descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Assitant, Restaurant, Manager, __START__, nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Senior, Elder, Law, Trusts, Estates, Associat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Economic, Development, Planning, Intern, __ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Producer, __START__, company, description, ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    merged_tokenized\n",
       "0  [Marketing, Coordinator, __START__, job, descr...\n",
       "2  [Assitant, Restaurant, Manager, __START__, nat...\n",
       "3  [Senior, Elder, Law, Trusts, Estates, Associat...\n",
       "5  [Economic, Development, Planning, Intern, __ST...\n",
       "6  [Producer, __START__, company, description, ra..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unwanted_token = {\"-\", \".\", \"!\", \"?\", \"...\", \"....\", \".....\", \"/\", \"|\", \"~\", \"`\", \"=\", \"+\", \"_\", \"*\"}\n",
    "\n",
    "tokenized_df_train.head()\n",
    "tokenized_df['merged_tokenized'] = tokenized_df['merged_tokenized'].apply(\n",
    "    lambda tokens: [t for t in tokens if t not in unwanted_token]\n",
    ")\n",
    "tokenized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merged_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88431</th>\n",
       "      <td>[Big, Spring, Market, Overnight, Perishables, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94806</th>\n",
       "      <td>[Front, Desk, Guest, Services, Representative,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67484</th>\n",
       "      <td>[AVP, Head, National, Markets, __START__, job,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29134</th>\n",
       "      <td>[Air, Export, Operations, Specialist, __START_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24667</th>\n",
       "      <td>[Outside, Sales, Representative, Medical, Equi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        merged_tokenized\n",
       "88431  [Big, Spring, Market, Overnight, Perishables, ...\n",
       "94806  [Front, Desk, Guest, Services, Representative,...\n",
       "67484  [AVP, Head, National, Markets, __START__, job,...\n",
       "29134  [Air, Export, Operations, Specialist, __START_...\n",
       "24667  [Outside, Sales, Representative, Medical, Equi..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(df:pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Get the vocabulary from the dataframe and the size of the vocabulary\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for _, row in df.iterrows():\n",
    "        merged_string = row[\"merged\"]\n",
    "        tokens = merged_string.split(\" \")\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab, len(vocab)\n",
    "\n",
    "\n",
    "def get_pruned_vocab_tokenized(df, column, top_k=50000):\n",
    "    \"\"\"\n",
    "    Get the pruned vocabulary from the tokenized dataframe\n",
    "    \"\"\"\n",
    "    token_counter = Counter()\n",
    "    for text in df[column]:\n",
    "        token_counter.update(text)\n",
    "    most_common = token_counter.most_common(top_k)\n",
    "    vocab = {token: idx for idx, (token, _) in enumerate(most_common)}\n",
    "    vocab['__START__'] = len(vocab)\n",
    "    vocab['__END__'] = len(vocab)\n",
    "    vocab['__UNK__'] = len(vocab)\n",
    "    return vocab, len(vocab)\n",
    "\n",
    "def get_pruned_vocab(df, column, top_k=50000):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary of the top_k most frequent tokens.\n",
    "    \"\"\"\n",
    "    token_counter = Counter()\n",
    "    for text in df[column]:\n",
    "        tokens = text.split(\" \")\n",
    "        token_counter.update(tokens)\n",
    "\n",
    "    most_common = token_counter.most_common(top_k)\n",
    "    vocab = {token: idx for idx, (token, _) in enumerate(most_common)}\n",
    "    vocab[\"__START__\"] = len(vocab)\n",
    "    vocab[\"__END__\"] = len(vocab)\n",
    "    vocab['__UNK__'] = len(vocab)\n",
    "    return vocab, len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_pruned_vocab() missing 1 required positional argument: 'column'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vocab, vocab_size = \u001b[43mget_pruned_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_df_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: get_pruned_vocab() missing 1 required positional argument: 'column'"
     ]
    }
   ],
   "source": [
    "# vocab, vocab_size = get_pruned_vocab(tokenized_df_train, top_k=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 0,\n",
       " '.': 1,\n",
       " 'experience': 2,\n",
       " 'work': 3,\n",
       " 'team': 4,\n",
       " 'skills': 5,\n",
       " 'job': 6,\n",
       " 'including': 7,\n",
       " 'ability': 8,\n",
       " 'business': 9,\n",
       " 'management': 10,\n",
       " 'time': 11,\n",
       " 'company': 12,\n",
       " 'required': 13,\n",
       " 'position': 14,\n",
       " 'customer': 15,\n",
       " 'support': 16,\n",
       " 'care': 17,\n",
       " 'service': 18,\n",
       " 'benefits': 19,\n",
       " 'years': 20,\n",
       " 'information': 21,\n",
       " 'health': 22,\n",
       " 'sales': 23,\n",
       " '__START__': 50000,\n",
       " '__END__': 50000,\n",
       " 'requirements': 26,\n",
       " 'services': 27,\n",
       " 'opportunity': 28,\n",
       " 'data': 29,\n",
       " 'development': 30,\n",
       " 'status': 31,\n",
       " 'role': 32,\n",
       " 'knowledge': 33,\n",
       " 'may': 34,\n",
       " 'must': 35,\n",
       " 'new': 36,\n",
       " 'environment': 37,\n",
       " 'employment': 38,\n",
       " 'working': 39,\n",
       " 'customers': 40,\n",
       " 'provide': 41,\n",
       " 'medical': 42,\n",
       " 'related': 43,\n",
       " 'responsibilities': 44,\n",
       " '!': 45,\n",
       " 'employees': 46,\n",
       " 'based': 47,\n",
       " 'ensure': 48,\n",
       " 'training': 49,\n",
       " 'program': 50,\n",
       " 'quality': 51,\n",
       " 'qualifications': 52,\n",
       " 'within': 53,\n",
       " 'us': 54,\n",
       " 'high': 55,\n",
       " 'strong': 56,\n",
       " 'project': 57,\n",
       " 'process': 58,\n",
       " 'disability': 59,\n",
       " 'employee': 60,\n",
       " 'communication': 61,\n",
       " 'duties': 62,\n",
       " 'one': 63,\n",
       " 'opportunities': 64,\n",
       " 'paid': 65,\n",
       " 'education': 66,\n",
       " 'life': 67,\n",
       " 'preferred': 68,\n",
       " 'employer': 69,\n",
       " 'product': 70,\n",
       " 'equal': 71,\n",
       " 'patient': 72,\n",
       " 'systems': 73,\n",
       " 'degree': 74,\n",
       " 'well': 75,\n",
       " 'needs': 76,\n",
       " 'solutions': 77,\n",
       " 'people': 78,\n",
       " 'insurance': 79,\n",
       " 'technical': 80,\n",
       " 'financial': 81,\n",
       " 'equipment': 82,\n",
       " 'pay': 83,\n",
       " 'state': 84,\n",
       " 'professional': 85,\n",
       " 'career': 86,\n",
       " 'manager': 87,\n",
       " 'performance': 88,\n",
       " 'maintain': 89,\n",
       " 'gender': 90,\n",
       " 'office': 91,\n",
       " 'products': 92,\n",
       " 'client': 93,\n",
       " 'level': 94,\n",
       " 'design': 95,\n",
       " 'day': 96,\n",
       " 'full': 97,\n",
       " 'safety': 98,\n",
       " 'national': 99,\n",
       " 'clients': 100,\n",
       " 'include': 101,\n",
       " 'part': 102,\n",
       " 'responsible': 103,\n",
       " 'vision': 104,\n",
       " 'procedures': 105,\n",
       " 'plan': 106,\n",
       " 'best': 107,\n",
       " 'able': 108,\n",
       " 'industry': 109,\n",
       " 'perform': 110,\n",
       " 'store': 111,\n",
       " 'programs': 112,\n",
       " 'range': 113,\n",
       " 'assigned': 114,\n",
       " 'technology': 115,\n",
       " 'protected': 116,\n",
       " 'teams': 117,\n",
       " 'across': 118,\n",
       " 'operations': 119,\n",
       " 'make': 120,\n",
       " 'develop': 121,\n",
       " 'engineering': 122,\n",
       " 'projects': 123,\n",
       " 'applicants': 124,\n",
       " 'please': 125,\n",
       " 'help': 126,\n",
       " 'location': 127,\n",
       " 'apply': 128,\n",
       " 'join': 129,\n",
       " 'system': 130,\n",
       " '?': 131,\n",
       " 'staff': 132,\n",
       " 'orientation': 133,\n",
       " 'compensation': 134,\n",
       " 'physical': 135,\n",
       " 'providing': 136,\n",
       " 'standards': 137,\n",
       " 'year': 138,\n",
       " 'members': 139,\n",
       " 'description': 140,\n",
       " 'patients': 141,\n",
       " 'software': 142,\n",
       " 'world': 143,\n",
       " 'growth': 144,\n",
       " 'use': 145,\n",
       " 'meet': 146,\n",
       " 'color': 147,\n",
       " 'also': 148,\n",
       " 'race': 149,\n",
       " 'application': 150,\n",
       " 'leadership': 151,\n",
       " 'sexual': 152,\n",
       " 'origin': 153,\n",
       " 'age': 154,\n",
       " 'veteran': 155,\n",
       " 'organization': 156,\n",
       " 'manage': 157,\n",
       " 'excellent': 158,\n",
       " 'minimum': 159,\n",
       " 'identity': 160,\n",
       " 'dental': 161,\n",
       " 'compliance': 162,\n",
       " 'activities': 163,\n",
       " 'plans': 164,\n",
       " 'local': 165,\n",
       " 'without': 166,\n",
       " 'religion': 167,\n",
       " 'healthcare': 168,\n",
       " 'community': 169,\n",
       " 'tools': 170,\n",
       " 'key': 171,\n",
       " 'hours': 172,\n",
       " 'department': 173,\n",
       " 'processes': 174,\n",
       " 'applicable': 175,\n",
       " 'needed': 176,\n",
       " 'marketing': 177,\n",
       " 'internal': 178,\n",
       " 'every': 179,\n",
       " 'lead': 180,\n",
       " 'policies': 181,\n",
       " 'relationships': 182,\n",
       " 'salary': 183,\n",
       " 'culture': 184,\n",
       " 'offer': 185,\n",
       " 'issues': 186,\n",
       " 'security': 187,\n",
       " 'law': 188,\n",
       " 'qualified': 189,\n",
       " 'field': 190,\n",
       " 'committed': 191,\n",
       " 'school': 192,\n",
       " 'need': 193,\n",
       " 'using': 194,\n",
       " 'build': 195,\n",
       " 'provides': 196,\n",
       " 'maintenance': 197,\n",
       " 'assist': 198,\n",
       " 'functions': 199,\n",
       " 'reports': 200,\n",
       " 'essential': 201,\n",
       " 'competitive': 202,\n",
       " 'understanding': 203,\n",
       " '1': 204,\n",
       " 'building': 205,\n",
       " 'assistance': 206,\n",
       " 'learn': 207,\n",
       " 'drive': 208,\n",
       " 'written': 209,\n",
       " 'per': 210,\n",
       " 'appropriate': 211,\n",
       " 'personal': 212,\n",
       " 'effectively': 213,\n",
       " 'goals': 214,\n",
       " 'individuals': 215,\n",
       " '5': 216,\n",
       " 'travel': 217,\n",
       " 'individual': 218,\n",
       " 'looking': 219,\n",
       " 'sex': 220,\n",
       " 'practices': 221,\n",
       " 'family': 222,\n",
       " 'bachelor': 223,\n",
       " 'market': 224,\n",
       " 'global': 225,\n",
       " 'federal': 226,\n",
       " 'planning': 227,\n",
       " '3': 228,\n",
       " 'additional': 229,\n",
       " 'analysis': 230,\n",
       " 'leading': 231,\n",
       " 'nursing': 232,\n",
       " 'candidate': 233,\n",
       " 'tasks': 234,\n",
       " 'clinical': 235,\n",
       " 'diverse': 236,\n",
       " 'current': 237,\n",
       " 'multiple': 238,\n",
       " 'success': 239,\n",
       " 'diversity': 240,\n",
       " 'term': 241,\n",
       " 'create': 242,\n",
       " 'schedule': 243,\n",
       " '2': 244,\n",
       " 'following': 245,\n",
       " 'resources': 246,\n",
       " 'computer': 247,\n",
       " 'comprehensive': 248,\n",
       " 'applications': 249,\n",
       " 'days': 250,\n",
       " 'flexible': 251,\n",
       " 'long': 252,\n",
       " 'mission': 253,\n",
       " 'various': 254,\n",
       " 'general': 255,\n",
       " 'ensuring': 256,\n",
       " 'daily': 257,\n",
       " 'candidates': 258,\n",
       " 'shift': 259,\n",
       " 'research': 260,\n",
       " 'etc': 261,\n",
       " 'site': 262,\n",
       " 'equivalent': 263,\n",
       " 'receive': 264,\n",
       " 'complex': 265,\n",
       " 'access': 266,\n",
       " 'center': 267,\n",
       " 'problem': 268,\n",
       " 'effective': 269,\n",
       " 'specific': 270,\n",
       " 'identify': 271,\n",
       " '-': 272,\n",
       " 'limited': 273,\n",
       " 'production': 274,\n",
       " 'deliver': 275,\n",
       " 'areas': 276,\n",
       " 'certification': 277,\n",
       " 'eligible': 278,\n",
       " 'complete': 279,\n",
       " 'control': 280,\n",
       " 'hiring': 281,\n",
       " 'area': 282,\n",
       " 'learning': 283,\n",
       " 'values': 284,\n",
       " 'policy': 285,\n",
       " 'review': 286,\n",
       " 'necessary': 287,\n",
       " 'relevant': 288,\n",
       " 'managing': 289,\n",
       " 'group': 290,\n",
       " 'regard': 291,\n",
       " 'impact': 292,\n",
       " 'results': 293,\n",
       " 'license': 294,\n",
       " 'value': 295,\n",
       " 'power': 296,\n",
       " 'available': 297,\n",
       " 'basic': 298,\n",
       " 'package': 299,\n",
       " 'first': 300,\n",
       " 'contact': 301,\n",
       " 'developing': 302,\n",
       " 'reporting': 303,\n",
       " 'seeking': 304,\n",
       " 'fast': 305,\n",
       " 'partners': 306,\n",
       " 'basis': 307,\n",
       " 'senior': 308,\n",
       " 'good': 309,\n",
       " 'change': 310,\n",
       " 'delivery': 311,\n",
       " 'home': 312,\n",
       " 'risk': 313,\n",
       " 'potential': 314,\n",
       " 'future': 315,\n",
       " 'plus': 316,\n",
       " 'inclusive': 317,\n",
       " 'communicate': 318,\n",
       " 'reasonable': 319,\n",
       " 'associates': 320,\n",
       " 'take': 321,\n",
       " 'offers': 322,\n",
       " 'testing': 323,\n",
       " 'end': 324,\n",
       " 'account': 325,\n",
       " 'strategic': 326,\n",
       " 'network': 327,\n",
       " 'accounting': 328,\n",
       " 'like': 329,\n",
       " 'action': 330,\n",
       " 'timely': 331,\n",
       " 'includes': 332,\n",
       " 'self': 333,\n",
       " 'regulations': 334,\n",
       " 'focus': 335,\n",
       " 'associate': 336,\n",
       " 'direct': 337,\n",
       " 'positive': 338,\n",
       " 'successful': 339,\n",
       " 'top': 340,\n",
       " 'great': 341,\n",
       " 'conditions': 342,\n",
       " 'making': 343,\n",
       " 'accommodation': 344,\n",
       " 'maintaining': 345,\n",
       " 'organizational': 346,\n",
       " 'holidays': 347,\n",
       " 'external': 348,\n",
       " 'understand': 349,\n",
       " 'critical': 350,\n",
       " 'expertise': 351,\n",
       " 'grow': 352,\n",
       " 'practice': 353,\n",
       " 'documentation': 354,\n",
       " 'construction': 355,\n",
       " 'bonus': 356,\n",
       " 'solving': 357,\n",
       " 'safe': 358,\n",
       " 'functional': 359,\n",
       " 'manufacturing': 360,\n",
       " 'microsoft': 361,\n",
       " 'person': 362,\n",
       " '10': 363,\n",
       " 'materials': 364,\n",
       " 'manner': 365,\n",
       " 'problems': 366,\n",
       " 'k': 367,\n",
       " 'nurse': 368,\n",
       " 'follow': 369,\n",
       " 'others': 370,\n",
       " 'order': 371,\n",
       " 'background': 372,\n",
       " 'variety': 373,\n",
       " 'retail': 374,\n",
       " 'communities': 375,\n",
       " 'accounts': 376,\n",
       " 'cross': 377,\n",
       " 'highly': 378,\n",
       " 'serve': 379,\n",
       " 'operational': 380,\n",
       " 'verbal': 381,\n",
       " 'laws': 382,\n",
       " 'record': 383,\n",
       " 'member': 384,\n",
       " 'workplace': 385,\n",
       " 'improve': 386,\n",
       " 'requires': 387,\n",
       " 'strategies': 388,\n",
       " 'summary': 389,\n",
       " 'commitment': 390,\n",
       " 'non': 391,\n",
       " 'achieve': 392,\n",
       " 'leave': 393,\n",
       " 'base': 394,\n",
       " 'improvement': 395,\n",
       " 'social': 396,\n",
       " 'meetings': 397,\n",
       " 'partner': 398,\n",
       " 'food': 399,\n",
       " 'test': 400,\n",
       " 'digital': 401,\n",
       " 'detail': 402,\n",
       " 'consideration': 403,\n",
       " 'leader': 404,\n",
       " 'annual': 405,\n",
       " 'states': 406,\n",
       " 'strategy': 407,\n",
       " 'place': 408,\n",
       " 'diploma': 409,\n",
       " 'coverage': 410,\n",
       " 'hospital': 411,\n",
       " 'creating': 412,\n",
       " 'technologies': 413,\n",
       " 'exceptional': 414,\n",
       " 'cost': 415,\n",
       " 'performing': 416,\n",
       " 'stakeholders': 417,\n",
       " 'collaborate': 418,\n",
       " 'disabilities': 419,\n",
       " 'mental': 420,\n",
       " 'implement': 421,\n",
       " 'lift': 422,\n",
       " 'operating': 423,\n",
       " 'inventory': 424,\n",
       " 'participate': 425,\n",
       " 'contract': 426,\n",
       " 'paced': 427,\n",
       " 'match': 428,\n",
       " 'u.s.': 429,\n",
       " 'united': 430,\n",
       " 'require': 431,\n",
       " 'driven': 432,\n",
       " 'innovative': 433,\n",
       " 'companies': 434,\n",
       " 'throughout': 435,\n",
       " 'documents': 436,\n",
       " 'least': 437,\n",
       " 'set': 438,\n",
       " 'accurate': 439,\n",
       " 'week': 440,\n",
       " 'point': 441,\n",
       " 'initiatives': 442,\n",
       " 'maintains': 443,\n",
       " 'decisions': 444,\n",
       " 'levels': 445,\n",
       " 'talent': 446,\n",
       " 'right': 447,\n",
       " 'hire': 448,\n",
       " 'advanced': 449,\n",
       " 'remote': 450,\n",
       " 'implementation': 451,\n",
       " 'locations': 452,\n",
       " 'free': 453,\n",
       " 'driving': 454,\n",
       " 'interpersonal': 455,\n",
       " 'primary': 456,\n",
       " 'excel': 457,\n",
       " 'legal': 458,\n",
       " 'better': 459,\n",
       " 'visit': 460,\n",
       " 'growing': 461,\n",
       " 'accordance': 462,\n",
       " 'inclusion': 463,\n",
       " 'innovation': 464,\n",
       " 'supporting': 465,\n",
       " 'want': 466,\n",
       " 'together': 467,\n",
       " 'core': 468,\n",
       " 'facility': 469,\n",
       " 'abilities': 470,\n",
       " 'science': 471,\n",
       " 'believe': 472,\n",
       " 'open': 473,\n",
       " 'cloud': 474,\n",
       " 'records': 475,\n",
       " 'energy': 476,\n",
       " 'oriented': 477,\n",
       " 'dedicated': 478,\n",
       " 'public': 479,\n",
       " 'type': 480,\n",
       " 'collaboration': 481,\n",
       " 'marital': 482,\n",
       " 'brand': 483,\n",
       " 'genetic': 484,\n",
       " 'corporate': 485,\n",
       " 'hour': 486,\n",
       " 'commercial': 487,\n",
       " 'experiences': 488,\n",
       " 'director': 489,\n",
       " '4': 490,\n",
       " 'email': 491,\n",
       " 'retirement': 492,\n",
       " 'date': 493,\n",
       " 'engineer': 494,\n",
       " 'weekly': 495,\n",
       " 'around': 496,\n",
       " 'independently': 497,\n",
       " '100': 498,\n",
       " 'assistant': 499,\n",
       " 'provided': 500,\n",
       " 'bring': 501,\n",
       " 'analytical': 502,\n",
       " 'performs': 503,\n",
       " 'regarding': 504,\n",
       " 'upon': 505,\n",
       " '50': 506,\n",
       " 'attention': 507,\n",
       " 'regulatory': 508,\n",
       " 'managers': 509,\n",
       " 'equity': 510,\n",
       " 'finance': 511,\n",
       " 'facilities': 512,\n",
       " 'drug': 513,\n",
       " 'act': 514,\n",
       " 'continuous': 515,\n",
       " 'excellence': 516,\n",
       " 'track': 517,\n",
       " 'unit': 518,\n",
       " 'objectives': 519,\n",
       " 'class': 520,\n",
       " 'hands': 521,\n",
       " 'hr': 522,\n",
       " 'check': 523,\n",
       " 'communications': 524,\n",
       " 'delivering': 525,\n",
       " 'execution': 526,\n",
       " 'today': 527,\n",
       " '401': 528,\n",
       " 'human': 529,\n",
       " 'firm': 530,\n",
       " 'established': 531,\n",
       " 'expression': 532,\n",
       " 'works': 533,\n",
       " 'focused': 534,\n",
       " 'proven': 535,\n",
       " 'personnel': 536,\n",
       " 'lives': 537,\n",
       " 'events': 538,\n",
       " 'addition': 539,\n",
       " 'prior': 540,\n",
       " 'proficiency': 541,\n",
       " 'unique': 542,\n",
       " 'orders': 543,\n",
       " '6': 544,\n",
       " 'assists': 545,\n",
       " 'stock': 546,\n",
       " 'demonstrated': 547,\n",
       " 'factors': 548,\n",
       " 'get': 549,\n",
       " 'completion': 550,\n",
       " 'regular': 551,\n",
       " 'report': 552,\n",
       " 'multi': 553,\n",
       " 'platform': 554,\n",
       " 'e': 555,\n",
       " 'electrical': 556,\n",
       " 'conduct': 557,\n",
       " 'professionals': 558,\n",
       " 'clean': 559,\n",
       " 'changes': 560,\n",
       " 'standard': 561,\n",
       " 'Manager': 562,\n",
       " 'registered': 563,\n",
       " 'existing': 564,\n",
       " 'passion': 565,\n",
       " 'short': 566,\n",
       " 'benefit': 567,\n",
       " 'integrity': 568,\n",
       " 'closely': 569,\n",
       " 'way': 570,\n",
       " 'accommodations': 571,\n",
       " 'helping': 572,\n",
       " 'subject': 573,\n",
       " 'details': 574,\n",
       " 'total': 575,\n",
       " 'media': 576,\n",
       " 'guidelines': 577,\n",
       " 'reach': 578,\n",
       " 'call': 579,\n",
       " 'military': 580,\n",
       " 'large': 581,\n",
       " 'content': 582,\n",
       " 'administration': 583,\n",
       " 'colleagues': 584,\n",
       " 'monitoring': 585,\n",
       " 'consistent': 586,\n",
       " 'driver': 587,\n",
       " 'credit': 588,\n",
       " 'certified': 589,\n",
       " 'code': 590,\n",
       " 'special': 591,\n",
       " 'reimbursement': 592,\n",
       " 'ensures': 593,\n",
       " 'monthly': 594,\n",
       " 'eligibility': 595,\n",
       " 'positions': 596,\n",
       " 'responsibility': 597,\n",
       " 'merchandise': 598,\n",
       " '30': 599,\n",
       " 'move': 600,\n",
       " 'know': 601,\n",
       " 'tax': 602,\n",
       " 'agency': 603,\n",
       " 'capital': 604,\n",
       " 'certifications': 605,\n",
       " 'requests': 606,\n",
       " 'supervision': 607,\n",
       " 'processing': 608,\n",
       " '8': 609,\n",
       " 'overall': 610,\n",
       " 'share': 611,\n",
       " 'setting': 612,\n",
       " 'start': 613,\n",
       " 'proud': 614,\n",
       " 'executive': 615,\n",
       " 'students': 616,\n",
       " 'pounds': 617,\n",
       " '401k': 618,\n",
       " 'treatment': 619,\n",
       " 'purpose': 620,\n",
       " 'operate': 621,\n",
       " 'line': 622,\n",
       " '7': 623,\n",
       " 'real': 624,\n",
       " 'prepare': 625,\n",
       " 'administrative': 626,\n",
       " 'experienced': 627,\n",
       " 'collaborative': 628,\n",
       " 'different': 629,\n",
       " 'vacation': 630,\n",
       " 'wide': 631,\n",
       " 'proper': 632,\n",
       " 'university': 633,\n",
       " 'valid': 634,\n",
       " 'workforce': 635,\n",
       " 'trends': 636,\n",
       " 'demonstrate': 637,\n",
       " 'america': 638,\n",
       " 'dynamic': 639,\n",
       " 'made': 640,\n",
       " 'options': 641,\n",
       " 'hybrid': 642,\n",
       " 'questions': 643,\n",
       " 'savings': 644,\n",
       " 'considered': 645,\n",
       " 'wellness': 646,\n",
       " 'analyze': 647,\n",
       " 'coordinate': 648,\n",
       " 'outside': 649,\n",
       " 'leaders': 650,\n",
       " 'overview': 651,\n",
       " 'two': 652,\n",
       " 'monitor': 653,\n",
       " 'pre': 654,\n",
       " 'enterprise': 655,\n",
       " 'rn': 656,\n",
       " 'guidance': 657,\n",
       " 'pregnancy': 658,\n",
       " 'provider': 659,\n",
       " 'promote': 660,\n",
       " 'approach': 661,\n",
       " 'mechanical': 662,\n",
       " 'families': 663,\n",
       " 'request': 664,\n",
       " 'months': 665,\n",
       " 'expectations': 666,\n",
       " 'representative': 667,\n",
       " 'supply': 668,\n",
       " 'efficient': 669,\n",
       " 'many': 670,\n",
       " 'would': 671,\n",
       " 'address': 672,\n",
       " 'direction': 673,\n",
       " '(': 674,\n",
       " ')': 675,\n",
       " 'creative': 676,\n",
       " 'tuition': 677,\n",
       " 'affirmative': 678,\n",
       " 'phone': 679,\n",
       " 'infrastructure': 680,\n",
       " 'find': 681,\n",
       " 'ideas': 682,\n",
       " 'schedules': 683,\n",
       " 'close': 684,\n",
       " 'supervisor': 685,\n",
       " 'principles': 686,\n",
       " 'online': 687,\n",
       " 'c': 688,\n",
       " 'ongoing': 689,\n",
       " 'read': 690,\n",
       " 'staffing': 691,\n",
       " 'roles': 692,\n",
       " 'property': 693,\n",
       " 'meeting': 694,\n",
       " 'contribute': 695,\n",
       " 'cash': 696,\n",
       " 'decision': 697,\n",
       " 'live': 698,\n",
       " 'motivated': 699,\n",
       " 'currently': 700,\n",
       " 'characteristic': 701,\n",
       " 'supports': 702,\n",
       " 'reviews': 703,\n",
       " 'active': 704,\n",
       " 'efforts': 705,\n",
       " 'word': 706,\n",
       " 'revenue': 707,\n",
       " 'largest': 708,\n",
       " 'difference': 709,\n",
       " 'successfully': 710,\n",
       " 'scheduling': 711,\n",
       " 'environmental': 712,\n",
       " 'city': 713,\n",
       " 'handling': 714,\n",
       " 'vehicle': 715,\n",
       " 'model': 716,\n",
       " 'execute': 717,\n",
       " 'suite': 718,\n",
       " 'user': 719,\n",
       " 'writing': 720,\n",
       " 'departments': 721,\n",
       " 'applicant': 722,\n",
       " 'techniques': 723,\n",
       " 'north': 724,\n",
       " 'directly': 725,\n",
       " 'according': 726,\n",
       " 'engagement': 727,\n",
       " 'passionate': 728,\n",
       " 'specialist': 729,\n",
       " 'groups': 730,\n",
       " 'demands': 731,\n",
       " 'accuracy': 732,\n",
       " 'possess': 733,\n",
       " 'weekends': 734,\n",
       " 'title': 735,\n",
       " 'come': 736,\n",
       " 'ready': 737,\n",
       " 'occasionally': 738,\n",
       " 'balance': 739,\n",
       " 'government': 740,\n",
       " 'resolve': 741,\n",
       " 'handle': 742,\n",
       " 'feedback': 743,\n",
       " 'progress': 744,\n",
       " 'note': 745,\n",
       " 'click': 746,\n",
       " 'college': 747,\n",
       " 'inc.': 748,\n",
       " 'portfolio': 749,\n",
       " 'challenges': 750,\n",
       " 'parts': 751,\n",
       " 'technician': 752,\n",
       " 'generous': 753,\n",
       " 'next': 754,\n",
       " 'love': 755,\n",
       " '12': 756,\n",
       " 'relationship': 757,\n",
       " 'transportation': 758,\n",
       " 'stand': 759,\n",
       " 'resource': 760,\n",
       " 'vary': 761,\n",
       " 'flexibility': 762,\n",
       " 'enhance': 763,\n",
       " 'document': 764,\n",
       " 'aspects': 765,\n",
       " 'consider': 766,\n",
       " 'budget': 767,\n",
       " 'analytics': 768,\n",
       " 'times': 769,\n",
       " 'integration': 770,\n",
       " 'see': 771,\n",
       " 'become': 772,\n",
       " 'operation': 773,\n",
       " 'via': 774,\n",
       " 'enable': 775,\n",
       " 'master': 776,\n",
       " 'ideal': 777,\n",
       " 'thrive': 778,\n",
       " 'prioritize': 779,\n",
       " 'respect': 780,\n",
       " 'accredited': 781,\n",
       " 'student': 782,\n",
       " 'posting': 783,\n",
       " 'changing': 784,\n",
       " 'proficient': 785,\n",
       " 'recruiting': 786,\n",
       " 'priorities': 787,\n",
       " 'de': 788,\n",
       " 'california': 789,\n",
       " 'recommendations': 790,\n",
       " 'material': 791,\n",
       " 'expected': 792,\n",
       " 'small': 793,\n",
       " 'insights': 794,\n",
       " 'controls': 795,\n",
       " 'everyone': 796,\n",
       " 'rewards': 797,\n",
       " 'eeo': 798,\n",
       " '20': 799,\n",
       " 'enjoy': 800,\n",
       " 'citizenship': 801,\n",
       " 'brands': 802,\n",
       " 'capabilities': 803,\n",
       " 'previous': 804,\n",
       " 'condition': 805,\n",
       " 'case': 806,\n",
       " '40': 807,\n",
       " 'located': 808,\n",
       " 'listed': 809,\n",
       " 'repair': 810,\n",
       " 'physician': 811,\n",
       " 'sick': 812,\n",
       " 'selling': 813,\n",
       " 'preparation': 814,\n",
       " 'tech': 815,\n",
       " 'encourage': 816,\n",
       " 'e.g.': 817,\n",
       " 'consulting': 818,\n",
       " 'rate': 819,\n",
       " 'recruitment': 820,\n",
       " 'thinking': 821,\n",
       " 'oral': 822,\n",
       " 'offered': 823,\n",
       " 'warehouse': 824,\n",
       " 'skill': 825,\n",
       " 'items': 826,\n",
       " 'vendors': 827,\n",
       " 'efficiency': 828,\n",
       " 'expert': 829,\n",
       " 'used': 830,\n",
       " 'leads': 831,\n",
       " 'industrial': 832,\n",
       " 'supplies': 833,\n",
       " 'depending': 834,\n",
       " 'implementing': 835,\n",
       " 'obtain': 836,\n",
       " 'coordination': 837,\n",
       " 'bank': 838,\n",
       " '25': 839,\n",
       " 'emergency': 840,\n",
       " 'go': 841,\n",
       " 'instructions': 842,\n",
       " 'onsite': 843,\n",
       " 'friendly': 844,\n",
       " 'clear': 845,\n",
       " 'assessment': 846,\n",
       " 'assisting': 847,\n",
       " 'guest': 848,\n",
       " 'calls': 849,\n",
       " 'solve': 850,\n",
       " 'parental': 851,\n",
       " 'english': 852,\n",
       " 'scope': 853,\n",
       " 'discounts': 854,\n",
       " 'website': 855,\n",
       " 'outcomes': 856,\n",
       " 'evaluate': 857,\n",
       " 'statement': 858,\n",
       " 'discriminate': 859,\n",
       " 'offering': 860,\n",
       " 'list': 861,\n",
       " 'post': 862,\n",
       " 'keep': 863,\n",
       " 'goal': 864,\n",
       " 'satisfaction': 865,\n",
       " 'interview': 866,\n",
       " 'function': 867,\n",
       " 'number': 868,\n",
       " 'designed': 869,\n",
       " 'resume': 870,\n",
       " 'platforms': 871,\n",
       " 'possible': 872,\n",
       " 'accurately': 873,\n",
       " 'makes': 874,\n",
       " 'web': 875,\n",
       " 'distribution': 876,\n",
       " 'associated': 877,\n",
       " 'providers': 878,\n",
       " 'back': 879,\n",
       " 'always': 880,\n",
       " 'three': 881,\n",
       " 'engineers': 882,\n",
       " 'solution': 883,\n",
       " 'ms': 884,\n",
       " 'performed': 885,\n",
       " 'york': 886,\n",
       " 'trust': 887,\n",
       " 'frequently': 888,\n",
       " 'similar': 889,\n",
       " 'specifications': 890,\n",
       " 'organizations': 891,\n",
       " 'feel': 892,\n",
       " 'sql': 893,\n",
       " 'fun': 894,\n",
       " 'major': 895,\n",
       " 'ancestry': 896,\n",
       " 'co': 897,\n",
       " 'regional': 898,\n",
       " 'pto': 899,\n",
       " 'educational': 900,\n",
       " 'establish': 901,\n",
       " 'five': 902,\n",
       " 'discrimination': 903,\n",
       " 'forward': 904,\n",
       " 'database': 905,\n",
       " 'standing': 906,\n",
       " 'agencies': 907,\n",
       " 'completed': 908,\n",
       " 'hourly': 909,\n",
       " 'lbs': 910,\n",
       " 'exciting': 911,\n",
       " 'criminal': 912,\n",
       " 'licensed': 913,\n",
       " 'automation': 914,\n",
       " 'international': 915,\n",
       " 'matter': 916,\n",
       " 'guests': 917,\n",
       " 'matters': 918,\n",
       " 'investment': 919,\n",
       " 'board': 920,\n",
       " 'centers': 921,\n",
       " 'troubleshooting': 922,\n",
       " 'scale': 923,\n",
       " 'starting': 924,\n",
       " 'important': 925,\n",
       " 'identifying': 926,\n",
       " 'incentive': 927,\n",
       " 'purchase': 928,\n",
       " 'oversee': 929,\n",
       " 'engage': 930,\n",
       " 'highest': 931,\n",
       " 'coaching': 932,\n",
       " 'submit': 933,\n",
       " 'history': 934,\n",
       " 'metrics': 935,\n",
       " 'organized': 936,\n",
       " 'quickly': 937,\n",
       " 'space': 938,\n",
       " 'participates': 939,\n",
       " 'architecture': 940,\n",
       " 'availability': 941,\n",
       " 'detailed': 942,\n",
       " 'actively': 943,\n",
       " 'Engineer': 944,\n",
       " 'met': 945,\n",
       " 'task': 946,\n",
       " 'serving': 947,\n",
       " 'verify': 948,\n",
       " 'determine': 949,\n",
       " 'chain': 950,\n",
       " 'play': 951,\n",
       " 'language': 952,\n",
       " 'deadlines': 953,\n",
       " 'demonstrates': 954,\n",
       " 'methods': 955,\n",
       " 'develops': 956,\n",
       " 'regularly': 957,\n",
       " 'along': 958,\n",
       " 'front': 959,\n",
       " 'entry': 960,\n",
       " 'careers': 961,\n",
       " 'outstanding': 962,\n",
       " '2024': 963,\n",
       " 'applying': 964,\n",
       " 'children': 965,\n",
       " 'desired': 966,\n",
       " 'agile': 967,\n",
       " 'interested': 968,\n",
       " 'floor': 969,\n",
       " 'recognized': 970,\n",
       " 'fully': 971,\n",
       " 'advancement': 972,\n",
       " 'plant': 973,\n",
       " 'presentation': 974,\n",
       " 'foster': 975,\n",
       " 'american': 976,\n",
       " 'analyst': 977,\n",
       " 'auto': 978,\n",
       " 'privacy': 979,\n",
       " 'spending': 980,\n",
       " 'consumer': 981,\n",
       " 'assignments': 982,\n",
       " 'partnership': 983,\n",
       " 'referral': 984,\n",
       " 'ai': 985,\n",
       " 'models': 986,\n",
       " 'therapy': 987,\n",
       " 'willing': 988,\n",
       " 'businesses': 989,\n",
       " 'comfortable': 990,\n",
       " 'components': 991,\n",
       " 'walk': 992,\n",
       " 'advance': 993,\n",
       " 'veterans': 994,\n",
       " 'programming': 995,\n",
       " 'protection': 996,\n",
       " 'due': 997,\n",
       " 'shifts': 998,\n",
       " 'hand': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_vocab, tokenized_vocab_size = get_pruned_vocab_tokenized(tokenized_df_train, \"merged_tokenized\", top_k=50000)\n",
    "tokenized_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.37it/s]\n",
      "100%|██████████| 98/98 [00:03<00:00, 27.08it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_embeddings_cache(vocab, embedding_model):\n",
    "    token_to_embedding = {}\n",
    "    # Process tokens in batches for efficiency\n",
    "    all_tokens = list(vocab.keys())\n",
    "    batch_size = 512\n",
    "    \n",
    "    for i in trange(0, len(all_tokens), batch_size):\n",
    "        batch = all_tokens[i:i+batch_size]\n",
    "        # Embed each token individually\n",
    "        embeddings = embedding_model.encode(batch, batch_size=batch_size)\n",
    "        \n",
    "        for j, token in enumerate(batch):\n",
    "            token_to_embedding[token] = embeddings[j]\n",
    "    \n",
    "    return token_to_embedding\n",
    "\n",
    "import pickle\n",
    "token_to_embedding = create_embeddings_cache(tokenized_vocab, embedding_model)\n",
    "with open('token_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(token_to_embedding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_values):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_values, label='Training Loss')\n",
    "    plt.axhline(0, color='red', linestyle='--', label='Zero Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    # Annotate the final loss value\n",
    "    final_loss = loss_values[-1]\n",
    "    plt.annotate(f'Final Loss: {final_loss:.2f}', xy=(len(loss_values)-1, final_loss),\n",
    "                 xytext=(len(loss_values)-1, final_loss+0.5),\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "                 fontsize=10, color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, df, vocab, input_dim, hidden_dim, output_dim, lr):\n",
    "        \"\"\"\n",
    "        Initialize the FeedForwardNN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        logging.info(f\"Initializing FeedForwardNN with input_dim: {input_dim}, hidden_dim: {hidden_dim}, output_dim: {output_dim}\")\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.last_window_tokens_index = 0\n",
    "        self.embedding_transformer = embedding_model\n",
    "        self.model = self.init_model()\n",
    "        self.N = 3\n",
    "        self.vocab = vocab\n",
    "        self.use_precomputed = True\n",
    "        self.token_to_embedding = token_to_embedding\n",
    "\n",
    "    def _get_vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of the vocabulary\n",
    "        \"\"\"\n",
    "        if self.vocab is None:\n",
    "            raise ValueError(\"Vocabulary is not initialized\")\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def _get_context_windows_batched(self, training_set: pd.DataFrame, N: int, batch_size: int):\n",
    "        \"\"\"\n",
    "        Generator that yields batches of (context_window, next_token) pairs across the dataset,\n",
    "        lazily building batches to avoid memory overload.\n",
    "        \"\"\"\n",
    "        logging.info(f\"..... Creating context windows .....\")\n",
    "        \n",
    "        batch_context_windows = []\n",
    "\n",
    "        for _, row in training_set.iterrows():\n",
    "            tokens = row[\"merged\"].split(\" \")\n",
    "            if len(tokens) <= N:\n",
    "                continue  # skip rows that are too short\n",
    "            for i in range(len(tokens) - N):\n",
    "                context_window = tokens[i:i+N]\n",
    "                next_token = tokens[i+N]\n",
    "                batch_context_windows.append((context_window, next_token))\n",
    "\n",
    "                if len(batch_context_windows) == batch_size:\n",
    "                    yield batch_context_windows\n",
    "                    batch_context_windows = []\n",
    "\n",
    "        if batch_context_windows:\n",
    "            yield batch_context_windows  # Yield last partial batch\n",
    "\n",
    "        logging.info(f\"..... Finished yielding context windows .....\")\n",
    "\n",
    "            \n",
    "    def init_model(self):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        )\n",
    "\n",
    "    def _embed_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Embed the tokens using pre-computed embeddings if available,\n",
    "        or on-the-fly otherwise.\n",
    "        \"\"\"\n",
    "        if self.use_precomputed:\n",
    "            # Use pre-computed embeddings\n",
    "            embeddings = []\n",
    "            for token in tokens:\n",
    "                if token in self.token_to_embedding:\n",
    "                    embeddings.append(self.token_to_embedding[token])\n",
    "                elif '<UNK>' in self.token_to_embedding:\n",
    "                    embeddings.append(self.token_to_embedding['<UNK>'])\n",
    "                else:\n",
    "                    # Fall back to computing it if not found\n",
    "                    embedding = self.embedding_transformer.encode(token, show_progress_bar=False)\n",
    "                    embeddings.append(embedding)\n",
    "            \n",
    "            # Average the embeddings for the context window\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            # Original method - compute on the fly\n",
    "            sentence_ = \" \".join(tokens)\n",
    "            return self.embedding_transformer.encode(sentence_, batch_size=32, show_progress_bar=False)\n",
    "\n",
    "\n",
    "    def fit(self, training_set: pd.DataFrame, batch_size=32, epochs=10, device='cpu'):\n",
    "        \"\"\"\n",
    "        Train the FFNN using transformer embeddings as input, processing one context window at a time.\n",
    "        \"\"\"\n",
    "        logging.info(f\"..... Training started .....\")\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        self.model = self.model.to(device)  # Move model to the appropriate device\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        loss_values = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            logging.info(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            batch_gen = self._get_context_windows_batched(training_set, self.N, batch_size)\n",
    "            batch_idx = 0\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "\n",
    "            for batch in batch_gen:\n",
    "                X_batch = []\n",
    "                y_batch = []\n",
    "\n",
    "                for context_window, next_token in batch:\n",
    "                    embedding = self._embed_tokens(context_window)\n",
    "                    X_batch.append(embedding)\n",
    "                    y_batch.append(self.vocab.get(next_token, self.vocab.get('<UNK>', 0)))\n",
    "\n",
    "                X_np = np.array(X_batch)\n",
    "                X_tensor = torch.tensor(X_np, dtype=torch.float32, device=device)\n",
    "                y_tensor = torch.tensor(y_batch, dtype=torch.long, device=device)\n",
    "\n",
    "                # This is important because it resets the gradients of previous batches to 0 it allows to compute the gradient for the current batch\n",
    "                # If we don't do this, the gradients will be accumulated and the model will not learn correctly\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(X_tensor)\n",
    "                loss = loss_fn(logits, y_tensor)\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                loss.backward()\n",
    "                # This is used to update the weights of the model\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_idx += 1\n",
    "                if batch_idx % 100 == 0:\n",
    "                    logging.info(f\"Epoch {epoch+1}, Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "\n",
    "            avg_epoch_loss = total_loss / num_batches\n",
    "            loss_values.append(avg_epoch_loss)\n",
    "            logging.info(f\"Epoch {epoch+1} completed.\")\n",
    "        \n",
    "        return loss_values\n",
    "            \n",
    "    \n",
    "    def predict_token(self, context_window, temperature):\n",
    "        \"\"\"\n",
    "        Predict the next token using the model.\n",
    "        context_window: list of tokens (strings)\n",
    "        \"\"\"\n",
    "        embedding = self._embed_tokens(context_window)\n",
    "        device = next(self.model.parameters()).device  # Get device from model\n",
    "        embedding_tensor = torch.tensor(embedding, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        logits = self.model(embedding_tensor)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        return next_token_idx\n",
    "\n",
    "    def generate_job_description(self, context_window, max_length, temperature):\n",
    "        \"\"\"\n",
    "        Generate a job description using the model.\n",
    "        context_window: list of tokens (strings), length N\n",
    "        \"\"\"\n",
    "        generated_tokens = []\n",
    "        idx_to_token = {idx: token for token, idx in self.vocab.items()}\n",
    "        for _ in range(max_length):\n",
    "            next_token_idx = self.predict_token(context_window, temperature)\n",
    "            next_token = idx_to_token.get(next_token_idx, '<UNK>')\n",
    "            generated_tokens.append(next_token)\n",
    "            context_window = context_window[1:] + [next_token]  # Slide window\n",
    "            if next_token == '<END>':\n",
    "                break\n",
    "        return ' '.join(generated_tokens)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 08:48:22,696 - INFO - True\n",
      "2025-05-07 08:48:22,697 - INFO - ..... Device: cuda .....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu126\n",
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(torch.cuda.is_available())\n",
    "logging.info(f\"..... Device: {device} .....\")\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 08:48:22,708 - INFO - True\n",
      "2025-05-07 08:48:22,709 - INFO - ..... Device: cuda .....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(torch.cuda.is_available())\n",
    "logging.info(f\"..... Device: {device} .....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 13:57:38,570 - INFO - True\n",
      "2025-05-03 13:57:38,571 - INFO - ..... Device: cuda .....\n",
      "2025-05-03 13:57:38,572 - INFO - Initializing FeedForwardNN with input_dim: 384, hidden_dim: 512, output_dim: 50001\n",
      "2025-05-03 13:57:38,670 - INFO - ..... Training started .....\n",
      "2025-05-03 13:57:38,680 - INFO - Epoch 1/5\n",
      "2025-05-03 13:57:38,681 - INFO - ..... Creating context windows .....\n",
      "2025-05-03 13:58:05,553 - INFO - Epoch 1, Batch 100: Loss = 7.3971\n",
      "2025-05-03 13:58:32,537 - INFO - Epoch 1, Batch 200: Loss = 6.9517\n",
      "2025-05-03 13:58:59,519 - INFO - Epoch 1, Batch 300: Loss = 6.8243\n",
      "2025-05-03 13:59:26,476 - INFO - Epoch 1, Batch 400: Loss = 6.7934\n",
      "2025-05-03 13:59:53,426 - INFO - Epoch 1, Batch 500: Loss = 6.5846\n",
      "2025-05-03 14:00:20,394 - INFO - Epoch 1, Batch 600: Loss = 6.4375\n",
      "2025-05-03 14:00:47,544 - INFO - Epoch 1, Batch 700: Loss = 6.3515\n",
      "2025-05-03 14:01:14,493 - INFO - Epoch 1, Batch 800: Loss = 6.3370\n",
      "2025-05-03 14:01:41,431 - INFO - Epoch 1, Batch 900: Loss = 6.1912\n",
      "2025-05-03 14:02:08,261 - INFO - Epoch 1, Batch 1000: Loss = 6.0937\n",
      "2025-05-03 14:02:35,163 - INFO - Epoch 1, Batch 1100: Loss = 6.1475\n",
      "2025-05-03 14:03:02,141 - INFO - Epoch 1, Batch 1200: Loss = 6.0610\n",
      "2025-05-03 14:03:29,115 - INFO - Epoch 1, Batch 1300: Loss = 5.9447\n",
      "2025-05-03 14:03:56,065 - INFO - Epoch 1, Batch 1400: Loss = 6.0353\n",
      "2025-05-03 14:04:22,954 - INFO - Epoch 1, Batch 1500: Loss = 5.8732\n",
      "2025-05-03 14:04:49,803 - INFO - Epoch 1, Batch 1600: Loss = 6.0487\n",
      "2025-05-03 14:05:16,821 - INFO - Epoch 1, Batch 1700: Loss = 5.8591\n",
      "2025-05-03 14:05:43,969 - INFO - Epoch 1, Batch 1800: Loss = 5.7109\n",
      "2025-05-03 14:06:10,897 - INFO - Epoch 1, Batch 1900: Loss = 5.4906\n",
      "2025-05-03 14:06:37,810 - INFO - Epoch 1, Batch 2000: Loss = 5.6182\n",
      "2025-05-03 14:07:04,766 - INFO - Epoch 1, Batch 2100: Loss = 5.7639\n",
      "2025-05-03 14:07:31,632 - INFO - Epoch 1, Batch 2200: Loss = 5.5462\n",
      "2025-05-03 14:07:58,507 - INFO - Epoch 1, Batch 2300: Loss = 5.6881\n",
      "2025-05-03 14:08:25,417 - INFO - Epoch 1, Batch 2400: Loss = 5.9506\n",
      "2025-05-03 14:08:52,306 - INFO - Epoch 1, Batch 2500: Loss = 5.6511\n",
      "2025-05-03 14:09:19,152 - INFO - Epoch 1, Batch 2600: Loss = 5.3813\n",
      "2025-05-03 14:09:46,015 - INFO - Epoch 1, Batch 2700: Loss = 5.6107\n",
      "2025-05-03 14:10:12,951 - INFO - Epoch 1, Batch 2800: Loss = 5.5534\n",
      "2025-05-03 14:10:40,124 - INFO - Epoch 1, Batch 2900: Loss = 5.4222\n",
      "2025-05-03 14:11:07,113 - INFO - Epoch 1, Batch 3000: Loss = 5.2655\n",
      "2025-05-03 14:11:34,041 - INFO - Epoch 1, Batch 3100: Loss = 5.3847\n",
      "2025-05-03 14:12:00,948 - INFO - Epoch 1, Batch 3200: Loss = 5.4442\n",
      "2025-05-03 14:12:27,914 - INFO - Epoch 1, Batch 3300: Loss = 5.4476\n",
      "2025-05-03 14:12:54,909 - INFO - Epoch 1, Batch 3400: Loss = 5.2792\n",
      "2025-05-03 14:13:21,838 - INFO - Epoch 1, Batch 3500: Loss = 5.5970\n",
      "2025-05-03 14:13:48,046 - INFO - Epoch 1, Batch 3600: Loss = 5.1017\n",
      "2025-05-03 14:14:13,929 - INFO - Epoch 1, Batch 3700: Loss = 4.9057\n",
      "2025-05-03 14:14:39,828 - INFO - Epoch 1, Batch 3800: Loss = 5.5642\n",
      "2025-05-03 14:15:05,729 - INFO - Epoch 1, Batch 3900: Loss = 5.0271\n",
      "2025-05-03 14:15:31,830 - INFO - Epoch 1, Batch 4000: Loss = 4.9680\n",
      "2025-05-03 14:15:57,709 - INFO - Epoch 1, Batch 4100: Loss = 5.2240\n",
      "2025-05-03 14:16:23,585 - INFO - Epoch 1, Batch 4200: Loss = 5.3298\n",
      "2025-05-03 14:16:49,455 - INFO - Epoch 1, Batch 4300: Loss = 5.2519\n",
      "2025-05-03 14:17:15,298 - INFO - Epoch 1, Batch 4400: Loss = 5.2872\n",
      "2025-05-03 14:17:41,148 - INFO - Epoch 1, Batch 4500: Loss = 5.1980\n",
      "2025-05-03 14:18:07,004 - INFO - Epoch 1, Batch 4600: Loss = 5.1521\n",
      "2025-05-03 14:18:32,876 - INFO - Epoch 1, Batch 4700: Loss = 5.3747\n",
      "2025-05-03 14:18:58,767 - INFO - Epoch 1, Batch 4800: Loss = 5.2563\n",
      "2025-05-03 14:19:24,652 - INFO - Epoch 1, Batch 4900: Loss = 4.6545\n",
      "2025-05-03 14:19:50,531 - INFO - Epoch 1, Batch 5000: Loss = 4.9437\n",
      "2025-05-03 14:20:16,610 - INFO - Epoch 1, Batch 5100: Loss = 5.5463\n",
      "2025-05-03 14:20:42,480 - INFO - Epoch 1, Batch 5200: Loss = 5.0457\n",
      "2025-05-03 14:21:08,351 - INFO - Epoch 1, Batch 5300: Loss = 5.0059\n",
      "2025-05-03 14:21:27,220 - INFO - ..... Finished yielding context windows .....\n",
      "2025-05-03 14:21:27,221 - INFO - Epoch 1 completed.\n",
      "2025-05-03 14:21:27,221 - INFO - Epoch 2/5\n",
      "2025-05-03 14:21:27,221 - INFO - ..... Creating context windows .....\n",
      "2025-05-03 14:21:53,270 - INFO - Epoch 2, Batch 100: Loss = 5.3013\n",
      "2025-05-03 14:22:19,147 - INFO - Epoch 2, Batch 200: Loss = 4.8537\n",
      "2025-05-03 14:22:45,022 - INFO - Epoch 2, Batch 300: Loss = 5.0647\n",
      "2025-05-03 14:23:10,900 - INFO - Epoch 2, Batch 400: Loss = 5.1966\n",
      "2025-05-03 14:23:36,800 - INFO - Epoch 2, Batch 500: Loss = 5.2198\n",
      "2025-05-03 14:24:02,663 - INFO - Epoch 2, Batch 600: Loss = 5.0621\n",
      "2025-05-03 14:24:28,539 - INFO - Epoch 2, Batch 700: Loss = 4.9179\n",
      "2025-05-03 14:24:54,431 - INFO - Epoch 2, Batch 800: Loss = 5.2863\n",
      "2025-05-03 14:25:20,330 - INFO - Epoch 2, Batch 900: Loss = 4.8857\n",
      "2025-05-03 14:25:46,244 - INFO - Epoch 2, Batch 1000: Loss = 4.9700\n",
      "2025-05-03 14:26:12,123 - INFO - Epoch 2, Batch 1100: Loss = 5.0784\n",
      "2025-05-03 14:26:38,029 - INFO - Epoch 2, Batch 1200: Loss = 5.1880\n",
      "2025-05-03 14:27:03,931 - INFO - Epoch 2, Batch 1300: Loss = 5.0274\n",
      "2025-05-03 14:27:29,823 - INFO - Epoch 2, Batch 1400: Loss = 5.2481\n",
      "2025-05-03 14:27:55,727 - INFO - Epoch 2, Batch 1500: Loss = 4.9525\n",
      "2025-05-03 14:28:21,608 - INFO - Epoch 2, Batch 1600: Loss = 5.3274\n",
      "2025-05-03 14:28:47,484 - INFO - Epoch 2, Batch 1700: Loss = 5.0505\n",
      "2025-05-03 14:29:13,381 - INFO - Epoch 2, Batch 1800: Loss = 4.6375\n",
      "2025-05-03 14:29:39,260 - INFO - Epoch 2, Batch 1900: Loss = 4.7598\n",
      "2025-05-03 14:30:05,130 - INFO - Epoch 2, Batch 2000: Loss = 4.8197\n",
      "2025-05-03 14:30:31,017 - INFO - Epoch 2, Batch 2100: Loss = 5.1115\n",
      "2025-05-03 14:30:56,898 - INFO - Epoch 2, Batch 2200: Loss = 4.7958\n",
      "2025-05-03 14:31:22,787 - INFO - Epoch 2, Batch 2300: Loss = 5.0716\n",
      "2025-05-03 14:31:48,655 - INFO - Epoch 2, Batch 2400: Loss = 5.4437\n",
      "2025-05-03 14:32:14,971 - INFO - Epoch 2, Batch 2500: Loss = 5.1818\n",
      "2025-05-03 14:32:42,058 - INFO - Epoch 2, Batch 2600: Loss = 4.7911\n",
      "2025-05-03 14:33:08,421 - INFO - Epoch 2, Batch 2700: Loss = 5.0723\n",
      "2025-05-03 14:33:34,356 - INFO - Epoch 2, Batch 2800: Loss = 5.0740\n",
      "2025-05-03 14:34:00,276 - INFO - Epoch 2, Batch 2900: Loss = 4.8611\n",
      "2025-05-03 14:34:26,209 - INFO - Epoch 2, Batch 3000: Loss = 4.7645\n",
      "2025-05-03 14:34:52,137 - INFO - Epoch 2, Batch 3100: Loss = 4.8981\n",
      "2025-05-03 14:35:18,083 - INFO - Epoch 2, Batch 3200: Loss = 4.9700\n",
      "2025-05-03 14:35:44,009 - INFO - Epoch 2, Batch 3300: Loss = 5.0678\n",
      "2025-05-03 14:36:10,015 - INFO - Epoch 2, Batch 3400: Loss = 4.8636\n",
      "2025-05-03 14:36:36,057 - INFO - Epoch 2, Batch 3500: Loss = 5.2375\n",
      "2025-05-03 14:37:02,175 - INFO - Epoch 2, Batch 3600: Loss = 4.6816\n",
      "2025-05-03 14:37:28,082 - INFO - Epoch 2, Batch 3700: Loss = 4.3710\n",
      "2025-05-03 14:37:54,000 - INFO - Epoch 2, Batch 3800: Loss = 5.1979\n",
      "2025-05-03 14:38:19,955 - INFO - Epoch 2, Batch 3900: Loss = 4.6115\n",
      "2025-05-03 14:38:45,846 - INFO - Epoch 2, Batch 4000: Loss = 4.5272\n",
      "2025-05-03 14:39:11,761 - INFO - Epoch 2, Batch 4100: Loss = 4.8925\n",
      "2025-05-03 14:39:37,675 - INFO - Epoch 2, Batch 4200: Loss = 4.9822\n",
      "2025-05-03 14:40:03,580 - INFO - Epoch 2, Batch 4300: Loss = 4.8912\n",
      "2025-05-03 14:40:29,482 - INFO - Epoch 2, Batch 4400: Loss = 4.9474\n",
      "2025-05-03 14:40:55,391 - INFO - Epoch 2, Batch 4500: Loss = 4.8631\n",
      "2025-05-03 14:41:21,289 - INFO - Epoch 2, Batch 4600: Loss = 4.7622\n",
      "2025-05-03 14:41:47,195 - INFO - Epoch 2, Batch 4700: Loss = 5.0595\n",
      "2025-05-03 14:42:13,117 - INFO - Epoch 2, Batch 4800: Loss = 4.9347\n",
      "2025-05-03 14:42:39,037 - INFO - Epoch 2, Batch 4900: Loss = 4.2782\n",
      "2025-05-03 14:43:04,930 - INFO - Epoch 2, Batch 5000: Loss = 4.6085\n",
      "2025-05-03 14:43:30,804 - INFO - Epoch 2, Batch 5100: Loss = 5.3022\n",
      "2025-05-03 14:43:56,702 - INFO - Epoch 2, Batch 5200: Loss = 4.7567\n",
      "2025-05-03 14:44:22,580 - INFO - Epoch 2, Batch 5300: Loss = 4.7095\n",
      "2025-05-03 14:44:41,543 - INFO - ..... Finished yielding context windows .....\n",
      "2025-05-03 14:44:41,543 - INFO - Epoch 2 completed.\n",
      "2025-05-03 14:44:41,543 - INFO - Epoch 3/5\n",
      "2025-05-03 14:44:41,544 - INFO - ..... Creating context windows .....\n",
      "2025-05-03 14:45:07,408 - INFO - Epoch 3, Batch 100: Loss = 5.0260\n",
      "2025-05-03 14:45:33,285 - INFO - Epoch 3, Batch 200: Loss = 4.6025\n",
      "2025-05-03 14:45:59,153 - INFO - Epoch 3, Batch 300: Loss = 4.8141\n",
      "2025-05-03 14:46:25,010 - INFO - Epoch 3, Batch 400: Loss = 4.9558\n",
      "2025-05-03 14:46:50,873 - INFO - Epoch 3, Batch 500: Loss = 4.9640\n",
      "2025-05-03 14:47:16,761 - INFO - Epoch 3, Batch 600: Loss = 4.8281\n",
      "2025-05-03 14:47:42,642 - INFO - Epoch 3, Batch 700: Loss = 4.6661\n",
      "2025-05-03 14:48:08,501 - INFO - Epoch 3, Batch 800: Loss = 5.0627\n",
      "2025-05-03 14:48:34,366 - INFO - Epoch 3, Batch 900: Loss = 4.6260\n",
      "2025-05-03 14:49:00,227 - INFO - Epoch 3, Batch 1000: Loss = 4.7244\n",
      "2025-05-03 14:49:26,105 - INFO - Epoch 3, Batch 1100: Loss = 4.8444\n",
      "2025-05-03 14:49:51,975 - INFO - Epoch 3, Batch 1200: Loss = 4.9901\n",
      "2025-05-03 14:50:17,859 - INFO - Epoch 3, Batch 1300: Loss = 4.8103\n",
      "2025-05-03 14:50:43,749 - INFO - Epoch 3, Batch 1400: Loss = 5.0551\n",
      "2025-05-03 14:51:09,618 - INFO - Epoch 3, Batch 1500: Loss = 4.7411\n",
      "2025-05-03 14:51:35,492 - INFO - Epoch 3, Batch 1600: Loss = 5.1479\n",
      "2025-05-03 14:52:01,353 - INFO - Epoch 3, Batch 1700: Loss = 4.8483\n",
      "2025-05-03 14:52:27,211 - INFO - Epoch 3, Batch 1800: Loss = 4.3651\n",
      "2025-05-03 14:52:53,056 - INFO - Epoch 3, Batch 1900: Loss = 4.5750\n",
      "2025-05-03 14:53:18,952 - INFO - Epoch 3, Batch 2000: Loss = 4.6216\n",
      "2025-05-03 14:53:44,822 - INFO - Epoch 3, Batch 2100: Loss = 4.9288\n",
      "2025-05-03 14:54:10,682 - INFO - Epoch 3, Batch 2200: Loss = 4.5832\n",
      "2025-05-03 14:54:36,557 - INFO - Epoch 3, Batch 2300: Loss = 4.8815\n",
      "2025-05-03 14:55:02,425 - INFO - Epoch 3, Batch 2400: Loss = 5.2853\n",
      "2025-05-03 14:55:28,297 - INFO - Epoch 3, Batch 2500: Loss = 5.0357\n",
      "2025-05-03 14:55:54,170 - INFO - Epoch 3, Batch 2600: Loss = 4.6081\n",
      "2025-05-03 14:56:20,052 - INFO - Epoch 3, Batch 2700: Loss = 4.9048\n",
      "2025-05-03 14:56:45,919 - INFO - Epoch 3, Batch 2800: Loss = 4.9217\n",
      "2025-05-03 14:57:11,785 - INFO - Epoch 3, Batch 2900: Loss = 4.6757\n",
      "2025-05-03 14:57:37,651 - INFO - Epoch 3, Batch 3000: Loss = 4.6000\n",
      "2025-05-03 14:58:03,517 - INFO - Epoch 3, Batch 3100: Loss = 4.7336\n",
      "2025-05-03 14:58:29,548 - INFO - Epoch 3, Batch 3200: Loss = 4.8037\n",
      "2025-05-03 14:58:55,520 - INFO - Epoch 3, Batch 3300: Loss = 4.9250\n",
      "2025-05-03 14:59:21,596 - INFO - Epoch 3, Batch 3400: Loss = 4.7242\n",
      "2025-05-03 14:59:47,469 - INFO - Epoch 3, Batch 3500: Loss = 5.1042\n",
      "2025-05-03 15:00:13,380 - INFO - Epoch 3, Batch 3600: Loss = 4.5216\n",
      "2025-05-03 15:00:39,260 - INFO - Epoch 3, Batch 3700: Loss = 4.1756\n",
      "2025-05-03 15:01:05,143 - INFO - Epoch 3, Batch 3800: Loss = 5.0502\n",
      "2025-05-03 15:01:31,042 - INFO - Epoch 3, Batch 3900: Loss = 4.4531\n",
      "2025-05-03 15:01:56,900 - INFO - Epoch 3, Batch 4000: Loss = 4.3524\n",
      "2025-05-03 15:02:22,745 - INFO - Epoch 3, Batch 4100: Loss = 4.7611\n",
      "2025-05-03 15:02:48,595 - INFO - Epoch 3, Batch 4200: Loss = 4.8413\n",
      "2025-05-03 15:03:14,465 - INFO - Epoch 3, Batch 4300: Loss = 4.7393\n",
      "2025-05-03 15:03:40,332 - INFO - Epoch 3, Batch 4400: Loss = 4.8066\n",
      "2025-05-03 15:04:06,199 - INFO - Epoch 3, Batch 4500: Loss = 4.7213\n",
      "2025-05-03 15:04:32,063 - INFO - Epoch 3, Batch 4600: Loss = 4.5988\n",
      "2025-05-03 15:04:57,948 - INFO - Epoch 3, Batch 4700: Loss = 4.9260\n",
      "2025-05-03 15:05:23,805 - INFO - Epoch 3, Batch 4800: Loss = 4.8018\n",
      "2025-05-03 15:05:49,678 - INFO - Epoch 3, Batch 4900: Loss = 4.1210\n",
      "2025-05-03 15:06:15,561 - INFO - Epoch 3, Batch 5000: Loss = 4.4603\n",
      "2025-05-03 15:06:41,433 - INFO - Epoch 3, Batch 5100: Loss = 5.1912\n",
      "2025-05-03 15:07:07,322 - INFO - Epoch 3, Batch 5200: Loss = 4.6299\n",
      "2025-05-03 15:07:33,206 - INFO - Epoch 3, Batch 5300: Loss = 4.5729\n",
      "2025-05-03 15:07:52,181 - INFO - ..... Finished yielding context windows .....\n",
      "2025-05-03 15:07:52,182 - INFO - Epoch 3 completed.\n",
      "2025-05-03 15:07:52,182 - INFO - Epoch 4/5\n",
      "2025-05-03 15:07:52,182 - INFO - ..... Creating context windows .....\n",
      "2025-05-03 15:08:18,040 - INFO - Epoch 4, Batch 100: Loss = 4.8984\n",
      "2025-05-03 15:08:43,923 - INFO - Epoch 4, Batch 200: Loss = 4.4869\n",
      "2025-05-03 15:09:09,805 - INFO - Epoch 4, Batch 300: Loss = 4.6968\n",
      "2025-05-03 15:09:35,704 - INFO - Epoch 4, Batch 400: Loss = 4.8429\n",
      "2025-05-03 15:10:01,613 - INFO - Epoch 4, Batch 500: Loss = 4.8403\n",
      "2025-05-03 15:10:27,527 - INFO - Epoch 4, Batch 600: Loss = 4.7154\n",
      "2025-05-03 15:10:53,450 - INFO - Epoch 4, Batch 700: Loss = 4.5545\n",
      "2025-05-03 15:11:19,369 - INFO - Epoch 4, Batch 800: Loss = 4.9512\n",
      "2025-05-03 15:11:45,243 - INFO - Epoch 4, Batch 900: Loss = 4.5020\n",
      "2025-05-03 15:12:11,103 - INFO - Epoch 4, Batch 1000: Loss = 4.6038\n",
      "2025-05-03 15:12:37,016 - INFO - Epoch 4, Batch 1100: Loss = 4.7287\n",
      "2025-05-03 15:13:02,890 - INFO - Epoch 4, Batch 1200: Loss = 4.8944\n",
      "2025-05-03 15:13:28,770 - INFO - Epoch 4, Batch 1300: Loss = 4.6971\n",
      "2025-05-03 15:13:54,669 - INFO - Epoch 4, Batch 1400: Loss = 4.9605\n",
      "2025-05-03 15:14:20,525 - INFO - Epoch 4, Batch 1500: Loss = 4.6365\n",
      "2025-05-03 15:14:46,411 - INFO - Epoch 4, Batch 1600: Loss = 5.0538\n",
      "2025-05-03 15:15:12,275 - INFO - Epoch 4, Batch 1700: Loss = 4.7442\n",
      "2025-05-03 15:15:38,148 - INFO - Epoch 4, Batch 1800: Loss = 4.2245\n",
      "2025-05-03 15:16:04,020 - INFO - Epoch 4, Batch 1900: Loss = 4.4805\n",
      "2025-05-03 15:16:29,870 - INFO - Epoch 4, Batch 2000: Loss = 4.5313\n",
      "2025-05-03 15:16:55,764 - INFO - Epoch 4, Batch 2100: Loss = 4.8351\n",
      "2025-05-03 15:17:21,626 - INFO - Epoch 4, Batch 2200: Loss = 4.4701\n",
      "2025-05-03 15:17:47,556 - INFO - Epoch 4, Batch 2300: Loss = 4.7804\n",
      "2025-05-03 15:18:13,438 - INFO - Epoch 4, Batch 2400: Loss = 5.1992\n",
      "2025-05-03 15:18:39,298 - INFO - Epoch 4, Batch 2500: Loss = 4.9557\n",
      "2025-05-03 15:19:05,182 - INFO - Epoch 4, Batch 2600: Loss = 4.5050\n",
      "2025-05-03 15:19:31,133 - INFO - Epoch 4, Batch 2700: Loss = 4.8182\n",
      "2025-05-03 15:19:56,993 - INFO - Epoch 4, Batch 2800: Loss = 4.8373\n",
      "2025-05-03 15:20:22,956 - INFO - Epoch 4, Batch 2900: Loss = 4.5726\n",
      "2025-05-03 15:20:48,937 - INFO - Epoch 4, Batch 3000: Loss = 4.5116\n",
      "2025-05-03 15:21:15,033 - INFO - Epoch 4, Batch 3100: Loss = 4.6448\n",
      "2025-05-03 15:21:40,917 - INFO - Epoch 4, Batch 3200: Loss = 4.7123\n",
      "2025-05-03 15:22:06,784 - INFO - Epoch 4, Batch 3300: Loss = 4.8441\n",
      "2025-05-03 15:22:32,696 - INFO - Epoch 4, Batch 3400: Loss = 4.6445\n",
      "2025-05-03 15:22:58,562 - INFO - Epoch 4, Batch 3500: Loss = 5.0302\n",
      "2025-05-03 15:23:24,433 - INFO - Epoch 4, Batch 3600: Loss = 4.4322\n",
      "2025-05-03 15:23:50,311 - INFO - Epoch 4, Batch 3700: Loss = 4.0674\n",
      "2025-05-03 15:24:16,203 - INFO - Epoch 4, Batch 3800: Loss = 4.9700\n",
      "2025-05-03 15:24:42,066 - INFO - Epoch 4, Batch 3900: Loss = 4.3598\n",
      "2025-05-03 15:25:07,949 - INFO - Epoch 4, Batch 4000: Loss = 4.2520\n",
      "2025-05-03 15:25:33,811 - INFO - Epoch 4, Batch 4100: Loss = 4.6821\n",
      "2025-05-03 15:25:59,685 - INFO - Epoch 4, Batch 4200: Loss = 4.7576\n",
      "2025-05-03 15:26:25,569 - INFO - Epoch 4, Batch 4300: Loss = 4.6501\n",
      "2025-05-03 15:26:51,422 - INFO - Epoch 4, Batch 4400: Loss = 4.7250\n",
      "2025-05-03 15:27:17,255 - INFO - Epoch 4, Batch 4500: Loss = 4.6365\n",
      "2025-05-03 15:27:43,080 - INFO - Epoch 4, Batch 4600: Loss = 4.5015\n",
      "2025-05-03 15:28:08,947 - INFO - Epoch 4, Batch 4700: Loss = 4.8499\n",
      "2025-05-03 15:28:34,835 - INFO - Epoch 4, Batch 4800: Loss = 4.7239\n",
      "2025-05-03 15:29:00,686 - INFO - Epoch 4, Batch 4900: Loss = 4.0298\n",
      "2025-05-03 15:29:26,538 - INFO - Epoch 4, Batch 5000: Loss = 4.3741\n",
      "2025-05-03 15:29:52,395 - INFO - Epoch 4, Batch 5100: Loss = 5.1237\n",
      "2025-05-03 15:30:18,300 - INFO - Epoch 4, Batch 5200: Loss = 4.5529\n",
      "2025-05-03 15:30:44,180 - INFO - Epoch 4, Batch 5300: Loss = 4.4917\n",
      "2025-05-03 15:31:03,144 - INFO - ..... Finished yielding context windows .....\n",
      "2025-05-03 15:31:03,144 - INFO - Epoch 4 completed.\n",
      "2025-05-03 15:31:03,144 - INFO - Epoch 5/5\n",
      "2025-05-03 15:31:03,145 - INFO - ..... Creating context windows .....\n",
      "2025-05-03 15:31:28,971 - INFO - Epoch 5, Batch 100: Loss = 4.8214\n",
      "2025-05-03 15:31:54,831 - INFO - Epoch 5, Batch 200: Loss = 4.4174\n",
      "2025-05-03 15:32:20,705 - INFO - Epoch 5, Batch 300: Loss = 4.6260\n",
      "2025-05-03 15:32:46,559 - INFO - Epoch 5, Batch 400: Loss = 4.7741\n",
      "2025-05-03 15:33:12,431 - INFO - Epoch 5, Batch 500: Loss = 4.7657\n",
      "2025-05-03 15:33:38,322 - INFO - Epoch 5, Batch 600: Loss = 4.6423\n",
      "2025-05-03 15:34:04,200 - INFO - Epoch 5, Batch 700: Loss = 4.4875\n",
      "2025-05-03 15:34:30,047 - INFO - Epoch 5, Batch 800: Loss = 4.8821\n",
      "2025-05-03 15:34:55,927 - INFO - Epoch 5, Batch 900: Loss = 4.4219\n",
      "2025-05-03 15:35:21,822 - INFO - Epoch 5, Batch 1000: Loss = 4.5292\n",
      "2025-05-03 15:35:47,695 - INFO - Epoch 5, Batch 1100: Loss = 4.6553\n",
      "2025-05-03 15:36:13,572 - INFO - Epoch 5, Batch 1200: Loss = 4.8353\n",
      "2025-05-03 15:36:39,502 - INFO - Epoch 5, Batch 1300: Loss = 4.6257\n",
      "2025-05-03 15:37:05,377 - INFO - Epoch 5, Batch 1400: Loss = 4.9005\n",
      "2025-05-03 15:37:31,226 - INFO - Epoch 5, Batch 1500: Loss = 4.5697\n",
      "2025-05-03 15:37:57,079 - INFO - Epoch 5, Batch 1600: Loss = 4.9949\n",
      "2025-05-03 15:38:22,908 - INFO - Epoch 5, Batch 1700: Loss = 4.6782\n",
      "2025-05-03 15:38:48,761 - INFO - Epoch 5, Batch 1800: Loss = 4.1347\n",
      "2025-05-03 15:39:14,624 - INFO - Epoch 5, Batch 1900: Loss = 4.4209\n",
      "2025-05-03 15:39:40,492 - INFO - Epoch 5, Batch 2000: Loss = 4.4733\n",
      "2025-05-03 15:40:06,336 - INFO - Epoch 5, Batch 2100: Loss = 4.7753\n",
      "2025-05-03 15:40:32,197 - INFO - Epoch 5, Batch 2200: Loss = 4.3982\n",
      "2025-05-03 15:40:58,039 - INFO - Epoch 5, Batch 2300: Loss = 4.7165\n",
      "2025-05-03 15:41:23,903 - INFO - Epoch 5, Batch 2400: Loss = 5.1440\n",
      "2025-05-03 15:41:49,770 - INFO - Epoch 5, Batch 2500: Loss = 4.9023\n",
      "2025-05-03 15:42:15,614 - INFO - Epoch 5, Batch 2600: Loss = 4.4384\n",
      "2025-05-03 15:42:41,585 - INFO - Epoch 5, Batch 2700: Loss = 4.7594\n",
      "2025-05-03 15:43:07,557 - INFO - Epoch 5, Batch 2800: Loss = 4.7821\n",
      "2025-05-03 15:43:33,598 - INFO - Epoch 5, Batch 2900: Loss = 4.5047\n",
      "2025-05-03 15:43:59,444 - INFO - Epoch 5, Batch 3000: Loss = 4.4558\n",
      "2025-05-03 15:44:25,298 - INFO - Epoch 5, Batch 3100: Loss = 4.5843\n",
      "2025-05-03 15:44:51,176 - INFO - Epoch 5, Batch 3200: Loss = 4.6531\n",
      "2025-05-03 15:45:17,029 - INFO - Epoch 5, Batch 3300: Loss = 4.7934\n",
      "2025-05-03 15:45:42,889 - INFO - Epoch 5, Batch 3400: Loss = 4.5896\n",
      "2025-05-03 15:46:08,752 - INFO - Epoch 5, Batch 3500: Loss = 4.9813\n",
      "2025-05-03 15:46:34,622 - INFO - Epoch 5, Batch 3600: Loss = 4.3743\n",
      "2025-05-03 15:47:00,486 - INFO - Epoch 5, Batch 3700: Loss = 3.9985\n",
      "2025-05-03 15:47:26,334 - INFO - Epoch 5, Batch 3800: Loss = 4.9184\n",
      "2025-05-03 15:47:52,202 - INFO - Epoch 5, Batch 3900: Loss = 4.2968\n",
      "2025-05-03 15:48:18,057 - INFO - Epoch 5, Batch 4000: Loss = 4.1868\n",
      "2025-05-03 15:48:43,910 - INFO - Epoch 5, Batch 4100: Loss = 4.6295\n",
      "2025-05-03 15:49:09,771 - INFO - Epoch 5, Batch 4200: Loss = 4.7006\n",
      "2025-05-03 15:49:35,612 - INFO - Epoch 5, Batch 4300: Loss = 4.5891\n",
      "2025-05-03 15:50:01,515 - INFO - Epoch 5, Batch 4400: Loss = 4.6716\n",
      "2025-05-03 15:50:27,366 - INFO - Epoch 5, Batch 4500: Loss = 4.5794\n",
      "2025-05-03 15:50:53,217 - INFO - Epoch 5, Batch 4600: Loss = 4.4355\n",
      "2025-05-03 15:51:19,045 - INFO - Epoch 5, Batch 4700: Loss = 4.7991\n",
      "2025-05-03 15:51:44,891 - INFO - Epoch 5, Batch 4800: Loss = 4.6718\n",
      "2025-05-03 15:52:10,716 - INFO - Epoch 5, Batch 4900: Loss = 3.9691\n",
      "2025-05-03 15:52:36,578 - INFO - Epoch 5, Batch 5000: Loss = 4.3167\n",
      "2025-05-03 15:53:02,495 - INFO - Epoch 5, Batch 5100: Loss = 5.0782\n",
      "2025-05-03 15:53:28,334 - INFO - Epoch 5, Batch 5200: Loss = 4.5009\n",
      "2025-05-03 15:53:54,190 - INFO - Epoch 5, Batch 5300: Loss = 4.4372\n",
      "2025-05-03 15:54:13,148 - INFO - ..... Finished yielding context windows .....\n",
      "2025-05-03 15:54:13,149 - INFO - Epoch 5 completed.\n"
     ]
    }
   ],
   "source": [
    "base_network = FeedForwardNN(df_train, tokenized_vocab, embedding_dim, 512, tokenized_vocab_size)\n",
    "base_loss = base_network.fit(df_train, batch_size=10000, epochs=10, device=device)\n",
    "base_network.save_model(\"base_network.pth\")\n",
    "plot_loss(base_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_token(self, context_window, temperature):\n",
    "    \"\"\"\n",
    "    Predict the next token using the model.\n",
    "    context_window: list of tokens (strings)\n",
    "    \"\"\"\n",
    "    embedding = self._embed_tokens(context_window)\n",
    "    device = next(self.model.parameters()).device  # Get device from model\n",
    "    embedding_tensor = torch.tensor(embedding, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    logits = self.model(embedding_tensor)\n",
    "    probs = torch.softmax(logits / temperature, dim=-1)\n",
    "    next_token_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "    return next_token_idx\n",
    "import types\n",
    "network.predict_token = types.MethodType(predict_token, network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job description summary the senior accountant will provide support for the team providing highest quality care to patients and families to ensure that all employees are eligible to participate in the application process. click below to review information the of business and <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context_window = [\"<START>\", \"Software\", \"Engineer\"]\n",
    "max_length =  100\n",
    "device = next(network.model.parameters()).device\n",
    "temperature = 0.4\n",
    "description_generated = network.generate_job_description(context_window, max_length, temperature)\n",
    "print(description_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Model - FFNN\n",
    "\n",
    "- Perplexity -> 106.24\n",
    "- Top-5 Accuracy -> 44.72%\n",
    "\n",
    "- I need to explore using more words I used the top 50K words in the data sate maybe I will scale up the words as much as possible on my GPU, I will try to add more neurons I started with  512 I will give 1024 a shot. \n",
    "\n",
    "- The more neuraonsa and layers the more I will be able to modelize complex data. I need to be very careful about overfitting when I add nuerons maybe I will start by adding depths first with more layers\n",
    "\n",
    "- Change the architecture by adding more layers ? Or test other actiation functions currenlty I am using RELU to break the linearity I need to explore oters\n",
    "\n",
    "- I need to take a look at other embedding models and then give word2vec a shot.\n",
    "\n",
    "- I need to explore adding an embedding layer instead of comuting them and then feeding the neural network \n",
    "\n",
    "Other possiblities:\n",
    "\n",
    "- word2vec or Glove\n",
    "- BatcNorm or LayerNorm ? ( I need to  look up a bit more about what it is )\n",
    "- Learning Rate Scheduling ( Same as BatchNorm need to look up )\n",
    "\n",
    "\n",
    "Some predictions I got with this: \n",
    "\n",
    "- job description summary the senior accountant will provide support for the team providing highest quality care to patients and families to ensure that all employees are eligible to participate in the application process. click below to review information the of business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Network (More layers) and More Training 5 -> 10 epochs\n",
    "\n",
    "We have 3 hidden Layers with 1024 neurons for this Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 18:09:35,762 - INFO - Initializing FeedForwardNN with input_dim: 384, hidden_dim: 1024, output_dim: 50001\n",
      "2025-05-04 18:09:35,956 - INFO - ..... Training started .....\n",
      "2025-05-04 18:09:35,988 - INFO - Epoch 1/10\n",
      "2025-05-04 18:09:35,988 - INFO - ..... Creating context windows .....\n",
      "2025-05-04 18:10:21,086 - INFO - Epoch 1, Batch 100: Loss = 7.3998\n",
      "2025-05-04 18:11:06,270 - INFO - Epoch 1, Batch 200: Loss = 7.1979\n",
      "2025-05-04 18:11:51,455 - INFO - Epoch 1, Batch 300: Loss = 7.2647\n",
      "2025-05-04 18:12:36,688 - INFO - Epoch 1, Batch 400: Loss = 7.3640\n",
      "2025-05-04 18:13:21,896 - INFO - Epoch 1, Batch 500: Loss = 7.0472\n",
      "2025-05-04 18:14:07,167 - INFO - Epoch 1, Batch 600: Loss = 6.7852\n",
      "2025-05-04 18:14:52,445 - INFO - Epoch 1, Batch 700: Loss = 6.6369\n",
      "2025-05-04 18:15:37,900 - INFO - Epoch 1, Batch 800: Loss = 6.5438\n",
      "2025-05-04 18:16:23,174 - INFO - Epoch 1, Batch 900: Loss = 6.4013\n",
      "2025-05-04 18:17:08,512 - INFO - Epoch 1, Batch 1000: Loss = 6.2558\n",
      "2025-05-04 18:17:54,287 - INFO - Epoch 1, Batch 1100: Loss = 6.3134\n",
      "2025-05-04 18:18:40,067 - INFO - Epoch 1, Batch 1200: Loss = 6.1961\n",
      "2025-05-04 18:19:25,537 - INFO - Epoch 1, Batch 1300: Loss = 6.0358\n",
      "2025-05-04 18:20:10,644 - INFO - Epoch 1, Batch 1400: Loss = 6.0986\n",
      "2025-05-04 18:20:55,722 - INFO - Epoch 1, Batch 1500: Loss = 5.9839\n",
      "2025-05-04 18:21:41,168 - INFO - Epoch 1, Batch 1600: Loss = 6.0708\n",
      "2025-05-04 18:22:26,413 - INFO - Epoch 1, Batch 1700: Loss = 5.9105\n",
      "2025-05-04 18:23:11,664 - INFO - Epoch 1, Batch 1800: Loss = 5.7587\n",
      "2025-05-04 18:23:56,962 - INFO - Epoch 1, Batch 1900: Loss = 5.5196\n",
      "2025-05-04 18:24:42,200 - INFO - Epoch 1, Batch 2000: Loss = 5.6194\n",
      "2025-05-04 18:25:27,426 - INFO - Epoch 1, Batch 2100: Loss = 5.7993\n",
      "2025-05-04 18:26:12,631 - INFO - Epoch 1, Batch 2200: Loss = 5.6081\n",
      "2025-05-04 18:26:57,862 - INFO - Epoch 1, Batch 2300: Loss = 5.6956\n",
      "2025-05-04 18:27:43,128 - INFO - Epoch 1, Batch 2400: Loss = 5.9487\n",
      "2025-05-04 18:28:28,366 - INFO - Epoch 1, Batch 2500: Loss = 5.5969\n",
      "2025-05-04 18:29:13,579 - INFO - Epoch 1, Batch 2600: Loss = 5.3608\n",
      "2025-05-04 18:29:58,470 - INFO - Epoch 1, Batch 2700: Loss = 5.5967\n",
      "2025-05-04 18:30:43,317 - INFO - Epoch 1, Batch 2800: Loss = 5.4716\n",
      "2025-05-04 18:31:28,437 - INFO - Epoch 1, Batch 2900: Loss = 5.3833\n",
      "2025-05-04 18:32:13,665 - INFO - Epoch 1, Batch 3000: Loss = 5.2281\n",
      "2025-05-04 18:32:58,866 - INFO - Epoch 1, Batch 3100: Loss = 5.3121\n",
      "2025-05-04 18:33:44,062 - INFO - Epoch 1, Batch 3200: Loss = 5.3895\n",
      "2025-05-04 18:34:28,878 - INFO - Epoch 1, Batch 3300: Loss = 5.3513\n",
      "2025-05-04 18:35:13,264 - INFO - Epoch 1, Batch 3400: Loss = 5.1776\n",
      "2025-05-04 18:35:57,585 - INFO - Epoch 1, Batch 3500: Loss = 5.4908\n",
      "2025-05-04 18:36:41,966 - INFO - Epoch 1, Batch 3600: Loss = 4.9897\n",
      "2025-05-04 18:37:26,593 - INFO - Epoch 1, Batch 3700: Loss = 4.7736\n",
      "2025-05-04 18:38:10,990 - INFO - Epoch 1, Batch 3800: Loss = 5.4730\n",
      "2025-05-04 18:38:55,344 - INFO - Epoch 1, Batch 3900: Loss = 4.8809\n",
      "2025-05-04 18:39:39,703 - INFO - Epoch 1, Batch 4000: Loss = 4.7961\n",
      "2025-05-04 18:40:24,328 - INFO - Epoch 1, Batch 4100: Loss = 5.0788\n",
      "2025-05-04 18:41:09,582 - INFO - Epoch 1, Batch 4200: Loss = 5.1795\n",
      "2025-05-04 18:41:54,793 - INFO - Epoch 1, Batch 4300: Loss = 5.0798\n",
      "2025-05-04 18:42:40,063 - INFO - Epoch 1, Batch 4400: Loss = 5.1532\n",
      "2025-05-04 18:43:25,525 - INFO - Epoch 1, Batch 4500: Loss = 5.0503\n",
      "2025-05-04 18:44:10,806 - INFO - Epoch 1, Batch 4600: Loss = 4.9554\n",
      "2025-05-04 18:44:56,132 - INFO - Epoch 1, Batch 4700: Loss = 5.2247\n",
      "2025-05-04 18:45:41,396 - INFO - Epoch 1, Batch 4800: Loss = 5.0843\n",
      "2025-05-04 18:46:26,644 - INFO - Epoch 1, Batch 4900: Loss = 4.4217\n",
      "2025-05-04 18:47:11,921 - INFO - Epoch 1, Batch 5000: Loss = 4.7376\n",
      "2025-05-04 18:47:57,110 - INFO - Epoch 1, Batch 5100: Loss = 5.3932\n",
      "2025-05-04 18:48:41,551 - INFO - Epoch 1, Batch 5200: Loss = 4.8986\n",
      "2025-05-04 18:49:25,446 - INFO - Epoch 1, Batch 5300: Loss = 4.8019\n",
      "2025-05-04 18:49:57,406 - INFO - ..... Finished yielding context windows .....\n",
      "2025-05-04 18:49:57,406 - INFO - Epoch 1 completed.\n",
      "2025-05-04 18:49:57,406 - INFO - Epoch 2/10\n",
      "2025-05-04 18:49:57,407 - INFO - ..... Creating context windows .....\n",
      "2025-05-04 18:50:41,294 - INFO - Epoch 2, Batch 100: Loss = 5.1481\n",
      "2025-05-04 18:51:24,990 - INFO - Epoch 2, Batch 200: Loss = 4.6381\n",
      "2025-05-04 18:52:08,682 - INFO - Epoch 2, Batch 300: Loss = 4.8942\n",
      "2025-05-04 18:52:52,369 - INFO - Epoch 2, Batch 400: Loss = 4.9939\n",
      "2025-05-04 18:53:36,061 - INFO - Epoch 2, Batch 500: Loss = 5.0192\n",
      "2025-05-04 18:54:19,750 - INFO - Epoch 2, Batch 600: Loss = 4.8384\n",
      "2025-05-04 18:55:03,437 - INFO - Epoch 2, Batch 700: Loss = 4.6697\n",
      "2025-05-04 18:55:47,131 - INFO - Epoch 2, Batch 800: Loss = 5.1052\n",
      "2025-05-04 18:56:30,980 - INFO - Epoch 2, Batch 900: Loss = 4.6229\n",
      "2025-05-04 18:57:14,644 - INFO - Epoch 2, Batch 1000: Loss = 4.7440\n",
      "2025-05-04 18:57:58,307 - INFO - Epoch 2, Batch 1100: Loss = 4.8790\n",
      "2025-05-04 18:58:41,977 - INFO - Epoch 2, Batch 1200: Loss = 4.9913\n",
      "2025-05-04 18:59:25,635 - INFO - Epoch 2, Batch 1300: Loss = 4.7927\n",
      "2025-05-04 19:00:09,316 - INFO - Epoch 2, Batch 1400: Loss = 5.0138\n",
      "2025-05-04 19:00:52,984 - INFO - Epoch 2, Batch 1500: Loss = 4.6821\n",
      "2025-05-04 19:01:36,658 - INFO - Epoch 2, Batch 1600: Loss = 5.1316\n",
      "2025-05-04 19:02:20,523 - INFO - Epoch 2, Batch 1700: Loss = 4.8264\n",
      "2025-05-04 19:03:04,206 - INFO - Epoch 2, Batch 1800: Loss = 4.3159\n",
      "2025-05-04 19:03:47,895 - INFO - Epoch 2, Batch 1900: Loss = 4.4979\n",
      "2025-05-04 19:04:31,580 - INFO - Epoch 2, Batch 2000: Loss = 4.5732\n",
      "2025-05-04 19:05:15,677 - INFO - Epoch 2, Batch 2100: Loss = 4.8468\n",
      "2025-05-04 19:06:00,868 - INFO - Epoch 2, Batch 2200: Loss = 4.5298\n",
      "2025-05-04 19:06:46,162 - INFO - Epoch 2, Batch 2300: Loss = 4.8166\n",
      "2025-05-04 19:07:31,476 - INFO - Epoch 2, Batch 2400: Loss = 5.2482\n",
      "2025-05-04 19:08:16,945 - INFO - Epoch 2, Batch 2500: Loss = 4.9411\n",
      "2025-05-04 19:09:01,829 - INFO - Epoch 2, Batch 2600: Loss = 4.4851\n",
      "2025-05-04 19:09:47,182 - INFO - Epoch 2, Batch 2700: Loss = 4.8694\n",
      "2025-05-04 19:10:32,518 - INFO - Epoch 2, Batch 2800: Loss = 4.8427\n",
      "2025-05-04 19:11:17,852 - INFO - Epoch 2, Batch 2900: Loss = 4.5457\n",
      "2025-05-04 19:12:03,167 - INFO - Epoch 2, Batch 3000: Loss = 4.4686\n",
      "2025-05-04 19:12:48,478 - INFO - Epoch 2, Batch 3100: Loss = 4.5880\n",
      "2025-05-04 19:13:33,743 - INFO - Epoch 2, Batch 3200: Loss = 4.6886\n",
      "2025-05-04 19:14:19,182 - INFO - Epoch 2, Batch 3300: Loss = 4.8167\n",
      "2025-05-04 19:15:04,181 - INFO - Epoch 2, Batch 3400: Loss = 4.5812\n",
      "2025-05-04 19:15:48,968 - INFO - Epoch 2, Batch 3500: Loss = 4.9935\n",
      "2025-05-04 19:16:34,141 - INFO - Epoch 2, Batch 3600: Loss = 4.3638\n",
      "2025-05-04 19:17:18,969 - INFO - Epoch 2, Batch 3700: Loss = 3.9723\n",
      "2025-05-04 19:18:03,836 - INFO - Epoch 2, Batch 3800: Loss = 4.9391\n",
      "2025-05-04 19:18:48,675 - INFO - Epoch 2, Batch 3900: Loss = 4.2572\n",
      "2025-05-04 19:19:33,989 - INFO - Epoch 2, Batch 4000: Loss = 4.1473\n",
      "2025-05-04 19:20:19,185 - INFO - Epoch 2, Batch 4100: Loss = 4.5526\n",
      "2025-05-04 19:21:03,933 - INFO - Epoch 2, Batch 4200: Loss = 4.6768\n",
      "2025-05-04 19:21:48,933 - INFO - Epoch 2, Batch 4300: Loss = 4.5398\n",
      "2025-05-04 19:22:34,200 - INFO - Epoch 2, Batch 4400: Loss = 4.6714\n",
      "2025-05-04 19:23:19,478 - INFO - Epoch 2, Batch 4500: Loss = 4.5164\n",
      "2025-05-04 19:24:04,961 - INFO - Epoch 2, Batch 4600: Loss = 4.3659\n",
      "2025-05-04 19:24:49,920 - INFO - Epoch 2, Batch 4700: Loss = 4.7648\n",
      "2025-05-04 19:25:35,147 - INFO - Epoch 2, Batch 4800: Loss = 4.6395\n",
      "2025-05-04 19:26:20,422 - INFO - Epoch 2, Batch 4900: Loss = 3.8149\n",
      "2025-05-04 19:27:05,696 - INFO - Epoch 2, Batch 5000: Loss = 4.2274\n",
      "2025-05-04 19:27:51,029 - INFO - Epoch 2, Batch 5100: Loss = 5.0309\n",
      "2025-05-04 19:28:36,397 - INFO - Epoch 2, Batch 5200: Loss = 4.4522\n",
      "2025-05-04 19:29:21,759 - INFO - Epoch 2, Batch 5300: Loss = 4.3606\n",
      "2025-05-04 19:29:55,067 - INFO - ..... Finished yielding context windows .....\n",
      "2025-05-04 19:29:55,068 - INFO - Epoch 2 completed.\n",
      "2025-05-04 19:29:55,068 - INFO - Epoch 3/10\n",
      "2025-05-04 19:29:55,068 - INFO - ..... Creating context windows .....\n",
      "2025-05-04 19:30:40,365 - INFO - Epoch 3, Batch 100: Loss = 4.7180\n",
      "2025-05-04 19:31:25,634 - INFO - Epoch 3, Batch 200: Loss = 4.2565\n",
      "2025-05-04 19:32:10,937 - INFO - Epoch 3, Batch 300: Loss = 4.5091\n",
      "2025-05-04 19:32:56,264 - INFO - Epoch 3, Batch 400: Loss = 4.6444\n",
      "2025-05-04 19:33:41,577 - INFO - Epoch 3, Batch 500: Loss = 4.6348\n",
      "2025-05-04 19:34:26,868 - INFO - Epoch 3, Batch 600: Loss = 4.4767\n",
      "2025-05-04 19:35:12,162 - INFO - Epoch 3, Batch 700: Loss = 4.3168\n",
      "2025-05-04 19:35:57,428 - INFO - Epoch 3, Batch 800: Loss = 4.7662\n",
      "2025-05-04 19:36:42,704 - INFO - Epoch 3, Batch 900: Loss = 4.2255\n",
      "2025-05-04 19:37:27,508 - INFO - Epoch 3, Batch 1000: Loss = 4.3882\n",
      "2025-05-04 19:38:11,204 - INFO - Epoch 3, Batch 1100: Loss = 4.5037\n",
      "2025-05-04 19:38:54,904 - INFO - Epoch 3, Batch 1200: Loss = 4.7276\n",
      "2025-05-04 19:39:38,581 - INFO - Epoch 3, Batch 1300: Loss = 4.4462\n",
      "2025-05-04 19:40:23,509 - INFO - Epoch 3, Batch 1400: Loss = 4.7096\n",
      "2025-05-04 19:41:08,816 - INFO - Epoch 3, Batch 1500: Loss = 4.3484\n",
      "2025-05-04 19:41:54,172 - INFO - Epoch 3, Batch 1600: Loss = 4.8449\n",
      "2025-05-04 19:42:39,410 - INFO - Epoch 3, Batch 1700: Loss = 4.5158\n",
      "2025-05-04 19:43:24,875 - INFO - Epoch 3, Batch 1800: Loss = 3.8653\n",
      "2025-05-04 19:44:10,122 - INFO - Epoch 3, Batch 1900: Loss = 4.2070\n",
      "2025-05-04 19:44:55,370 - INFO - Epoch 3, Batch 2000: Loss = 4.2630\n",
      "2025-05-04 19:45:40,473 - INFO - Epoch 3, Batch 2100: Loss = 4.5515\n",
      "2025-05-04 19:46:25,132 - INFO - Epoch 3, Batch 2200: Loss = 4.2115\n",
      "2025-05-04 19:47:10,490 - INFO - Epoch 3, Batch 2300: Loss = 4.5168\n",
      "2025-05-04 19:47:55,836 - INFO - Epoch 3, Batch 2400: Loss = 4.9924\n",
      "2025-05-04 19:48:41,174 - INFO - Epoch 3, Batch 2500: Loss = 4.7067\n",
      "2025-05-04 19:49:26,697 - INFO - Epoch 3, Batch 2600: Loss = 4.1878\n",
      "2025-05-04 19:50:11,938 - INFO - Epoch 3, Batch 2700: Loss = 4.6093\n",
      "2025-05-04 19:50:57,195 - INFO - Epoch 3, Batch 2800: Loss = 4.6176\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m network_v2 = FeedForwardNN(df_train, vocab, embedding_dim, \u001b[32m1024\u001b[39m, vocab_size)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mnetwork_v2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mFeedForwardNN.fit\u001b[39m\u001b[34m(self, training_set, batch_size, epochs, device)\u001b[39m\n\u001b[32m    118\u001b[39m     y_batch.append(\u001b[38;5;28mself\u001b[39m.vocab.get(next_token, \u001b[38;5;28mself\u001b[39m.vocab.get(\u001b[33m'\u001b[39m\u001b[33m<UNK>\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m)))\n\u001b[32m    120\u001b[39m X_np = np.array(X_batch)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m X_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m y_tensor = torch.tensor(y_batch, dtype=torch.long, device=device)\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# This is important because it resets the gradients of previous batches to 0 it allows to compute the gradient for the current batch\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# If we don't do this, the gradients will be accumulated and the model will not learn correctly\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "network_v2 = FeedForwardNN(df_train, tokenized_vocab, embedding_dim, 1024, tokenized_vocab_size)\n",
    "# network_v2.fit(df_train, batch_size=10000, epochs=10, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical scale up of the model\n",
    "\n",
    "The model seems to hit a plateau early in the training (Epoch 2) with having values ranging from 4.3 to 5.3\n",
    "\n",
    "The plateau is still seen on Epoch 3. The approach of deeper network and more neurons doesnt change anything for the loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing another Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 13:42:55,020 - INFO - Initializing FeedForwardNN with input_dim: 384, hidden_dim: 1024, output_dim: 50001\n"
     ]
    }
   ],
   "source": [
    "network_lr = FeedForwardNN(df_train, tokenized_vocab, embedding_dim, 1024, tokenized_vocab_size, lr=5e-4)\n",
    "# values = network_lr.fit(df_train, batch_size=10000, epochs=5, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network_lr.model.state_dict(), \"model_cache/network_deep_lr.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayerFFNN(nn.Module):\n",
    "    def __init__(self, vocab,df, input_size, hidden_size, embedding_dim, dropout, num_layers, output_dim, context_size, lr):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.lr = lr\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_dim)\n",
    "        self.model = self.init_model()\n",
    "        assert self.embedding.num_embeddings == len(self.vocab), \"Embedding size and vocab size must match!\"\n",
    "\n",
    "    def _get_context_windows_batched(self, training_set: pd.DataFrame, N: int, batch_size: int):\n",
    "        \"\"\"\n",
    "        Generator that yields batches of (context_window, next_token) pairs across the dataset,\n",
    "        lazily building batches to avoid memory overload.\n",
    "        \"\"\"\n",
    "        logging.info(f\"..... Creating context windows .....\")\n",
    "        \n",
    "        batch_context_windows = []\n",
    "\n",
    "        for _, row in training_set.iterrows():\n",
    "            tokens = row[\"merged_tokenized\"]\n",
    "            if len(tokens) <= N:\n",
    "                continue  # skip rows that are too short\n",
    "            for i in range(len(tokens) - N):\n",
    "                context_window = tokens[i:i+N]\n",
    "                next_token = tokens[i+N]\n",
    "                batch_context_windows.append((context_window, next_token))\n",
    "\n",
    "                if len(batch_context_windows) == batch_size:\n",
    "                    yield batch_context_windows\n",
    "                    batch_context_windows = []\n",
    "\n",
    "        if batch_context_windows:\n",
    "            yield batch_context_windows  # Yield last partial batch\n",
    "\n",
    "        logging.info(f\"..... Finished yielding context windows .....\")\n",
    "    \n",
    "    def init_model(self):\n",
    "        if '__UNK__' not in self.vocab:\n",
    "            self.vocab['__UNK__'] = len(self.vocab)\n",
    "        layers = []\n",
    "        input_dim = self.embedding_dim * self.context_size\n",
    "        for _ in range(self.num_layers):\n",
    "            layers.append(nn.Linear(input_dim, self.hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(self.dropout))\n",
    "            input_dim = self.hidden_dim\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.output_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def fit(self, epochs, batch_size, device):\n",
    "        # Move model to device\n",
    "        self.to(device)\n",
    "        \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        loss_values = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            logging.info(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            epoch_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            context_windows_batch = self._get_context_windows_batched(self.df, self.context_size, batch_size)\n",
    "            \n",
    "            for batch_idx, batch in enumerate(context_windows_batch):\n",
    "                batch_X = []\n",
    "                batch_y = []\n",
    "                \n",
    "                # Process all examples in the batch\n",
    "                for context_window, next_token in batch:\n",
    "                    context_window_indices = [self.vocab.get(token, self.vocab['__UNK__']) for token in context_window]\n",
    "                    embedding = self.embedding(torch.tensor(context_window_indices, dtype=torch.long, device=device))\n",
    "                    \n",
    "                    # Add this example to the batch\n",
    "                    batch_X.append(embedding.view(1, -1))\n",
    "                    batch_y.append(self.vocab.get(next_token, self.vocab['__UNK__']))\n",
    "                \n",
    "                # Stack all examples in the batch\n",
    "                X_tensor = torch.cat(batch_X, dim=0)  # Already on correct device\n",
    "                y_tensor = torch.tensor(batch_y, dtype=torch.long, device=device)\n",
    "                \n",
    "                # Forward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(X_tensor)\n",
    "                loss = loss_fn(output, y_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track loss\n",
    "                epoch_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                if batch_idx % 100 == 0:\n",
    "                    logging.info(f\"Epoch {epoch+1}, Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            # Calculate average loss for the epoch\n",
    "            avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else 0\n",
    "            loss_values.append(avg_epoch_loss)\n",
    "            logging.info(f\"Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        return loss_values\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_vocab, tokenized_vocab_size = get_pruned_vocab_tokenized(tokenized_df_train, \"merged_tokenized\", top_k=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 23:05:54,799 - INFO - Epoch 1/5\n",
      "2025-05-05 23:05:54,799 - INFO - ..... Creating context windows .....\n",
      "2025-05-05 23:05:54,989 - INFO - Epoch 1, Batch 0: Loss = 10.8163\n",
      "2025-05-05 23:06:13,729 - INFO - Epoch 1, Batch 100: Loss = 8.0401\n",
      "2025-05-05 23:06:32,510 - INFO - Epoch 1, Batch 200: Loss = 7.6529\n",
      "2025-05-05 23:06:51,295 - INFO - Epoch 1, Batch 300: Loss = 8.8934\n",
      "2025-05-05 23:07:10,072 - INFO - Epoch 1, Batch 400: Loss = 7.1551\n",
      "2025-05-05 23:07:28,807 - INFO - Epoch 1, Batch 500: Loss = 7.6411\n",
      "2025-05-05 23:07:47,501 - INFO - Epoch 1, Batch 600: Loss = 8.1577\n",
      "2025-05-05 23:08:06,228 - INFO - Epoch 1, Batch 700: Loss = 6.1835\n",
      "2025-05-05 23:08:24,988 - INFO - Epoch 1, Batch 800: Loss = 7.8130\n",
      "2025-05-05 23:08:43,705 - INFO - Epoch 1, Batch 900: Loss = 7.8798\n",
      "2025-05-05 23:09:02,424 - INFO - Epoch 1, Batch 1000: Loss = 8.7048\n",
      "2025-05-05 23:09:21,156 - INFO - Epoch 1, Batch 1100: Loss = 8.1755\n",
      "2025-05-05 23:09:39,389 - INFO - Epoch 1, Batch 1200: Loss = 7.8218\n",
      "2025-05-05 23:09:57,355 - INFO - Epoch 1, Batch 1300: Loss = 8.5450\n",
      "2025-05-05 23:10:15,317 - INFO - Epoch 1, Batch 1400: Loss = 6.8516\n",
      "2025-05-05 23:10:33,281 - INFO - Epoch 1, Batch 1500: Loss = 7.8634\n",
      "2025-05-05 23:10:51,243 - INFO - Epoch 1, Batch 1600: Loss = 7.6808\n",
      "2025-05-05 23:11:09,203 - INFO - Epoch 1, Batch 1700: Loss = 7.4179\n",
      "2025-05-05 23:11:27,166 - INFO - Epoch 1, Batch 1800: Loss = 7.0610\n",
      "2025-05-05 23:11:45,127 - INFO - Epoch 1, Batch 1900: Loss = 7.2190\n",
      "2025-05-05 23:12:03,091 - INFO - Epoch 1, Batch 2000: Loss = 7.3791\n",
      "2025-05-05 23:12:21,057 - INFO - Epoch 1, Batch 2100: Loss = 7.7249\n",
      "2025-05-05 23:12:39,017 - INFO - Epoch 1, Batch 2200: Loss = 8.2461\n",
      "2025-05-05 23:12:56,978 - INFO - Epoch 1, Batch 2300: Loss = 9.4410\n",
      "2025-05-05 23:13:14,940 - INFO - Epoch 1, Batch 2400: Loss = 8.1733\n",
      "2025-05-05 23:13:32,899 - INFO - Epoch 1, Batch 2500: Loss = 7.7516\n",
      "2025-05-05 23:13:50,861 - INFO - Epoch 1, Batch 2600: Loss = 7.2337\n",
      "2025-05-05 23:14:08,823 - INFO - Epoch 1, Batch 2700: Loss = 8.6122\n",
      "2025-05-05 23:14:26,786 - INFO - Epoch 1, Batch 2800: Loss = 8.2869\n",
      "2025-05-05 23:14:44,749 - INFO - Epoch 1, Batch 2900: Loss = 7.2574\n",
      "2025-05-05 23:15:02,713 - INFO - Epoch 1, Batch 3000: Loss = 7.2500\n",
      "2025-05-05 23:15:20,676 - INFO - Epoch 1, Batch 3100: Loss = 7.5162\n",
      "2025-05-05 23:15:38,639 - INFO - Epoch 1, Batch 3200: Loss = 7.6999\n",
      "2025-05-05 23:15:56,602 - INFO - Epoch 1, Batch 3300: Loss = 8.2600\n",
      "2025-05-05 23:16:14,563 - INFO - Epoch 1, Batch 3400: Loss = 6.7894\n",
      "2025-05-05 23:16:32,526 - INFO - Epoch 1, Batch 3500: Loss = 7.7828\n",
      "2025-05-05 23:16:50,488 - INFO - Epoch 1, Batch 3600: Loss = 7.0558\n",
      "2025-05-05 23:17:08,451 - INFO - Epoch 1, Batch 3700: Loss = 7.9194\n",
      "2025-05-05 23:17:26,411 - INFO - Epoch 1, Batch 3800: Loss = 8.2277\n",
      "2025-05-05 23:17:44,385 - INFO - Epoch 1, Batch 3900: Loss = 7.6604\n",
      "2025-05-05 23:18:02,345 - INFO - Epoch 1, Batch 4000: Loss = 7.3562\n",
      "2025-05-05 23:18:20,306 - INFO - Epoch 1, Batch 4100: Loss = 7.8895\n",
      "2025-05-05 23:18:38,269 - INFO - Epoch 1, Batch 4200: Loss = 8.0971\n",
      "2025-05-05 23:18:56,228 - INFO - Epoch 1, Batch 4300: Loss = 7.8603\n",
      "2025-05-05 23:19:14,189 - INFO - Epoch 1, Batch 4400: Loss = 6.7913\n",
      "2025-05-05 23:19:32,132 - INFO - Epoch 1, Batch 4500: Loss = 7.2414\n",
      "2025-05-05 23:19:50,034 - INFO - Epoch 1, Batch 4600: Loss = 6.8717\n",
      "2025-05-05 23:20:07,935 - INFO - Epoch 1, Batch 4700: Loss = 8.0436\n",
      "2025-05-05 23:20:25,836 - INFO - Epoch 1, Batch 4800: Loss = 7.5256\n",
      "2025-05-05 23:20:43,737 - INFO - Epoch 1, Batch 4900: Loss = 7.4896\n",
      "2025-05-05 23:21:01,638 - INFO - Epoch 1, Batch 5000: Loss = 5.2356\n",
      "2025-05-05 23:21:19,541 - INFO - Epoch 1, Batch 5100: Loss = 8.0259\n",
      "2025-05-05 23:21:37,444 - INFO - Epoch 1, Batch 5200: Loss = 6.5397\n",
      "2025-05-05 23:21:55,345 - INFO - Epoch 1, Batch 5300: Loss = 7.1621\n",
      "2025-05-05 23:22:13,247 - INFO - Epoch 1, Batch 5400: Loss = 7.5808\n",
      "2025-05-05 23:22:31,148 - INFO - Epoch 1, Batch 5500: Loss = 7.5695\n",
      "2025-05-05 23:22:49,050 - INFO - Epoch 1, Batch 5600: Loss = 7.7919\n",
      "2025-05-05 23:23:06,949 - INFO - Epoch 1, Batch 5700: Loss = 7.4596\n",
      "2025-05-05 23:23:24,851 - INFO - Epoch 1, Batch 5800: Loss = 7.4490\n",
      "2025-05-05 23:23:42,752 - INFO - Epoch 1, Batch 5900: Loss = 7.7962\n",
      "2025-05-05 23:24:00,654 - INFO - Epoch 1, Batch 6000: Loss = 5.4288\n",
      "2025-05-05 23:24:18,556 - INFO - Epoch 1, Batch 6100: Loss = 8.7152\n",
      "2025-05-05 23:24:36,460 - INFO - Epoch 1, Batch 6200: Loss = 6.1298\n",
      "2025-05-05 23:24:54,361 - INFO - Epoch 1, Batch 6300: Loss = 6.9116\n",
      "2025-05-05 23:25:12,262 - INFO - Epoch 1, Batch 6400: Loss = 8.0614\n",
      "2025-05-05 23:25:30,160 - INFO - Epoch 1, Batch 6500: Loss = 8.8149\n",
      "2025-05-05 23:25:48,063 - INFO - Epoch 1, Batch 6600: Loss = 7.3292\n",
      "2025-05-05 23:26:05,962 - INFO - Epoch 1, Batch 6700: Loss = 7.5118\n",
      "2025-05-05 23:26:23,862 - INFO - Epoch 1, Batch 6800: Loss = 7.8524\n",
      "2025-05-05 23:26:41,762 - INFO - Epoch 1, Batch 6900: Loss = 7.0388\n",
      "2025-05-05 23:26:59,664 - INFO - Epoch 1, Batch 7000: Loss = 7.2996\n",
      "2025-05-05 23:27:17,566 - INFO - Epoch 1, Batch 7100: Loss = 7.4220\n",
      "2025-05-05 23:27:35,466 - INFO - Epoch 1, Batch 7200: Loss = 7.0011\n",
      "2025-05-05 23:27:53,367 - INFO - Epoch 1, Batch 7300: Loss = 8.1289\n",
      "2025-05-05 23:28:11,266 - INFO - Epoch 1, Batch 7400: Loss = 7.2940\n",
      "2025-05-05 23:28:29,165 - INFO - Epoch 1, Batch 7500: Loss = 7.8791\n",
      "2025-05-05 23:28:47,065 - INFO - Epoch 1, Batch 7600: Loss = 5.6829\n",
      "2025-05-05 23:29:04,965 - INFO - Epoch 1, Batch 7700: Loss = 6.4614\n",
      "2025-05-05 23:29:22,865 - INFO - Epoch 1, Batch 7800: Loss = 7.3954\n",
      "2025-05-05 23:29:40,764 - INFO - Epoch 1, Batch 7900: Loss = 7.1396\n",
      "2025-05-05 23:29:58,665 - INFO - Epoch 1, Batch 8000: Loss = 5.4630\n",
      "2025-05-05 23:30:16,563 - INFO - Epoch 1, Batch 8100: Loss = 6.2636\n",
      "2025-05-05 23:30:34,463 - INFO - Epoch 1, Batch 8200: Loss = 7.8384\n",
      "2025-05-05 23:30:52,361 - INFO - Epoch 1, Batch 8300: Loss = 8.3662\n",
      "2025-05-05 23:31:10,262 - INFO - Epoch 1, Batch 8400: Loss = 7.5747\n",
      "2025-05-05 23:31:28,161 - INFO - Epoch 1, Batch 8500: Loss = 7.6585\n",
      "2025-05-05 23:31:46,060 - INFO - Epoch 1, Batch 8600: Loss = 7.0623\n",
      "2025-05-05 23:32:03,958 - INFO - Epoch 1, Batch 8700: Loss = 6.9474\n",
      "2025-05-05 23:32:21,857 - INFO - Epoch 1, Batch 8800: Loss = 6.2717\n",
      "2025-05-05 23:32:39,755 - INFO - Epoch 1, Batch 8900: Loss = 7.7461\n",
      "2025-05-05 23:32:57,656 - INFO - Epoch 1, Batch 9000: Loss = 7.1170\n",
      "2025-05-05 23:33:15,553 - INFO - Epoch 1, Batch 9100: Loss = 7.5506\n",
      "2025-05-05 23:33:33,454 - INFO - Epoch 1, Batch 9200: Loss = 7.3708\n",
      "2025-05-05 23:33:51,352 - INFO - Epoch 1, Batch 9300: Loss = 7.7482\n",
      "2025-05-05 23:34:09,249 - INFO - Epoch 1, Batch 9400: Loss = 7.0749\n",
      "2025-05-05 23:34:27,146 - INFO - Epoch 1, Batch 9500: Loss = 7.4705\n",
      "2025-05-05 23:34:45,043 - INFO - Epoch 1, Batch 9600: Loss = 5.0041\n",
      "2025-05-05 23:35:02,945 - INFO - Epoch 1, Batch 9700: Loss = 7.6114\n",
      "2025-05-05 23:35:20,843 - INFO - Epoch 1, Batch 9800: Loss = 7.0809\n",
      "2025-05-05 23:35:38,743 - INFO - Epoch 1, Batch 9900: Loss = 7.1152\n",
      "2025-05-05 23:35:56,642 - INFO - Epoch 1, Batch 10000: Loss = 8.3031\n",
      "2025-05-05 23:36:14,542 - INFO - Epoch 1, Batch 10100: Loss = 7.5680\n",
      "2025-05-05 23:36:32,441 - INFO - Epoch 1, Batch 10200: Loss = 6.2899\n",
      "2025-05-05 23:36:50,340 - INFO - Epoch 1, Batch 10300: Loss = 7.4206\n",
      "2025-05-05 23:37:08,241 - INFO - Epoch 1, Batch 10400: Loss = 7.3421\n",
      "2025-05-05 23:37:26,139 - INFO - Epoch 1, Batch 10500: Loss = 6.6021\n",
      "2025-05-05 23:37:44,040 - INFO - Epoch 1, Batch 10600: Loss = 7.2246\n",
      "2025-05-05 23:38:01,940 - INFO - Epoch 1, Batch 10700: Loss = 7.4585\n",
      "2025-05-05 23:38:19,836 - INFO - Epoch 1, Batch 10800: Loss = 8.2366\n",
      "2025-05-05 23:38:37,736 - INFO - Epoch 1, Batch 10900: Loss = 7.2499\n",
      "2025-05-05 23:38:55,636 - INFO - Epoch 1, Batch 11000: Loss = 6.6735\n",
      "2025-05-05 23:39:13,534 - INFO - Epoch 1, Batch 11100: Loss = 7.6637\n",
      "2025-05-05 23:39:31,434 - INFO - Epoch 1, Batch 11200: Loss = 7.1773\n",
      "2025-05-05 23:39:49,336 - INFO - Epoch 1, Batch 11300: Loss = 6.7973\n",
      "2025-05-05 23:40:07,234 - INFO - Epoch 1, Batch 11400: Loss = 7.1992\n",
      "2025-05-05 23:40:25,133 - INFO - Epoch 1, Batch 11500: Loss = 7.1407\n",
      "2025-05-05 23:40:43,032 - INFO - Epoch 1, Batch 11600: Loss = 4.4888\n",
      "2025-05-05 23:41:00,933 - INFO - Epoch 1, Batch 11700: Loss = 7.0482\n",
      "2025-05-05 23:41:18,831 - INFO - Epoch 1, Batch 11800: Loss = 7.0766\n",
      "2025-05-05 23:41:36,731 - INFO - Epoch 1, Batch 11900: Loss = 8.2947\n",
      "2025-05-05 23:41:54,629 - INFO - Epoch 1, Batch 12000: Loss = 6.4756\n",
      "2025-05-05 23:42:12,530 - INFO - Epoch 1, Batch 12100: Loss = 7.0231\n",
      "2025-05-05 23:42:30,430 - INFO - Epoch 1, Batch 12200: Loss = 6.6905\n",
      "2025-05-05 23:42:48,331 - INFO - Epoch 1, Batch 12300: Loss = 8.3250\n",
      "2025-05-05 23:43:06,232 - INFO - Epoch 1, Batch 12400: Loss = 6.5230\n",
      "2025-05-05 23:43:24,130 - INFO - Epoch 1, Batch 12500: Loss = 7.3073\n",
      "2025-05-05 23:43:42,031 - INFO - Epoch 1, Batch 12600: Loss = 6.8066\n",
      "2025-05-05 23:43:59,931 - INFO - Epoch 1, Batch 12700: Loss = 7.7766\n",
      "2025-05-05 23:44:17,829 - INFO - Epoch 1, Batch 12800: Loss = 7.6492\n",
      "2025-05-05 23:44:35,729 - INFO - Epoch 1, Batch 12900: Loss = 6.6631\n",
      "2025-05-05 23:44:53,627 - INFO - Epoch 1, Batch 13000: Loss = 7.6282\n",
      "2025-05-05 23:45:11,525 - INFO - Epoch 1, Batch 13100: Loss = 7.9605\n",
      "2025-05-05 23:45:29,422 - INFO - Epoch 1, Batch 13200: Loss = 8.8015\n",
      "2025-05-05 23:45:47,320 - INFO - Epoch 1, Batch 13300: Loss = 7.7804\n",
      "2025-05-05 23:46:05,221 - INFO - Epoch 1, Batch 13400: Loss = 6.6206\n",
      "2025-05-05 23:46:23,123 - INFO - Epoch 1, Batch 13500: Loss = 7.9677\n",
      "2025-05-05 23:46:41,022 - INFO - Epoch 1, Batch 13600: Loss = 7.2217\n",
      "2025-05-05 23:46:58,922 - INFO - Epoch 1, Batch 13700: Loss = 8.2212\n",
      "2025-05-05 23:47:16,822 - INFO - Epoch 1, Batch 13800: Loss = 7.5707\n",
      "2025-05-05 23:47:34,720 - INFO - Epoch 1, Batch 13900: Loss = 8.4047\n",
      "2025-05-05 23:47:52,618 - INFO - Epoch 1, Batch 14000: Loss = 7.9987\n",
      "2025-05-05 23:48:10,519 - INFO - Epoch 1, Batch 14100: Loss = 6.6920\n",
      "2025-05-05 23:48:28,417 - INFO - Epoch 1, Batch 14200: Loss = 6.7232\n",
      "2025-05-05 23:48:46,315 - INFO - Epoch 1, Batch 14300: Loss = 6.5529\n",
      "2025-05-05 23:49:04,215 - INFO - Epoch 1, Batch 14400: Loss = 7.4333\n",
      "2025-05-05 23:49:22,113 - INFO - Epoch 1, Batch 14500: Loss = 7.5431\n",
      "2025-05-05 23:49:40,012 - INFO - Epoch 1, Batch 14600: Loss = 3.7900\n",
      "2025-05-05 23:49:57,912 - INFO - Epoch 1, Batch 14700: Loss = 7.1115\n",
      "2025-05-05 23:50:15,813 - INFO - Epoch 1, Batch 14800: Loss = 6.9575\n",
      "2025-05-05 23:50:33,711 - INFO - Epoch 1, Batch 14900: Loss = 7.2014\n",
      "2025-05-05 23:50:51,610 - INFO - Epoch 1, Batch 15000: Loss = 7.0157\n",
      "2025-05-05 23:51:09,509 - INFO - Epoch 1, Batch 15100: Loss = 7.1610\n",
      "2025-05-05 23:51:27,409 - INFO - Epoch 1, Batch 15200: Loss = 7.1034\n",
      "2025-05-05 23:51:45,307 - INFO - Epoch 1, Batch 15300: Loss = 7.7922\n",
      "2025-05-05 23:52:03,206 - INFO - Epoch 1, Batch 15400: Loss = 6.4919\n",
      "2025-05-05 23:52:21,105 - INFO - Epoch 1, Batch 15500: Loss = 6.4747\n",
      "2025-05-05 23:52:39,006 - INFO - Epoch 1, Batch 15600: Loss = 7.5104\n",
      "2025-05-05 23:52:56,905 - INFO - Epoch 1, Batch 15700: Loss = 7.2238\n",
      "2025-05-05 23:53:14,804 - INFO - Epoch 1, Batch 15800: Loss = 4.9037\n",
      "2025-05-05 23:53:32,704 - INFO - Epoch 1, Batch 15900: Loss = 7.9346\n",
      "2025-05-05 23:53:50,603 - INFO - Epoch 1, Batch 16000: Loss = 7.7167\n",
      "2025-05-05 23:54:08,507 - INFO - Epoch 1, Batch 16100: Loss = 7.3284\n",
      "2025-05-05 23:54:26,407 - INFO - Epoch 1, Batch 16200: Loss = 7.9916\n",
      "2025-05-05 23:54:44,309 - INFO - Epoch 1, Batch 16300: Loss = 7.4655\n",
      "2025-05-05 23:55:02,209 - INFO - Epoch 1, Batch 16400: Loss = 7.3643\n",
      "2025-05-05 23:55:20,108 - INFO - Epoch 1, Batch 16500: Loss = 8.4364\n",
      "2025-05-05 23:55:38,008 - INFO - Epoch 1, Batch 16600: Loss = 7.3940\n",
      "2025-05-05 23:55:55,908 - INFO - Epoch 1, Batch 16700: Loss = 7.5886\n",
      "2025-05-05 23:56:13,806 - INFO - Epoch 1, Batch 16800: Loss = 6.8312\n",
      "2025-05-05 23:56:31,705 - INFO - Epoch 1, Batch 16900: Loss = 7.9027\n",
      "2025-05-05 23:56:49,604 - INFO - Epoch 1, Batch 17000: Loss = 7.2933\n",
      "2025-05-05 23:57:07,504 - INFO - Epoch 1, Batch 17100: Loss = 7.1002\n",
      "2025-05-05 23:57:25,404 - INFO - Epoch 1, Batch 17200: Loss = 7.2789\n",
      "2025-05-05 23:57:43,303 - INFO - Epoch 1, Batch 17300: Loss = 8.3921\n",
      "2025-05-05 23:58:01,203 - INFO - Epoch 1, Batch 17400: Loss = 8.0928\n",
      "2025-05-05 23:58:19,103 - INFO - Epoch 1, Batch 17500: Loss = 7.3607\n",
      "2025-05-05 23:58:37,004 - INFO - Epoch 1, Batch 17600: Loss = 7.0731\n",
      "2025-05-05 23:58:54,904 - INFO - Epoch 1, Batch 17700: Loss = 7.3504\n",
      "2025-05-05 23:59:12,804 - INFO - Epoch 1, Batch 17800: Loss = 7.4813\n",
      "2025-05-05 23:59:30,704 - INFO - Epoch 1, Batch 17900: Loss = 8.1573\n",
      "2025-05-05 23:59:48,604 - INFO - Epoch 1, Batch 18000: Loss = 7.0877\n",
      "2025-05-06 00:00:06,505 - INFO - Epoch 1, Batch 18100: Loss = 3.9429\n",
      "2025-05-06 00:00:24,404 - INFO - Epoch 1, Batch 18200: Loss = 7.5521\n",
      "2025-05-06 00:00:42,303 - INFO - Epoch 1, Batch 18300: Loss = 6.7385\n",
      "2025-05-06 00:01:00,205 - INFO - Epoch 1, Batch 18400: Loss = 7.2429\n",
      "2025-05-06 00:01:18,105 - INFO - Epoch 1, Batch 18500: Loss = 7.5880\n",
      "2025-05-06 00:01:36,005 - INFO - Epoch 1, Batch 18600: Loss = 7.2282\n",
      "2025-05-06 00:01:53,904 - INFO - Epoch 1, Batch 18700: Loss = 7.2570\n",
      "2025-05-06 00:02:11,803 - INFO - Epoch 1, Batch 18800: Loss = 6.8119\n",
      "2025-05-06 00:02:29,702 - INFO - Epoch 1, Batch 18900: Loss = 7.2506\n",
      "2025-05-06 00:02:47,602 - INFO - Epoch 1, Batch 19000: Loss = 4.2500\n",
      "2025-05-06 00:03:05,502 - INFO - Epoch 1, Batch 19100: Loss = 7.9873\n",
      "2025-05-06 00:03:23,399 - INFO - Epoch 1, Batch 19200: Loss = 6.9115\n",
      "2025-05-06 00:03:41,316 - INFO - Epoch 1, Batch 19300: Loss = 6.4137\n",
      "2025-05-06 00:03:59,214 - INFO - Epoch 1, Batch 19400: Loss = 8.0879\n",
      "2025-05-06 00:04:17,113 - INFO - Epoch 1, Batch 19500: Loss = 6.9422\n",
      "2025-05-06 00:04:35,011 - INFO - Epoch 1, Batch 19600: Loss = 7.9091\n",
      "2025-05-06 00:04:52,911 - INFO - Epoch 1, Batch 19700: Loss = 6.9606\n",
      "2025-05-06 00:05:10,813 - INFO - Epoch 1, Batch 19800: Loss = 7.6061\n",
      "2025-05-06 00:05:28,712 - INFO - Epoch 1, Batch 19900: Loss = 7.6910\n",
      "2025-05-06 00:05:46,611 - INFO - Epoch 1, Batch 20000: Loss = 6.7403\n",
      "2025-05-06 00:06:04,509 - INFO - Epoch 1, Batch 20100: Loss = 6.4062\n",
      "2025-05-06 00:06:22,408 - INFO - Epoch 1, Batch 20200: Loss = 6.6620\n",
      "2025-05-06 00:06:40,304 - INFO - Epoch 1, Batch 20300: Loss = 7.4238\n",
      "2025-05-06 00:06:58,203 - INFO - Epoch 1, Batch 20400: Loss = 6.2763\n",
      "2025-05-06 00:07:16,101 - INFO - Epoch 1, Batch 20500: Loss = 8.0267\n",
      "2025-05-06 00:07:33,999 - INFO - Epoch 1, Batch 20600: Loss = 7.3039\n",
      "2025-05-06 00:07:51,898 - INFO - Epoch 1, Batch 20700: Loss = 7.1302\n",
      "2025-05-06 00:08:09,796 - INFO - Epoch 1, Batch 20800: Loss = 7.7338\n",
      "2025-05-06 00:08:27,694 - INFO - Epoch 1, Batch 20900: Loss = 7.3964\n",
      "2025-05-06 00:08:45,594 - INFO - Epoch 1, Batch 21000: Loss = 7.8691\n",
      "2025-05-06 00:09:03,494 - INFO - Epoch 1, Batch 21100: Loss = 7.8558\n",
      "2025-05-06 00:09:21,391 - INFO - Epoch 1, Batch 21200: Loss = 6.3462\n",
      "2025-05-06 00:09:39,290 - INFO - Epoch 1, Batch 21300: Loss = 7.9923\n",
      "2025-05-06 00:09:57,189 - INFO - Epoch 1, Batch 21400: Loss = 7.4463\n",
      "2025-05-06 00:10:15,086 - INFO - Epoch 1, Batch 21500: Loss = 7.1029\n",
      "2025-05-06 00:10:32,981 - INFO - Epoch 1, Batch 21600: Loss = 7.5981\n",
      "2025-05-06 00:10:50,879 - INFO - Epoch 1, Batch 21700: Loss = 7.9019\n",
      "2025-05-06 00:11:08,777 - INFO - Epoch 1, Batch 21800: Loss = 7.8073\n",
      "2025-05-06 00:11:26,676 - INFO - Epoch 1, Batch 21900: Loss = 8.3976\n",
      "2025-05-06 00:11:44,575 - INFO - Epoch 1, Batch 22000: Loss = 8.4975\n",
      "2025-05-06 00:12:02,475 - INFO - Epoch 1, Batch 22100: Loss = 7.1218\n",
      "2025-05-06 00:12:20,374 - INFO - Epoch 1, Batch 22200: Loss = 8.0464\n",
      "2025-05-06 00:12:38,274 - INFO - Epoch 1, Batch 22300: Loss = 4.0149\n",
      "2025-05-06 00:12:56,173 - INFO - Epoch 1, Batch 22400: Loss = 7.3589\n",
      "2025-05-06 00:13:14,073 - INFO - Epoch 1, Batch 22500: Loss = 7.3222\n",
      "2025-05-06 00:13:31,972 - INFO - Epoch 1, Batch 22600: Loss = 7.1713\n",
      "2025-05-06 00:13:49,871 - INFO - Epoch 1, Batch 22700: Loss = 4.8457\n",
      "2025-05-06 00:14:07,771 - INFO - Epoch 1, Batch 22800: Loss = 7.4340\n",
      "2025-05-06 00:14:25,670 - INFO - Epoch 1, Batch 22900: Loss = 6.9679\n",
      "2025-05-06 00:14:43,570 - INFO - Epoch 1, Batch 23000: Loss = 7.3429\n",
      "2025-05-06 00:15:01,470 - INFO - Epoch 1, Batch 23100: Loss = 8.2166\n",
      "2025-05-06 00:15:19,371 - INFO - Epoch 1, Batch 23200: Loss = 7.1594\n",
      "2025-05-06 00:15:37,269 - INFO - Epoch 1, Batch 23300: Loss = 7.4560\n",
      "2025-05-06 00:15:55,167 - INFO - Epoch 1, Batch 23400: Loss = 7.3995\n",
      "2025-05-06 00:16:13,066 - INFO - Epoch 1, Batch 23500: Loss = 7.1592\n",
      "2025-05-06 00:16:30,964 - INFO - Epoch 1, Batch 23600: Loss = 7.2575\n",
      "2025-05-06 00:16:48,864 - INFO - Epoch 1, Batch 23700: Loss = 7.7784\n",
      "2025-05-06 00:17:06,762 - INFO - Epoch 1, Batch 23800: Loss = 8.8619\n",
      "2025-05-06 00:17:24,660 - INFO - Epoch 1, Batch 23900: Loss = 8.8532\n",
      "2025-05-06 00:17:42,557 - INFO - Epoch 1, Batch 24000: Loss = 8.1163\n",
      "2025-05-06 00:18:00,455 - INFO - Epoch 1, Batch 24100: Loss = 4.3640\n",
      "2025-05-06 00:18:18,354 - INFO - Epoch 1, Batch 24200: Loss = 7.4208\n",
      "2025-05-06 00:18:36,253 - INFO - Epoch 1, Batch 24300: Loss = 3.8753\n",
      "2025-05-06 00:18:54,150 - INFO - Epoch 1, Batch 24400: Loss = 7.3417\n",
      "2025-05-06 00:19:12,048 - INFO - Epoch 1, Batch 24500: Loss = 7.3712\n",
      "2025-05-06 00:19:29,948 - INFO - Epoch 1, Batch 24600: Loss = 7.2052\n",
      "2025-05-06 00:19:47,846 - INFO - Epoch 1, Batch 24700: Loss = 7.8653\n",
      "2025-05-06 00:20:05,746 - INFO - Epoch 1, Batch 24800: Loss = 6.7816\n",
      "2025-05-06 00:20:23,645 - INFO - Epoch 1, Batch 24900: Loss = 7.2076\n",
      "2025-05-06 00:20:41,541 - INFO - Epoch 1, Batch 25000: Loss = 7.3501\n",
      "2025-05-06 00:20:59,439 - INFO - Epoch 1, Batch 25100: Loss = 8.3008\n",
      "2025-05-06 00:21:17,334 - INFO - Epoch 1, Batch 25200: Loss = 7.4509\n",
      "2025-05-06 00:21:35,233 - INFO - Epoch 1, Batch 25300: Loss = 7.1768\n",
      "2025-05-06 00:21:53,129 - INFO - Epoch 1, Batch 25400: Loss = 5.3171\n",
      "2025-05-06 00:22:11,027 - INFO - Epoch 1, Batch 25500: Loss = 7.0768\n",
      "2025-05-06 00:22:28,925 - INFO - Epoch 1, Batch 25600: Loss = 7.5743\n",
      "2025-05-06 00:22:46,827 - INFO - Epoch 1, Batch 25700: Loss = 6.6444\n",
      "2025-05-06 00:23:04,726 - INFO - Epoch 1, Batch 25800: Loss = 8.1456\n",
      "2025-05-06 00:23:22,623 - INFO - Epoch 1, Batch 25900: Loss = 5.5666\n",
      "2025-05-06 00:23:40,521 - INFO - Epoch 1, Batch 26000: Loss = 6.9971\n",
      "2025-05-06 00:23:58,419 - INFO - Epoch 1, Batch 26100: Loss = 6.9828\n",
      "2025-05-06 00:24:16,318 - INFO - Epoch 1, Batch 26200: Loss = 7.2486\n",
      "2025-05-06 00:24:34,216 - INFO - Epoch 1, Batch 26300: Loss = 7.0609\n",
      "2025-05-06 00:24:52,112 - INFO - Epoch 1, Batch 26400: Loss = 7.4099\n",
      "2025-05-06 00:25:10,009 - INFO - Epoch 1, Batch 26500: Loss = 7.5051\n",
      "2025-05-06 00:25:27,905 - INFO - Epoch 1, Batch 26600: Loss = 6.8842\n",
      "2025-05-06 00:25:45,805 - INFO - Epoch 1, Batch 26700: Loss = 6.9921\n",
      "2025-05-06 00:26:03,702 - INFO - Epoch 1, Batch 26800: Loss = 7.4746\n",
      "2025-05-06 00:26:21,599 - INFO - Epoch 1, Batch 26900: Loss = 6.8345\n",
      "2025-05-06 00:26:39,497 - INFO - Epoch 1, Batch 27000: Loss = 6.7367\n",
      "2025-05-06 00:26:57,394 - INFO - Epoch 1, Batch 27100: Loss = 8.5504\n",
      "2025-05-06 00:27:15,292 - INFO - Epoch 1, Batch 27200: Loss = 7.3849\n",
      "2025-05-06 00:27:33,188 - INFO - Epoch 1, Batch 27300: Loss = 9.3393\n",
      "2025-05-06 00:27:51,085 - INFO - Epoch 1, Batch 27400: Loss = 5.9719\n",
      "2025-05-06 00:28:08,980 - INFO - Epoch 1, Batch 27500: Loss = 6.5819\n",
      "2025-05-06 00:28:26,876 - INFO - Epoch 1, Batch 27600: Loss = 7.1750\n",
      "2025-05-06 00:28:44,772 - INFO - Epoch 1, Batch 27700: Loss = 7.5802\n",
      "2025-05-06 00:29:02,669 - INFO - Epoch 1, Batch 27800: Loss = 7.9339\n",
      "2025-05-06 00:29:20,565 - INFO - Epoch 1, Batch 27900: Loss = 7.4634\n",
      "2025-05-06 00:29:38,463 - INFO - Epoch 1, Batch 28000: Loss = 7.6848\n",
      "2025-05-06 00:29:56,361 - INFO - Epoch 1, Batch 28100: Loss = 7.1975\n",
      "2025-05-06 00:30:14,258 - INFO - Epoch 1, Batch 28200: Loss = 6.9301\n",
      "2025-05-06 00:30:32,155 - INFO - Epoch 1, Batch 28300: Loss = 8.3259\n",
      "2025-05-06 00:30:50,052 - INFO - Epoch 1, Batch 28400: Loss = 6.9002\n",
      "2025-05-06 00:31:07,947 - INFO - Epoch 1, Batch 28500: Loss = 7.8183\n",
      "2025-05-06 00:31:25,846 - INFO - Epoch 1, Batch 28600: Loss = 8.1566\n",
      "2025-05-06 00:31:43,744 - INFO - Epoch 1, Batch 28700: Loss = 7.9415\n",
      "2025-05-06 00:32:01,641 - INFO - Epoch 1, Batch 28800: Loss = 6.7577\n",
      "2025-05-06 00:32:19,538 - INFO - Epoch 1, Batch 28900: Loss = 6.9801\n",
      "2025-05-06 00:32:37,434 - INFO - Epoch 1, Batch 29000: Loss = 7.4834\n",
      "2025-05-06 00:32:55,333 - INFO - Epoch 1, Batch 29100: Loss = 4.5840\n",
      "2025-05-06 00:33:13,231 - INFO - Epoch 1, Batch 29200: Loss = 7.9601\n",
      "2025-05-06 00:33:31,126 - INFO - Epoch 1, Batch 29300: Loss = 4.1453\n",
      "2025-05-06 00:33:49,022 - INFO - Epoch 1, Batch 29400: Loss = 7.1548\n",
      "2025-05-06 00:34:06,919 - INFO - Epoch 1, Batch 29500: Loss = 6.9403\n",
      "2025-05-06 00:34:24,819 - INFO - Epoch 1, Batch 29600: Loss = 7.5712\n",
      "2025-05-06 00:34:42,717 - INFO - Epoch 1, Batch 29700: Loss = 7.3983\n",
      "2025-05-06 00:35:00,617 - INFO - Epoch 1, Batch 29800: Loss = 6.8311\n",
      "2025-05-06 00:35:18,514 - INFO - Epoch 1, Batch 29900: Loss = 6.9866\n",
      "2025-05-06 00:35:36,414 - INFO - Epoch 1, Batch 30000: Loss = 7.9529\n",
      "2025-05-06 00:35:54,314 - INFO - Epoch 1, Batch 30100: Loss = 6.3435\n",
      "2025-05-06 00:36:12,212 - INFO - Epoch 1, Batch 30200: Loss = 7.6965\n",
      "2025-05-06 00:36:30,110 - INFO - Epoch 1, Batch 30300: Loss = 7.6571\n",
      "2025-05-06 00:36:48,009 - INFO - Epoch 1, Batch 30400: Loss = 7.7573\n",
      "2025-05-06 00:37:05,907 - INFO - Epoch 1, Batch 30500: Loss = 5.5890\n",
      "2025-05-06 00:37:23,804 - INFO - Epoch 1, Batch 30600: Loss = 6.7072\n",
      "2025-05-06 00:37:41,702 - INFO - Epoch 1, Batch 30700: Loss = 7.5082\n",
      "2025-05-06 00:37:59,598 - INFO - Epoch 1, Batch 30800: Loss = 6.7860\n",
      "2025-05-06 00:38:17,494 - INFO - Epoch 1, Batch 30900: Loss = 6.5741\n",
      "2025-05-06 00:38:35,391 - INFO - Epoch 1, Batch 31000: Loss = 8.0342\n",
      "2025-05-06 00:38:53,287 - INFO - Epoch 1, Batch 31100: Loss = 7.2766\n",
      "2025-05-06 00:39:11,184 - INFO - Epoch 1, Batch 31200: Loss = 8.1320\n",
      "2025-05-06 00:39:29,083 - INFO - Epoch 1, Batch 31300: Loss = 7.4486\n",
      "2025-05-06 00:39:46,981 - INFO - Epoch 1, Batch 31400: Loss = 8.5252\n",
      "2025-05-06 00:40:04,879 - INFO - Epoch 1, Batch 31500: Loss = 7.5298\n",
      "2025-05-06 00:40:22,778 - INFO - Epoch 1, Batch 31600: Loss = 7.7268\n",
      "2025-05-06 00:40:40,675 - INFO - Epoch 1, Batch 31700: Loss = 6.9181\n",
      "2025-05-06 00:40:58,573 - INFO - Epoch 1, Batch 31800: Loss = 7.7382\n",
      "2025-05-06 00:41:16,468 - INFO - Epoch 1, Batch 31900: Loss = 7.0629\n",
      "2025-05-06 00:41:34,366 - INFO - Epoch 1, Batch 32000: Loss = 7.5342\n",
      "2025-05-06 00:41:52,262 - INFO - Epoch 1, Batch 32100: Loss = 7.5100\n",
      "2025-05-06 00:42:10,160 - INFO - Epoch 1, Batch 32200: Loss = 6.2409\n",
      "2025-05-06 00:42:28,060 - INFO - Epoch 1, Batch 32300: Loss = 6.2856\n",
      "2025-05-06 00:42:45,959 - INFO - Epoch 1, Batch 32400: Loss = 7.6043\n",
      "2025-05-06 00:43:03,859 - INFO - Epoch 1, Batch 32500: Loss = 8.4198\n",
      "2025-05-06 00:43:21,756 - INFO - Epoch 1, Batch 32600: Loss = 6.6955\n",
      "2025-05-06 00:43:39,653 - INFO - Epoch 1, Batch 32700: Loss = 8.1606\n",
      "2025-05-06 00:43:57,551 - INFO - Epoch 1, Batch 32800: Loss = 6.8216\n",
      "2025-05-06 00:44:15,448 - INFO - Epoch 1, Batch 32900: Loss = 7.6256\n",
      "2025-05-06 00:44:33,345 - INFO - Epoch 1, Batch 33000: Loss = 6.8809\n",
      "2025-05-06 00:44:51,243 - INFO - Epoch 1, Batch 33100: Loss = 7.0786\n",
      "2025-05-06 00:45:09,140 - INFO - Epoch 1, Batch 33200: Loss = 7.8653\n",
      "2025-05-06 00:45:27,036 - INFO - Epoch 1, Batch 33300: Loss = 7.6456\n",
      "2025-05-06 00:45:44,934 - INFO - Epoch 1, Batch 33400: Loss = 6.8549\n",
      "2025-05-06 00:46:02,831 - INFO - Epoch 1, Batch 33500: Loss = 6.1152\n",
      "2025-05-06 00:46:20,729 - INFO - Epoch 1, Batch 33600: Loss = 5.4008\n",
      "2025-05-06 00:46:38,627 - INFO - Epoch 1, Batch 33700: Loss = 7.4128\n",
      "2025-05-06 00:46:56,525 - INFO - Epoch 1, Batch 33800: Loss = 6.5185\n",
      "2025-05-06 00:47:14,425 - INFO - Epoch 1, Batch 33900: Loss = 7.5197\n",
      "2025-05-06 00:47:32,323 - INFO - Epoch 1, Batch 34000: Loss = 7.0258\n",
      "2025-05-06 00:47:50,220 - INFO - Epoch 1, Batch 34100: Loss = 6.7424\n",
      "2025-05-06 00:48:08,116 - INFO - Epoch 1, Batch 34200: Loss = 7.1359\n",
      "2025-05-06 00:48:26,012 - INFO - Epoch 1, Batch 34300: Loss = 7.7492\n",
      "2025-05-06 00:48:43,912 - INFO - Epoch 1, Batch 34400: Loss = 6.9757\n",
      "2025-05-06 00:49:01,811 - INFO - Epoch 1, Batch 34500: Loss = 7.8009\n",
      "2025-05-06 00:49:19,710 - INFO - Epoch 1, Batch 34600: Loss = 7.3827\n",
      "2025-05-06 00:49:37,607 - INFO - Epoch 1, Batch 34700: Loss = 7.2859\n",
      "2025-05-06 00:49:55,505 - INFO - Epoch 1, Batch 34800: Loss = 7.6783\n",
      "2025-05-06 00:50:13,403 - INFO - Epoch 1, Batch 34900: Loss = 7.1680\n",
      "2025-05-06 00:50:31,299 - INFO - Epoch 1, Batch 35000: Loss = 3.9573\n",
      "2025-05-06 00:50:49,194 - INFO - Epoch 1, Batch 35100: Loss = 8.3979\n",
      "2025-05-06 00:51:07,091 - INFO - Epoch 1, Batch 35200: Loss = 8.0981\n",
      "2025-05-06 00:51:24,989 - INFO - Epoch 1, Batch 35300: Loss = 7.2484\n",
      "2025-05-06 00:51:42,886 - INFO - Epoch 1, Batch 35400: Loss = 7.9158\n",
      "2025-05-06 00:52:00,783 - INFO - Epoch 1, Batch 35500: Loss = 6.4196\n",
      "2025-05-06 00:52:18,681 - INFO - Epoch 1, Batch 35600: Loss = 6.6892\n",
      "2025-05-06 00:52:36,580 - INFO - Epoch 1, Batch 35700: Loss = 7.2402\n",
      "2025-05-06 00:52:54,477 - INFO - Epoch 1, Batch 35800: Loss = 6.6671\n",
      "2025-05-06 00:53:12,374 - INFO - Epoch 1, Batch 35900: Loss = 8.3229\n",
      "2025-05-06 00:53:30,270 - INFO - Epoch 1, Batch 36000: Loss = 6.9016\n",
      "2025-05-06 00:53:48,167 - INFO - Epoch 1, Batch 36100: Loss = 2.3031\n",
      "2025-05-06 00:54:06,064 - INFO - Epoch 1, Batch 36200: Loss = 5.3403\n",
      "2025-05-06 00:54:23,960 - INFO - Epoch 1, Batch 36300: Loss = 6.2849\n",
      "2025-05-06 00:54:41,857 - INFO - Epoch 1, Batch 36400: Loss = 7.0993\n",
      "2025-05-06 00:54:59,756 - INFO - Epoch 1, Batch 36500: Loss = 7.3439\n",
      "2025-05-06 00:55:17,655 - INFO - Epoch 1, Batch 36600: Loss = 6.6131\n",
      "2025-05-06 00:55:35,552 - INFO - Epoch 1, Batch 36700: Loss = 7.8841\n",
      "2025-05-06 00:55:53,451 - INFO - Epoch 1, Batch 36800: Loss = 4.2509\n",
      "2025-05-06 00:56:11,350 - INFO - Epoch 1, Batch 36900: Loss = 7.7898\n",
      "2025-05-06 00:56:29,250 - INFO - Epoch 1, Batch 37000: Loss = 6.7028\n",
      "2025-05-06 00:56:47,148 - INFO - Epoch 1, Batch 37100: Loss = 6.2883\n",
      "2025-05-06 00:57:05,044 - INFO - Epoch 1, Batch 37200: Loss = 7.2760\n",
      "2025-05-06 00:57:22,940 - INFO - Epoch 1, Batch 37300: Loss = 6.5485\n",
      "2025-05-06 00:57:40,838 - INFO - Epoch 1, Batch 37400: Loss = 6.3775\n",
      "2025-05-06 00:57:58,736 - INFO - Epoch 1, Batch 37500: Loss = 6.2088\n",
      "2025-05-06 00:58:16,633 - INFO - Epoch 1, Batch 37600: Loss = 7.5078\n",
      "2025-05-06 00:58:34,529 - INFO - Epoch 1, Batch 37700: Loss = 8.6355\n",
      "2025-05-06 00:58:52,425 - INFO - Epoch 1, Batch 37800: Loss = 6.0284\n",
      "2025-05-06 00:59:10,322 - INFO - Epoch 1, Batch 37900: Loss = 7.1494\n",
      "2025-05-06 00:59:28,221 - INFO - Epoch 1, Batch 38000: Loss = 7.6086\n",
      "2025-05-06 00:59:46,117 - INFO - Epoch 1, Batch 38100: Loss = 7.4707\n",
      "2025-05-06 01:00:04,017 - INFO - Epoch 1, Batch 38200: Loss = 7.6044\n",
      "2025-05-06 01:00:21,914 - INFO - Epoch 1, Batch 38300: Loss = 6.4690\n",
      "2025-05-06 01:00:39,812 - INFO - Epoch 1, Batch 38400: Loss = 6.1735\n",
      "2025-05-06 01:00:57,710 - INFO - Epoch 1, Batch 38500: Loss = 7.4940\n",
      "2025-05-06 01:01:15,606 - INFO - Epoch 1, Batch 38600: Loss = 6.6578\n",
      "2025-05-06 01:01:33,502 - INFO - Epoch 1, Batch 38700: Loss = 6.9121\n",
      "2025-05-06 01:01:51,399 - INFO - Epoch 1, Batch 38800: Loss = 5.3825\n",
      "2025-05-06 01:02:09,296 - INFO - Epoch 1, Batch 38900: Loss = 7.6949\n",
      "2025-05-06 01:02:27,195 - INFO - Epoch 1, Batch 39000: Loss = 7.3378\n",
      "2025-05-06 01:02:45,093 - INFO - Epoch 1, Batch 39100: Loss = 7.0714\n",
      "2025-05-06 01:03:02,991 - INFO - Epoch 1, Batch 39200: Loss = 6.7402\n",
      "2025-05-06 01:03:20,888 - INFO - Epoch 1, Batch 39300: Loss = 6.4076\n",
      "2025-05-06 01:03:38,785 - INFO - Epoch 1, Batch 39400: Loss = 7.8728\n",
      "2025-05-06 01:03:56,682 - INFO - Epoch 1, Batch 39500: Loss = 7.0657\n",
      "2025-05-06 01:04:14,582 - INFO - Epoch 1, Batch 39600: Loss = 7.1436\n",
      "2025-05-06 01:04:32,479 - INFO - Epoch 1, Batch 39700: Loss = 7.1471\n",
      "2025-05-06 01:04:50,378 - INFO - Epoch 1, Batch 39800: Loss = 7.8328\n",
      "2025-05-06 01:05:08,277 - INFO - Epoch 1, Batch 39900: Loss = 7.5385\n",
      "2025-05-06 01:05:26,175 - INFO - Epoch 1, Batch 40000: Loss = 7.1994\n",
      "2025-05-06 01:05:44,074 - INFO - Epoch 1, Batch 40100: Loss = 8.5287\n",
      "2025-05-06 01:06:01,970 - INFO - Epoch 1, Batch 40200: Loss = 7.1418\n",
      "2025-05-06 01:06:19,868 - INFO - Epoch 1, Batch 40300: Loss = 7.1975\n",
      "2025-05-06 01:06:37,766 - INFO - Epoch 1, Batch 40400: Loss = 6.3108\n",
      "2025-05-06 01:06:55,666 - INFO - Epoch 1, Batch 40500: Loss = 6.8101\n",
      "2025-05-06 01:07:13,564 - INFO - Epoch 1, Batch 40600: Loss = 3.8844\n",
      "2025-05-06 01:07:31,460 - INFO - Epoch 1, Batch 40700: Loss = 7.2695\n",
      "2025-05-06 01:07:49,357 - INFO - Epoch 1, Batch 40800: Loss = 6.9684\n",
      "2025-05-06 01:08:07,255 - INFO - Epoch 1, Batch 40900: Loss = 6.5402\n",
      "2025-05-06 01:08:25,153 - INFO - Epoch 1, Batch 41000: Loss = 6.6252\n",
      "2025-05-06 01:08:43,051 - INFO - Epoch 1, Batch 41100: Loss = 6.0849\n",
      "2025-05-06 01:09:00,947 - INFO - Epoch 1, Batch 41200: Loss = 7.0810\n",
      "2025-05-06 01:09:18,844 - INFO - Epoch 1, Batch 41300: Loss = 6.3531\n",
      "2025-05-06 01:09:36,744 - INFO - Epoch 1, Batch 41400: Loss = 7.5527\n",
      "2025-05-06 01:09:54,639 - INFO - Epoch 1, Batch 41500: Loss = 7.6295\n",
      "2025-05-06 01:10:12,537 - INFO - Epoch 1, Batch 41600: Loss = 6.8920\n",
      "2025-05-06 01:10:30,434 - INFO - Epoch 1, Batch 41700: Loss = 6.9316\n",
      "2025-05-06 01:10:48,330 - INFO - Epoch 1, Batch 41800: Loss = 7.8062\n",
      "2025-05-06 01:11:06,227 - INFO - Epoch 1, Batch 41900: Loss = 6.0728\n",
      "2025-05-06 01:11:24,125 - INFO - Epoch 1, Batch 42000: Loss = 6.5819\n",
      "2025-05-06 01:11:42,023 - INFO - Epoch 1, Batch 42100: Loss = 7.2884\n",
      "2025-05-06 01:11:59,920 - INFO - Epoch 1, Batch 42200: Loss = 7.1606\n",
      "2025-05-06 01:12:17,817 - INFO - Epoch 1, Batch 42300: Loss = 5.0050\n",
      "2025-05-06 01:12:35,713 - INFO - Epoch 1, Batch 42400: Loss = 6.3352\n",
      "2025-05-06 01:12:53,611 - INFO - Epoch 1, Batch 42500: Loss = 7.2179\n",
      "2025-05-06 01:13:11,508 - INFO - Epoch 1, Batch 42600: Loss = 8.1618\n",
      "2025-05-06 01:13:29,403 - INFO - Epoch 1, Batch 42700: Loss = 7.9406\n",
      "2025-05-06 01:13:47,301 - INFO - Epoch 1, Batch 42800: Loss = 7.1995\n",
      "2025-05-06 01:14:05,200 - INFO - Epoch 1, Batch 42900: Loss = 5.4500\n",
      "2025-05-06 01:14:23,099 - INFO - Epoch 1, Batch 43000: Loss = 4.2441\n",
      "2025-05-06 01:14:40,998 - INFO - Epoch 1, Batch 43100: Loss = 7.2200\n",
      "2025-05-06 01:14:58,896 - INFO - Epoch 1, Batch 43200: Loss = 6.5740\n",
      "2025-05-06 01:15:16,796 - INFO - Epoch 1, Batch 43300: Loss = 7.9500\n",
      "2025-05-06 01:15:34,694 - INFO - Epoch 1, Batch 43400: Loss = 6.7970\n",
      "2025-05-06 01:15:52,593 - INFO - Epoch 1, Batch 43500: Loss = 8.5588\n",
      "2025-05-06 01:16:10,491 - INFO - Epoch 1, Batch 43600: Loss = 7.4605\n",
      "2025-05-06 01:16:28,390 - INFO - Epoch 1, Batch 43700: Loss = 5.4444\n",
      "2025-05-06 01:16:46,288 - INFO - Epoch 1, Batch 43800: Loss = 6.8568\n",
      "2025-05-06 01:17:04,187 - INFO - Epoch 1, Batch 43900: Loss = 5.4650\n",
      "2025-05-06 01:17:22,084 - INFO - Epoch 1, Batch 44000: Loss = 6.9055\n",
      "2025-05-06 01:17:39,982 - INFO - Epoch 1, Batch 44100: Loss = 7.5218\n",
      "2025-05-06 01:17:57,880 - INFO - Epoch 1, Batch 44200: Loss = 5.8582\n",
      "2025-05-06 01:18:15,780 - INFO - Epoch 1, Batch 44300: Loss = 5.9109\n",
      "2025-05-06 01:18:33,676 - INFO - Epoch 1, Batch 44400: Loss = 8.0184\n",
      "2025-05-06 01:18:51,573 - INFO - Epoch 1, Batch 44500: Loss = 6.4225\n",
      "2025-05-06 01:19:09,470 - INFO - Epoch 1, Batch 44600: Loss = 5.4896\n",
      "2025-05-06 01:19:27,368 - INFO - Epoch 1, Batch 44700: Loss = 6.8782\n",
      "2025-05-06 01:19:45,265 - INFO - Epoch 1, Batch 44800: Loss = 7.5383\n",
      "2025-05-06 01:20:03,165 - INFO - Epoch 1, Batch 44900: Loss = 7.6502\n",
      "2025-05-06 01:20:21,064 - INFO - Epoch 1, Batch 45000: Loss = 6.8602\n",
      "2025-05-06 01:20:38,962 - INFO - Epoch 1, Batch 45100: Loss = 7.7678\n",
      "2025-05-06 01:20:56,859 - INFO - Epoch 1, Batch 45200: Loss = 6.9649\n",
      "2025-05-06 01:21:14,759 - INFO - Epoch 1, Batch 45300: Loss = 4.4073\n",
      "2025-05-06 01:21:32,658 - INFO - Epoch 1, Batch 45400: Loss = 7.0471\n",
      "2025-05-06 01:21:50,556 - INFO - Epoch 1, Batch 45500: Loss = 5.8216\n",
      "2025-05-06 01:22:08,455 - INFO - Epoch 1, Batch 45600: Loss = 7.6113\n",
      "2025-05-06 01:22:26,355 - INFO - Epoch 1, Batch 45700: Loss = 7.9278\n",
      "2025-05-06 01:22:44,254 - INFO - Epoch 1, Batch 45800: Loss = 6.8526\n",
      "2025-05-06 01:23:02,155 - INFO - Epoch 1, Batch 45900: Loss = 7.1001\n",
      "2025-05-06 01:23:20,054 - INFO - Epoch 1, Batch 46000: Loss = 7.4797\n",
      "2025-05-06 01:23:37,954 - INFO - Epoch 1, Batch 46100: Loss = 7.1537\n",
      "2025-05-06 01:23:55,855 - INFO - Epoch 1, Batch 46200: Loss = 7.1304\n",
      "2025-05-06 01:24:13,753 - INFO - Epoch 1, Batch 46300: Loss = 7.1702\n",
      "2025-05-06 01:24:31,653 - INFO - Epoch 1, Batch 46400: Loss = 7.2140\n",
      "2025-05-06 01:24:49,554 - INFO - Epoch 1, Batch 46500: Loss = 7.2132\n",
      "2025-05-06 01:25:07,453 - INFO - Epoch 1, Batch 46600: Loss = 6.8489\n",
      "2025-05-06 01:25:25,353 - INFO - Epoch 1, Batch 46700: Loss = 6.3220\n",
      "2025-05-06 01:25:43,254 - INFO - Epoch 1, Batch 46800: Loss = 6.4607\n",
      "2025-05-06 01:26:01,152 - INFO - Epoch 1, Batch 46900: Loss = 5.7260\n",
      "2025-05-06 01:26:19,049 - INFO - Epoch 1, Batch 47000: Loss = 6.7824\n",
      "2025-05-06 01:26:36,948 - INFO - Epoch 1, Batch 47100: Loss = 7.2567\n",
      "2025-05-06 01:26:54,847 - INFO - Epoch 1, Batch 47200: Loss = 7.0515\n",
      "2025-05-06 01:27:12,745 - INFO - Epoch 1, Batch 47300: Loss = 7.7153\n",
      "2025-05-06 01:27:30,644 - INFO - Epoch 1, Batch 47400: Loss = 8.4831\n",
      "2025-05-06 01:27:48,541 - INFO - Epoch 1, Batch 47500: Loss = 7.9120\n",
      "2025-05-06 01:28:06,438 - INFO - Epoch 1, Batch 47600: Loss = 7.0300\n",
      "2025-05-06 01:28:24,337 - INFO - Epoch 1, Batch 47700: Loss = 7.5590\n",
      "2025-05-06 01:28:42,235 - INFO - Epoch 1, Batch 47800: Loss = 6.6728\n",
      "2025-05-06 01:29:00,132 - INFO - Epoch 1, Batch 47900: Loss = 7.2214\n",
      "2025-05-06 01:29:18,029 - INFO - Epoch 1, Batch 48000: Loss = 7.1536\n",
      "2025-05-06 01:29:35,929 - INFO - Epoch 1, Batch 48100: Loss = 7.3850\n",
      "2025-05-06 01:29:53,828 - INFO - Epoch 1, Batch 48200: Loss = 6.9936\n",
      "2025-05-06 01:30:11,724 - INFO - Epoch 1, Batch 48300: Loss = 5.0919\n",
      "2025-05-06 01:30:29,621 - INFO - Epoch 1, Batch 48400: Loss = 7.8513\n",
      "2025-05-06 01:30:47,519 - INFO - Epoch 1, Batch 48500: Loss = 7.3138\n",
      "2025-05-06 01:31:05,418 - INFO - Epoch 1, Batch 48600: Loss = 7.1919\n",
      "2025-05-06 01:31:23,316 - INFO - Epoch 1, Batch 48700: Loss = 7.1644\n",
      "2025-05-06 01:31:41,216 - INFO - Epoch 1, Batch 48800: Loss = 5.7960\n",
      "2025-05-06 01:31:59,116 - INFO - Epoch 1, Batch 48900: Loss = 7.0796\n",
      "2025-05-06 01:32:17,017 - INFO - Epoch 1, Batch 49000: Loss = 6.5856\n",
      "2025-05-06 01:32:34,916 - INFO - Epoch 1, Batch 49100: Loss = 7.1983\n",
      "2025-05-06 01:32:52,815 - INFO - Epoch 1, Batch 49200: Loss = 6.5663\n",
      "2025-05-06 01:33:10,713 - INFO - Epoch 1, Batch 49300: Loss = 7.4500\n",
      "2025-05-06 01:33:28,611 - INFO - Epoch 1, Batch 49400: Loss = 6.9126\n",
      "2025-05-06 01:33:46,510 - INFO - Epoch 1, Batch 49500: Loss = 8.5763\n",
      "2025-05-06 01:34:04,408 - INFO - Epoch 1, Batch 49600: Loss = 7.4766\n",
      "2025-05-06 01:34:22,305 - INFO - Epoch 1, Batch 49700: Loss = 6.7537\n",
      "2025-05-06 01:34:40,205 - INFO - Epoch 1, Batch 49800: Loss = 7.8289\n",
      "2025-05-06 01:34:58,103 - INFO - Epoch 1, Batch 49900: Loss = 8.6697\n",
      "2025-05-06 01:35:16,001 - INFO - Epoch 1, Batch 50000: Loss = 6.7541\n",
      "2025-05-06 01:35:33,898 - INFO - Epoch 1, Batch 50100: Loss = 7.5713\n",
      "2025-05-06 01:35:51,799 - INFO - Epoch 1, Batch 50200: Loss = 7.3460\n",
      "2025-05-06 01:36:09,698 - INFO - Epoch 1, Batch 50300: Loss = 7.4103\n",
      "2025-05-06 01:36:27,598 - INFO - Epoch 1, Batch 50400: Loss = 7.5002\n",
      "2025-05-06 01:36:45,496 - INFO - Epoch 1, Batch 50500: Loss = 7.9019\n",
      "2025-05-06 01:37:03,397 - INFO - Epoch 1, Batch 50600: Loss = 7.5900\n",
      "2025-05-06 01:37:21,293 - INFO - Epoch 1, Batch 50700: Loss = 7.1558\n",
      "2025-05-06 01:37:39,191 - INFO - Epoch 1, Batch 50800: Loss = 7.7555\n",
      "2025-05-06 01:37:57,090 - INFO - Epoch 1, Batch 50900: Loss = 6.8717\n",
      "2025-05-06 01:38:14,990 - INFO - Epoch 1, Batch 51000: Loss = 3.5164\n",
      "2025-05-06 01:38:32,890 - INFO - Epoch 1, Batch 51100: Loss = 4.6438\n",
      "2025-05-06 01:38:50,789 - INFO - Epoch 1, Batch 51200: Loss = 8.0656\n",
      "2025-05-06 01:39:08,690 - INFO - Epoch 1, Batch 51300: Loss = 8.6035\n",
      "2025-05-06 01:39:26,591 - INFO - Epoch 1, Batch 51400: Loss = 5.3725\n",
      "2025-05-06 01:39:44,490 - INFO - Epoch 1, Batch 51500: Loss = 8.3616\n",
      "2025-05-06 01:40:02,391 - INFO - Epoch 1, Batch 51600: Loss = 6.7380\n",
      "2025-05-06 01:40:20,291 - INFO - Epoch 1, Batch 51700: Loss = 7.5990\n",
      "2025-05-06 01:40:38,192 - INFO - Epoch 1, Batch 51800: Loss = 8.1428\n",
      "2025-05-06 01:40:56,092 - INFO - Epoch 1, Batch 51900: Loss = 7.5912\n",
      "2025-05-06 01:41:13,989 - INFO - Epoch 1, Batch 52000: Loss = 6.8635\n",
      "2025-05-06 01:41:31,889 - INFO - Epoch 1, Batch 52100: Loss = 7.0833\n",
      "2025-05-06 01:41:49,790 - INFO - Epoch 1, Batch 52200: Loss = 7.7065\n",
      "2025-05-06 01:42:07,691 - INFO - Epoch 1, Batch 52300: Loss = 6.4136\n",
      "2025-05-06 01:42:25,589 - INFO - Epoch 1, Batch 52400: Loss = 7.3862\n",
      "2025-05-06 01:42:43,489 - INFO - Epoch 1, Batch 52500: Loss = 7.5507\n",
      "2025-05-06 01:43:01,390 - INFO - Epoch 1, Batch 52600: Loss = 7.2748\n",
      "2025-05-06 01:43:19,287 - INFO - Epoch 1, Batch 52700: Loss = 7.6443\n",
      "2025-05-06 01:43:37,185 - INFO - Epoch 1, Batch 52800: Loss = 6.8077\n",
      "2025-05-06 01:43:55,083 - INFO - Epoch 1, Batch 52900: Loss = 6.6513\n",
      "2025-05-06 01:44:12,983 - INFO - Epoch 1, Batch 53000: Loss = 7.3323\n",
      "2025-05-06 01:44:30,882 - INFO - Epoch 1, Batch 53100: Loss = 5.0808\n",
      "2025-05-06 01:44:48,782 - INFO - Epoch 1, Batch 53200: Loss = 7.1705\n",
      "2025-05-06 01:45:06,682 - INFO - Epoch 1, Batch 53300: Loss = 8.2413\n",
      "2025-05-06 01:45:24,582 - INFO - Epoch 1, Batch 53400: Loss = 8.0579\n",
      "2025-05-06 01:45:42,479 - INFO - Epoch 1, Batch 53500: Loss = 7.9565\n",
      "2025-05-06 01:46:00,380 - INFO - Epoch 1, Batch 53600: Loss = 7.8307\n",
      "2025-05-06 01:46:18,278 - INFO - Epoch 1, Batch 53700: Loss = 6.9961\n",
      "2025-05-06 01:46:36,177 - INFO - Epoch 1, Batch 53800: Loss = 7.8695\n",
      "2025-05-06 01:46:54,075 - INFO - Epoch 1, Batch 53900: Loss = 7.7187\n",
      "2025-05-06 01:47:11,972 - INFO - Epoch 1, Batch 54000: Loss = 7.2563\n",
      "2025-05-06 01:47:29,871 - INFO - Epoch 1, Batch 54100: Loss = 6.0050\n",
      "2025-05-06 01:47:47,769 - INFO - Epoch 1, Batch 54200: Loss = 7.5643\n",
      "2025-05-06 01:48:05,667 - INFO - Epoch 1, Batch 54300: Loss = 7.0700\n",
      "2025-05-06 01:48:23,563 - INFO - Epoch 1, Batch 54400: Loss = 7.8006\n",
      "2025-05-06 01:48:41,460 - INFO - Epoch 1, Batch 54500: Loss = 6.5927\n",
      "2025-05-06 01:48:59,357 - INFO - Epoch 1, Batch 54600: Loss = 6.2666\n",
      "2025-05-06 01:49:17,255 - INFO - Epoch 1, Batch 54700: Loss = 7.0107\n",
      "2025-05-06 01:49:35,152 - INFO - Epoch 1, Batch 54800: Loss = 8.2360\n",
      "2025-05-06 01:49:53,052 - INFO - Epoch 1, Batch 54900: Loss = 6.9958\n",
      "2025-05-06 01:50:10,948 - INFO - Epoch 1, Batch 55000: Loss = 6.7520\n",
      "2025-05-06 01:50:28,845 - INFO - Epoch 1, Batch 55100: Loss = 7.1671\n",
      "2025-05-06 01:50:46,741 - INFO - Epoch 1, Batch 55200: Loss = 6.6438\n",
      "2025-05-06 01:51:04,639 - INFO - Epoch 1, Batch 55300: Loss = 8.2207\n",
      "2025-05-06 01:51:22,536 - INFO - Epoch 1, Batch 55400: Loss = 5.6279\n",
      "2025-05-06 01:51:40,433 - INFO - Epoch 1, Batch 55500: Loss = 7.4953\n",
      "2025-05-06 01:51:58,332 - INFO - Epoch 1, Batch 55600: Loss = 7.5532\n",
      "2025-05-06 01:52:16,232 - INFO - Epoch 1, Batch 55700: Loss = 6.3768\n",
      "2025-05-06 01:52:34,130 - INFO - Epoch 1, Batch 55800: Loss = 6.5421\n",
      "2025-05-06 01:52:52,029 - INFO - Epoch 1, Batch 55900: Loss = 6.8270\n",
      "2025-05-06 01:53:09,927 - INFO - Epoch 1, Batch 56000: Loss = 7.1596\n",
      "2025-05-06 01:53:27,824 - INFO - Epoch 1, Batch 56100: Loss = 6.0716\n",
      "2025-05-06 01:53:45,724 - INFO - Epoch 1, Batch 56200: Loss = 7.0072\n",
      "2025-05-06 01:54:03,621 - INFO - Epoch 1, Batch 56300: Loss = 7.5900\n",
      "2025-05-06 01:54:21,521 - INFO - Epoch 1, Batch 56400: Loss = 5.7521\n",
      "2025-05-06 01:54:39,421 - INFO - Epoch 1, Batch 56500: Loss = 4.0282\n",
      "2025-05-06 01:54:57,322 - INFO - Epoch 1, Batch 56600: Loss = 5.8364\n",
      "2025-05-06 01:55:15,224 - INFO - Epoch 1, Batch 56700: Loss = 7.3714\n",
      "2025-05-06 01:55:33,124 - INFO - Epoch 1, Batch 56800: Loss = 7.0314\n",
      "2025-05-06 01:55:51,027 - INFO - Epoch 1, Batch 56900: Loss = 8.2137\n",
      "2025-05-06 01:56:08,926 - INFO - Epoch 1, Batch 57000: Loss = 4.6761\n",
      "2025-05-06 01:56:26,827 - INFO - Epoch 1, Batch 57100: Loss = 6.9523\n",
      "2025-05-06 01:56:44,726 - INFO - Epoch 1, Batch 57200: Loss = 6.7283\n",
      "2025-05-06 01:57:02,625 - INFO - Epoch 1, Batch 57300: Loss = 7.8487\n",
      "2025-05-06 01:57:20,525 - INFO - Epoch 1, Batch 57400: Loss = 6.8909\n",
      "2025-05-06 01:57:38,423 - INFO - Epoch 1, Batch 57500: Loss = 9.5963\n",
      "2025-05-06 01:57:56,322 - INFO - Epoch 1, Batch 57600: Loss = 6.8206\n",
      "2025-05-06 01:58:14,221 - INFO - Epoch 1, Batch 57700: Loss = 7.1003\n",
      "2025-05-06 01:58:32,120 - INFO - Epoch 1, Batch 57800: Loss = 6.6339\n",
      "2025-05-06 01:58:50,019 - INFO - Epoch 1, Batch 57900: Loss = 7.4427\n",
      "2025-05-06 01:59:07,916 - INFO - Epoch 1, Batch 58000: Loss = 7.2582\n",
      "2025-05-06 01:59:25,814 - INFO - Epoch 1, Batch 58100: Loss = 7.0540\n",
      "2025-05-06 01:59:43,711 - INFO - Epoch 1, Batch 58200: Loss = 6.8934\n",
      "2025-05-06 02:00:01,611 - INFO - Epoch 1, Batch 58300: Loss = 7.5545\n",
      "2025-05-06 02:00:19,508 - INFO - Epoch 1, Batch 58400: Loss = 6.4660\n",
      "2025-05-06 02:00:37,407 - INFO - Epoch 1, Batch 58500: Loss = 7.6634\n",
      "2025-05-06 02:00:55,308 - INFO - Epoch 1, Batch 58600: Loss = 6.6313\n",
      "2025-05-06 02:01:13,207 - INFO - Epoch 1, Batch 58700: Loss = 6.8641\n",
      "2025-05-06 02:01:31,107 - INFO - Epoch 1, Batch 58800: Loss = 7.8394\n",
      "2025-05-06 02:01:49,006 - INFO - Epoch 1, Batch 58900: Loss = 9.1685\n",
      "2025-05-06 02:02:06,906 - INFO - Epoch 1, Batch 59000: Loss = 7.1229\n",
      "2025-05-06 02:02:24,806 - INFO - Epoch 1, Batch 59100: Loss = 7.6902\n",
      "2025-05-06 02:02:42,705 - INFO - Epoch 1, Batch 59200: Loss = 6.6068\n",
      "2025-05-06 02:03:00,605 - INFO - Epoch 1, Batch 59300: Loss = 8.5057\n",
      "2025-05-06 02:03:18,503 - INFO - Epoch 1, Batch 59400: Loss = 8.3104\n",
      "2025-05-06 02:03:36,402 - INFO - Epoch 1, Batch 59500: Loss = 8.1479\n",
      "2025-05-06 02:03:54,303 - INFO - Epoch 1, Batch 59600: Loss = 6.4442\n",
      "2025-05-06 02:04:12,202 - INFO - Epoch 1, Batch 59700: Loss = 5.4335\n",
      "2025-05-06 02:04:30,099 - INFO - Epoch 1, Batch 59800: Loss = 6.5832\n",
      "2025-05-06 02:04:47,998 - INFO - Epoch 1, Batch 59900: Loss = 5.8960\n",
      "2025-05-06 02:05:05,895 - INFO - Epoch 1, Batch 60000: Loss = 6.7787\n",
      "2025-05-06 02:05:23,794 - INFO - Epoch 1, Batch 60100: Loss = 8.5156\n",
      "2025-05-06 02:05:41,692 - INFO - Epoch 1, Batch 60200: Loss = 7.2289\n",
      "2025-05-06 02:05:59,593 - INFO - Epoch 1, Batch 60300: Loss = 6.2661\n",
      "2025-05-06 02:06:17,492 - INFO - Epoch 1, Batch 60400: Loss = 6.8267\n",
      "2025-05-06 02:06:35,392 - INFO - Epoch 1, Batch 60500: Loss = 7.4984\n",
      "2025-05-06 02:06:53,290 - INFO - Epoch 1, Batch 60600: Loss = 6.0852\n",
      "2025-05-06 02:07:11,188 - INFO - Epoch 1, Batch 60700: Loss = 7.9683\n",
      "2025-05-06 02:07:29,084 - INFO - Epoch 1, Batch 60800: Loss = 7.4310\n",
      "2025-05-06 02:07:46,981 - INFO - Epoch 1, Batch 60900: Loss = 6.6631\n",
      "2025-05-06 02:08:04,883 - INFO - Epoch 1, Batch 61000: Loss = 7.0669\n",
      "2025-05-06 02:08:22,782 - INFO - Epoch 1, Batch 61100: Loss = 6.8475\n",
      "2025-05-06 02:08:40,682 - INFO - Epoch 1, Batch 61200: Loss = 6.7271\n",
      "2025-05-06 02:08:58,581 - INFO - Epoch 1, Batch 61300: Loss = 7.9350\n",
      "2025-05-06 02:09:16,479 - INFO - Epoch 1, Batch 61400: Loss = 6.1028\n",
      "2025-05-06 02:09:34,380 - INFO - Epoch 1, Batch 61500: Loss = 6.8289\n",
      "2025-05-06 02:09:52,281 - INFO - Epoch 1, Batch 61600: Loss = 6.7143\n",
      "2025-05-06 02:10:10,181 - INFO - Epoch 1, Batch 61700: Loss = 7.2361\n",
      "2025-05-06 02:10:28,079 - INFO - Epoch 1, Batch 61800: Loss = 7.9126\n",
      "2025-05-06 02:10:45,978 - INFO - Epoch 1, Batch 61900: Loss = 7.6006\n",
      "2025-05-06 02:11:03,876 - INFO - Epoch 1, Batch 62000: Loss = 7.4896\n",
      "2025-05-06 02:11:21,773 - INFO - Epoch 1, Batch 62100: Loss = 7.4605\n",
      "2025-05-06 02:11:39,670 - INFO - Epoch 1, Batch 62200: Loss = 7.7770\n",
      "2025-05-06 02:11:57,569 - INFO - Epoch 1, Batch 62300: Loss = 7.4262\n",
      "2025-05-06 02:12:15,466 - INFO - Epoch 1, Batch 62400: Loss = 7.2766\n",
      "2025-05-06 02:12:33,366 - INFO - Epoch 1, Batch 62500: Loss = 5.2910\n",
      "2025-05-06 02:12:51,266 - INFO - Epoch 1, Batch 62600: Loss = 7.0231\n",
      "2025-05-06 02:13:09,164 - INFO - Epoch 1, Batch 62700: Loss = 7.2670\n",
      "2025-05-06 02:13:27,063 - INFO - Epoch 1, Batch 62800: Loss = 7.0706\n",
      "2025-05-06 02:13:44,962 - INFO - Epoch 1, Batch 62900: Loss = 7.2811\n",
      "2025-05-06 02:14:02,860 - INFO - Epoch 1, Batch 63000: Loss = 7.3733\n",
      "2025-05-06 02:14:20,761 - INFO - Epoch 1, Batch 63100: Loss = 7.3749\n",
      "2025-05-06 02:14:38,660 - INFO - Epoch 1, Batch 63200: Loss = 7.4752\n",
      "2025-05-06 02:14:56,557 - INFO - Epoch 1, Batch 63300: Loss = 7.4513\n",
      "2025-05-06 02:15:14,454 - INFO - Epoch 1, Batch 63400: Loss = 6.9773\n",
      "2025-05-06 02:15:32,352 - INFO - Epoch 1, Batch 63500: Loss = 6.4798\n",
      "2025-05-06 02:15:50,252 - INFO - Epoch 1, Batch 63600: Loss = 6.5776\n",
      "2025-05-06 02:16:08,152 - INFO - Epoch 1, Batch 63700: Loss = 3.5622\n",
      "2025-05-06 02:16:26,050 - INFO - Epoch 1, Batch 63800: Loss = 6.6523\n",
      "2025-05-06 02:16:43,948 - INFO - Epoch 1, Batch 63900: Loss = 6.9960\n",
      "2025-05-06 02:17:01,849 - INFO - Epoch 1, Batch 64000: Loss = 7.2703\n",
      "2025-05-06 02:17:19,749 - INFO - Epoch 1, Batch 64100: Loss = 7.7815\n",
      "2025-05-06 02:17:37,649 - INFO - Epoch 1, Batch 64200: Loss = 7.4498\n",
      "2025-05-06 02:17:55,551 - INFO - Epoch 1, Batch 64300: Loss = 6.5786\n",
      "2025-05-06 02:18:13,448 - INFO - Epoch 1, Batch 64400: Loss = 6.9653\n",
      "2025-05-06 02:18:31,349 - INFO - Epoch 1, Batch 64500: Loss = 8.4754\n",
      "2025-05-06 02:18:49,249 - INFO - Epoch 1, Batch 64600: Loss = 7.4769\n",
      "2025-05-06 02:19:07,150 - INFO - Epoch 1, Batch 64700: Loss = 7.0686\n",
      "2025-05-06 02:19:25,049 - INFO - Epoch 1, Batch 64800: Loss = 7.6225\n",
      "2025-05-06 02:19:42,949 - INFO - Epoch 1, Batch 64900: Loss = 7.8002\n",
      "2025-05-06 02:20:00,847 - INFO - Epoch 1, Batch 65000: Loss = 7.3492\n",
      "2025-05-06 02:20:18,747 - INFO - Epoch 1, Batch 65100: Loss = 7.3744\n",
      "2025-05-06 02:20:36,647 - INFO - Epoch 1, Batch 65200: Loss = 7.3896\n",
      "2025-05-06 02:20:54,546 - INFO - Epoch 1, Batch 65300: Loss = 6.8265\n",
      "2025-05-06 02:21:12,446 - INFO - Epoch 1, Batch 65400: Loss = 6.9128\n",
      "2025-05-06 02:21:30,345 - INFO - Epoch 1, Batch 65500: Loss = 6.3452\n",
      "2025-05-06 02:21:48,244 - INFO - Epoch 1, Batch 65600: Loss = 8.4309\n",
      "2025-05-06 02:22:06,143 - INFO - Epoch 1, Batch 65700: Loss = 6.1564\n",
      "2025-05-06 02:22:24,040 - INFO - Epoch 1, Batch 65800: Loss = 7.5021\n",
      "2025-05-06 02:22:41,940 - INFO - Epoch 1, Batch 65900: Loss = 7.5800\n",
      "2025-05-06 02:22:59,840 - INFO - Epoch 1, Batch 66000: Loss = 11.2259\n",
      "2025-05-06 02:23:17,739 - INFO - Epoch 1, Batch 66100: Loss = 6.2835\n",
      "2025-05-06 02:23:35,639 - INFO - Epoch 1, Batch 66200: Loss = 7.8890\n",
      "2025-05-06 02:23:53,537 - INFO - Epoch 1, Batch 66300: Loss = 7.0845\n",
      "2025-05-06 02:24:11,434 - INFO - Epoch 1, Batch 66400: Loss = 7.2018\n",
      "2025-05-06 02:24:29,333 - INFO - Epoch 1, Batch 66500: Loss = 7.1775\n",
      "2025-05-06 02:24:47,232 - INFO - Epoch 1, Batch 66600: Loss = 7.1832\n",
      "2025-05-06 02:25:05,132 - INFO - Epoch 1, Batch 66700: Loss = 7.0117\n",
      "2025-05-06 02:25:23,030 - INFO - Epoch 1, Batch 66800: Loss = 7.0734\n",
      "2025-05-06 02:25:40,930 - INFO - Epoch 1, Batch 66900: Loss = 7.4630\n",
      "2025-05-06 02:25:58,832 - INFO - Epoch 1, Batch 67000: Loss = 6.7450\n",
      "2025-05-06 02:26:16,733 - INFO - Epoch 1, Batch 67100: Loss = 7.8671\n",
      "2025-05-06 02:26:34,633 - INFO - Epoch 1, Batch 67200: Loss = 7.5700\n",
      "2025-05-06 02:26:52,533 - INFO - Epoch 1, Batch 67300: Loss = 7.9817\n",
      "2025-05-06 02:27:10,433 - INFO - Epoch 1, Batch 67400: Loss = 5.3593\n",
      "2025-05-06 02:27:28,332 - INFO - Epoch 1, Batch 67500: Loss = 7.0450\n",
      "2025-05-06 02:27:46,234 - INFO - Epoch 1, Batch 67600: Loss = 6.4696\n",
      "2025-05-06 02:28:04,135 - INFO - Epoch 1, Batch 67700: Loss = 7.6631\n",
      "2025-05-06 02:28:22,034 - INFO - Epoch 1, Batch 67800: Loss = 7.1512\n",
      "2025-05-06 02:28:39,936 - INFO - Epoch 1, Batch 67900: Loss = 7.5946\n",
      "2025-05-06 02:28:57,837 - INFO - Epoch 1, Batch 68000: Loss = 7.4674\n",
      "2025-05-06 02:29:15,736 - INFO - Epoch 1, Batch 68100: Loss = 8.1361\n",
      "2025-05-06 02:29:33,637 - INFO - Epoch 1, Batch 68200: Loss = 7.4098\n",
      "2025-05-06 02:29:51,536 - INFO - Epoch 1, Batch 68300: Loss = 7.4983\n",
      "2025-05-06 02:30:09,433 - INFO - Epoch 1, Batch 68400: Loss = 6.9381\n",
      "2025-05-06 02:30:27,333 - INFO - Epoch 1, Batch 68500: Loss = 6.8473\n",
      "2025-05-06 02:30:45,232 - INFO - Epoch 1, Batch 68600: Loss = 7.6302\n",
      "2025-05-06 02:31:03,132 - INFO - Epoch 1, Batch 68700: Loss = 7.0625\n",
      "2025-05-06 02:31:21,030 - INFO - Epoch 1, Batch 68800: Loss = 6.7403\n",
      "2025-05-06 02:31:38,932 - INFO - Epoch 1, Batch 68900: Loss = 5.3404\n",
      "2025-05-06 02:31:56,833 - INFO - Epoch 1, Batch 69000: Loss = 7.5890\n",
      "2025-05-06 02:32:14,732 - INFO - Epoch 1, Batch 69100: Loss = 6.7883\n",
      "2025-05-06 02:32:32,633 - INFO - Epoch 1, Batch 69200: Loss = 7.5842\n",
      "2025-05-06 02:32:50,533 - INFO - Epoch 1, Batch 69300: Loss = 6.3249\n",
      "2025-05-06 02:33:08,433 - INFO - Epoch 1, Batch 69400: Loss = 7.4617\n",
      "2025-05-06 02:33:26,332 - INFO - Epoch 1, Batch 69500: Loss = 7.9048\n",
      "2025-05-06 02:33:44,233 - INFO - Epoch 1, Batch 69600: Loss = 3.8753\n",
      "2025-05-06 02:34:02,134 - INFO - Epoch 1, Batch 69700: Loss = 5.1623\n",
      "2025-05-06 02:34:20,032 - INFO - Epoch 1, Batch 69800: Loss = 6.7622\n",
      "2025-05-06 02:34:37,930 - INFO - Epoch 1, Batch 69900: Loss = 6.7239\n",
      "2025-05-06 02:34:55,832 - INFO - Epoch 1, Batch 70000: Loss = 6.0714\n",
      "2025-05-06 02:35:13,732 - INFO - Epoch 1, Batch 70100: Loss = 7.7812\n",
      "2025-05-06 02:35:31,632 - INFO - Epoch 1, Batch 70200: Loss = 8.0551\n",
      "2025-05-06 02:35:49,531 - INFO - Epoch 1, Batch 70300: Loss = 6.3462\n",
      "2025-05-06 02:36:07,431 - INFO - Epoch 1, Batch 70400: Loss = 7.0872\n",
      "2025-05-06 02:36:25,332 - INFO - Epoch 1, Batch 70500: Loss = 7.5180\n",
      "2025-05-06 02:36:43,232 - INFO - Epoch 1, Batch 70600: Loss = 7.3699\n",
      "2025-05-06 02:37:01,131 - INFO - Epoch 1, Batch 70700: Loss = 6.9765\n",
      "2025-05-06 02:37:19,031 - INFO - Epoch 1, Batch 70800: Loss = 7.6974\n",
      "2025-05-06 02:37:36,931 - INFO - Epoch 1, Batch 70900: Loss = 7.2050\n",
      "2025-05-06 02:37:54,833 - INFO - Epoch 1, Batch 71000: Loss = 6.2898\n",
      "2025-05-06 02:38:12,734 - INFO - Epoch 1, Batch 71100: Loss = 7.7892\n",
      "2025-05-06 02:38:30,636 - INFO - Epoch 1, Batch 71200: Loss = 7.6092\n",
      "2025-05-06 02:38:48,536 - INFO - Epoch 1, Batch 71300: Loss = 3.3522\n",
      "2025-05-06 02:39:06,437 - INFO - Epoch 1, Batch 71400: Loss = 6.5170\n",
      "2025-05-06 02:39:24,338 - INFO - Epoch 1, Batch 71500: Loss = 7.9660\n",
      "2025-05-06 02:39:42,239 - INFO - Epoch 1, Batch 71600: Loss = 6.2954\n",
      "2025-05-06 02:40:00,139 - INFO - Epoch 1, Batch 71700: Loss = 2.8700\n",
      "2025-05-06 02:40:18,038 - INFO - Epoch 1, Batch 71800: Loss = 6.2845\n",
      "2025-05-06 02:40:35,937 - INFO - Epoch 1, Batch 71900: Loss = 7.1678\n",
      "2025-05-06 02:40:53,839 - INFO - Epoch 1, Batch 72000: Loss = 7.2626\n",
      "2025-05-06 02:41:11,739 - INFO - Epoch 1, Batch 72100: Loss = 6.2504\n",
      "2025-05-06 02:41:29,640 - INFO - Epoch 1, Batch 72200: Loss = 6.9817\n",
      "2025-05-06 02:41:47,542 - INFO - Epoch 1, Batch 72300: Loss = 6.9980\n",
      "2025-05-06 02:42:05,441 - INFO - Epoch 1, Batch 72400: Loss = 6.4754\n",
      "2025-05-06 02:42:23,341 - INFO - Epoch 1, Batch 72500: Loss = 5.8164\n",
      "2025-05-06 02:42:41,241 - INFO - Epoch 1, Batch 72600: Loss = 7.2435\n",
      "2025-05-06 02:42:59,141 - INFO - Epoch 1, Batch 72700: Loss = 6.7773\n",
      "2025-05-06 02:43:17,040 - INFO - Epoch 1, Batch 72800: Loss = 7.5060\n",
      "2025-05-06 02:43:34,939 - INFO - Epoch 1, Batch 72900: Loss = 9.0148\n",
      "2025-05-06 02:43:52,838 - INFO - Epoch 1, Batch 73000: Loss = 4.4610\n",
      "2025-05-06 02:44:10,740 - INFO - Epoch 1, Batch 73100: Loss = 7.1655\n",
      "2025-05-06 02:44:28,642 - INFO - Epoch 1, Batch 73200: Loss = 7.4055\n",
      "2025-05-06 02:44:46,542 - INFO - Epoch 1, Batch 73300: Loss = 7.6702\n",
      "2025-05-06 02:45:04,441 - INFO - Epoch 1, Batch 73400: Loss = 6.5466\n",
      "2025-05-06 02:45:22,340 - INFO - Epoch 1, Batch 73500: Loss = 6.7977\n",
      "2025-05-06 02:45:40,239 - INFO - Epoch 1, Batch 73600: Loss = 6.8477\n",
      "2025-05-06 02:45:58,137 - INFO - Epoch 1, Batch 73700: Loss = 7.4693\n",
      "2025-05-06 02:46:16,035 - INFO - Epoch 1, Batch 73800: Loss = 6.4318\n",
      "2025-05-06 02:46:33,932 - INFO - Epoch 1, Batch 73900: Loss = 7.0198\n",
      "2025-05-06 02:46:51,834 - INFO - Epoch 1, Batch 74000: Loss = 7.6374\n",
      "2025-05-06 02:47:09,733 - INFO - Epoch 1, Batch 74100: Loss = 7.1388\n",
      "2025-05-06 02:47:27,632 - INFO - Epoch 1, Batch 74200: Loss = 8.1236\n",
      "2025-05-06 02:47:45,533 - INFO - Epoch 1, Batch 74300: Loss = 6.7953\n",
      "2025-05-06 02:48:03,435 - INFO - Epoch 1, Batch 74400: Loss = 7.7645\n",
      "2025-05-06 02:48:21,334 - INFO - Epoch 1, Batch 74500: Loss = 6.0268\n",
      "2025-05-06 02:48:39,235 - INFO - Epoch 1, Batch 74600: Loss = 7.5645\n",
      "2025-05-06 02:48:57,133 - INFO - Epoch 1, Batch 74700: Loss = 6.7189\n",
      "2025-05-06 02:49:15,032 - INFO - Epoch 1, Batch 74800: Loss = 8.4331\n",
      "2025-05-06 02:49:32,932 - INFO - Epoch 1, Batch 74900: Loss = 7.2610\n",
      "2025-05-06 02:49:50,833 - INFO - Epoch 1, Batch 75000: Loss = 7.6698\n",
      "2025-05-06 02:50:08,733 - INFO - Epoch 1, Batch 75100: Loss = 5.8978\n",
      "2025-05-06 02:50:26,632 - INFO - Epoch 1, Batch 75200: Loss = 7.0487\n",
      "2025-05-06 02:50:44,531 - INFO - Epoch 1, Batch 75300: Loss = 7.8428\n",
      "2025-05-06 02:51:02,432 - INFO - Epoch 1, Batch 75400: Loss = 7.7352\n",
      "2025-05-06 02:51:20,331 - INFO - Epoch 1, Batch 75500: Loss = 6.7788\n",
      "2025-05-06 02:51:38,233 - INFO - Epoch 1, Batch 75600: Loss = 7.7187\n",
      "2025-05-06 02:51:56,131 - INFO - Epoch 1, Batch 75700: Loss = 8.0747\n",
      "2025-05-06 02:52:14,034 - INFO - Epoch 1, Batch 75800: Loss = 5.9288\n",
      "2025-05-06 02:52:31,932 - INFO - Epoch 1, Batch 75900: Loss = 8.4020\n",
      "2025-05-06 02:52:49,833 - INFO - Epoch 1, Batch 76000: Loss = 8.0315\n",
      "2025-05-06 02:53:07,732 - INFO - Epoch 1, Batch 76100: Loss = 6.7170\n",
      "2025-05-06 02:53:25,631 - INFO - Epoch 1, Batch 76200: Loss = 6.6229\n",
      "2025-05-06 02:53:43,532 - INFO - Epoch 1, Batch 76300: Loss = 7.0051\n",
      "2025-05-06 02:54:01,431 - INFO - Epoch 1, Batch 76400: Loss = 2.5586\n",
      "2025-05-06 02:54:19,332 - INFO - Epoch 1, Batch 76500: Loss = 6.4659\n",
      "2025-05-06 02:54:37,232 - INFO - Epoch 1, Batch 76600: Loss = 7.4327\n",
      "2025-05-06 02:54:55,133 - INFO - Epoch 1, Batch 76700: Loss = 7.2695\n",
      "2025-05-06 02:55:13,034 - INFO - Epoch 1, Batch 76800: Loss = 5.4291\n",
      "2025-05-06 02:55:30,932 - INFO - Epoch 1, Batch 76900: Loss = 7.5799\n",
      "2025-05-06 02:55:48,832 - INFO - Epoch 1, Batch 77000: Loss = 7.3920\n",
      "2025-05-06 02:56:06,733 - INFO - Epoch 1, Batch 77100: Loss = 6.7366\n",
      "2025-05-06 02:56:24,633 - INFO - Epoch 1, Batch 77200: Loss = 7.9667\n",
      "2025-05-06 02:56:42,531 - INFO - Epoch 1, Batch 77300: Loss = 6.7453\n",
      "2025-05-06 02:57:00,431 - INFO - Epoch 1, Batch 77400: Loss = 7.0872\n",
      "2025-05-06 02:57:18,331 - INFO - Epoch 1, Batch 77500: Loss = 6.6283\n",
      "2025-05-06 02:57:36,236 - INFO - Epoch 1, Batch 77600: Loss = 7.1208\n",
      "2025-05-06 02:57:54,138 - INFO - Epoch 1, Batch 77700: Loss = 7.2643\n",
      "2025-05-06 02:58:12,038 - INFO - Epoch 1, Batch 77800: Loss = 4.2836\n",
      "2025-05-06 02:58:29,938 - INFO - Epoch 1, Batch 77900: Loss = 7.4316\n",
      "2025-05-06 02:58:47,839 - INFO - Epoch 1, Batch 78000: Loss = 7.4658\n",
      "2025-05-06 02:59:05,739 - INFO - Epoch 1, Batch 78100: Loss = 8.0258\n",
      "2025-05-06 02:59:23,682 - INFO - Epoch 1, Batch 78200: Loss = 6.9361\n",
      "2025-05-06 02:59:42,104 - INFO - Epoch 1, Batch 78300: Loss = 7.8386\n",
      "2025-05-06 03:00:00,828 - INFO - Epoch 1, Batch 78400: Loss = 6.8276\n",
      "2025-05-06 03:00:18,790 - INFO - Epoch 1, Batch 78500: Loss = 7.0296\n",
      "2025-05-06 03:00:36,753 - INFO - Epoch 1, Batch 78600: Loss = 8.8210\n",
      "2025-05-06 03:00:54,718 - INFO - Epoch 1, Batch 78700: Loss = 6.6457\n",
      "2025-05-06 03:01:12,683 - INFO - Epoch 1, Batch 78800: Loss = 7.4060\n",
      "2025-05-06 03:01:30,645 - INFO - Epoch 1, Batch 78900: Loss = 7.1947\n",
      "2025-05-06 03:01:48,609 - INFO - Epoch 1, Batch 79000: Loss = 6.8268\n",
      "2025-05-06 03:02:06,571 - INFO - Epoch 1, Batch 79100: Loss = 6.7598\n",
      "2025-05-06 03:02:24,534 - INFO - Epoch 1, Batch 79200: Loss = 7.3155\n",
      "2025-05-06 03:02:42,497 - INFO - Epoch 1, Batch 79300: Loss = 7.1076\n",
      "2025-05-06 03:03:00,462 - INFO - Epoch 1, Batch 79400: Loss = 7.0311\n",
      "2025-05-06 03:03:18,423 - INFO - Epoch 1, Batch 79500: Loss = 8.2096\n",
      "2025-05-06 03:03:36,388 - INFO - Epoch 1, Batch 79600: Loss = 5.9480\n",
      "2025-05-06 03:03:54,350 - INFO - Epoch 1, Batch 79700: Loss = 6.5416\n",
      "2025-05-06 03:04:12,312 - INFO - Epoch 1, Batch 79800: Loss = 6.0846\n",
      "2025-05-06 03:04:30,273 - INFO - Epoch 1, Batch 79900: Loss = 6.9487\n",
      "2025-05-06 03:04:48,235 - INFO - Epoch 1, Batch 80000: Loss = 7.2041\n",
      "2025-05-06 03:05:06,198 - INFO - Epoch 1, Batch 80100: Loss = 6.9378\n",
      "2025-05-06 03:05:24,157 - INFO - Epoch 1, Batch 80200: Loss = 8.0041\n",
      "2025-05-06 03:05:42,119 - INFO - Epoch 1, Batch 80300: Loss = 6.9915\n",
      "2025-05-06 03:06:00,082 - INFO - Epoch 1, Batch 80400: Loss = 6.2621\n",
      "2025-05-06 03:06:18,040 - INFO - Epoch 1, Batch 80500: Loss = 7.1337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_dim = len(tokenized_vocab)\n",
    "embedding_layer_network = EmbeddingLayerFFNN(\n",
    "    tokenized_vocab, \n",
    "    tokenized_df_train, \n",
    "    embedding_dim, \n",
    "    1024, \n",
    "    1024, \n",
    "    0.5, \n",
    "    3, \n",
    "    tokenized_vocab_size, \n",
    "    3, \n",
    "    1e-4)\n",
    "loss_values = embedding_layer_network.fit(5, 64, device)\n",
    "plot_loss(loss_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Embedding Layer\n",
    "\n",
    "Using my own embedding layer is not doing great looking at the first few epochs compared to using precomputated embeddings from the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Aproach\n",
    "\n",
    "I will now test some sort of Hybrid Approach, that  means using Transformer initialized embedding and fine tunine them with an embedding layer in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class ContextWindowDataset(IterableDataset):\n",
    "    def __init__(self, df, vocab, context_size):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle documents each epoch\n",
    "        rows = self.df.sample(frac=1).itertuples(index=False)\n",
    "        N = self.context_size\n",
    "        for row in rows:\n",
    "            tokens = getattr(row, \"merged_tokenized\")\n",
    "            L = len(tokens)\n",
    "            if L <= N:\n",
    "                continue\n",
    "            # Shuffle window positions per document\n",
    "            positions = list(range(L - N))\n",
    "            random.shuffle(positions)\n",
    "            for i in positions:\n",
    "                context = tokens[i : i + N]\n",
    "                target = tokens[i + N]\n",
    "                # map to indices\n",
    "                ctx_idxs = [self.vocab.get(t, self.vocab.get(\"__UNK__\")) for t in context]\n",
    "                tgt_idx = self.vocab.get(target, self.vocab.get(\"__UNK__\"))\n",
    "                yield torch.LongTensor(ctx_idxs), tgt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import math\n",
    "class BenchmarkModel:\n",
    "    def __init__(self, test_df, model_to_test):\n",
    "        \"\"\"Initialize with test DataFrame and a trained model.\"\"\"\n",
    "        self.test_df = test_df\n",
    "        self.model = model_to_test.to(next(model_to_test.parameters()).device)\n",
    "        self.context_size = self.model.context_size\n",
    "        self.vocab = self.model.vocab\n",
    "        self.rev_vocab = {i: t for t, i in self.vocab.items()}\n",
    "\n",
    "    def predict_job_description(self, context_window, max_length=50, temperature=1.0):\n",
    "        \"\"\"Generate a sequence autoregressively from a starting context.\"\"\"\n",
    "        self.model.eval()\n",
    "        device = next(self.model.parameters()).device\n",
    "        tokens = context_window.copy()\n",
    "        for _ in range(max_length):\n",
    "            idxs = torch.LongTensor([\n",
    "                self.vocab.get(t, self.vocab.get(\"__UNK__\")) for t in tokens[-self.context_size:]\n",
    "            ]).unsqueeze(0).to(device)\n",
    "            logits = self.model(idxs).squeeze(0) / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_tok = self.rev_vocab.get(next_idx, \"<unk>\")\n",
    "            tokens.append(next_tok)\n",
    "        return tokens\n",
    "\n",
    "    def perplexity(self):\n",
    "        \"\"\"Compute perplexity on the test set.\"\"\"\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        dataset = ContextWindowDataset(self.test_df, self.vocab, self.context_size)\n",
    "        \n",
    "        # Modified collate function to handle the dataset output correctly\n",
    "        def collate_fn(batch):\n",
    "            contexts = torch.stack([x[0] for x in batch])  # Get context_idxs\n",
    "            targets = torch.LongTensor([x[1] for x in batch])  # Get target_idx\n",
    "            return contexts, targets\n",
    "        \n",
    "        loader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=32, \n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        total_loss, count = 0.0, 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in loader:\n",
    "                inputs = inputs.to(next(self.model.parameters()).device)\n",
    "                targets = targets.to(next(self.model.parameters()).device)\n",
    "                logits = self.model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "                \n",
    "        avg_loss = total_loss / count if count else float('nan')\n",
    "        return math.exp(avg_loss)\n",
    "\n",
    "    def bleu_score(self):\n",
    "        \"\"\"Compute corpus BLEU over next-token predictions.\"\"\"\n",
    "        dataset = ContextWindowDataset(self.test_df, self.vocab, self.context_size)\n",
    "        refs, hyps = [], []\n",
    "        for ctx_idxs, tgt_idx in dataset:  # Only unpack the two values that are actually returned\n",
    "            pred_idx = self.model(ctx_idxs.unsqueeze(0).to(next(self.model.parameters()).device)).argmax(dim=-1).item()\n",
    "            pred_token = self.rev_vocab.get(pred_idx, \"<unk>\")\n",
    "            target_token = self.rev_vocab.get(tgt_idx, \"<unk>\")\n",
    "            hyps.append([pred_token])\n",
    "            refs.append([[target_token]])\n",
    "        return corpus_bleu(refs, hyps)\n",
    "\n",
    "    def rouge_score(self):\n",
    "        \"\"\"Compute average ROUGE-1 and ROUGE-L F1 over single-token predictions.\"\"\"\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)\n",
    "        total1, totalL, count = 0.0, 0.0, 0\n",
    "        dataset = ContextWindowDataset(self.test_df, self.vocab, self.context_size)\n",
    "        for ctx_idxs, tgt_idx in dataset:  # Only unpack the two values that are actually returned\n",
    "            pred_idx = self.model(ctx_idxs.unsqueeze(0).to(next(self.model.parameters()).device)).argmax(dim=-1).item()\n",
    "            pred_token = self.rev_vocab.get(pred_idx, \"<unk>\")\n",
    "            target_token = self.rev_vocab.get(tgt_idx, \"<unk>\")\n",
    "            scores = scorer.score(target_token, pred_token)\n",
    "            total1 += scores['rouge1'].fmeasure\n",
    "            totalL += scores['rougeL'].fmeasure\n",
    "            count += 1\n",
    "        return {'rouge1': total1/count if count else 0.0,\n",
    "                'rougeL': totalL/count if count else 0.0}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "class HybridFFNN(nn.Module):\n",
    "    def __init__(self, vocab, hidden_size, dropout, num_layers, context_size, lr, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.rev_vocab = {i: t for t, i in vocab.items()}\n",
    "        self.lr = lr\n",
    "        self.N = context_size\n",
    "        self.context_size = context_size\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.embedding_dim = (\n",
    "            pretrained_embeddings.shape[1] if pretrained_embeddings is not None else 384\n",
    "        )\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        if pretrained_embeddings is not None:\n",
    "            w = torch.FloatTensor(pretrained_embeddings) if isinstance(pretrained_embeddings, np.ndarray) else pretrained_embeddings\n",
    "            self.embedding.weight.data.copy_(w)\n",
    "        layers = []\n",
    "        input_dim = self.embedding_dim * self.context_size\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.Linear(input_dim, hidden_size), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            input_dim = hidden_size\n",
    "        layers.append(nn.Linear(hidden_size, self.vocab_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        flat = embedded.view(embedded.size(0), -1)\n",
    "        return self.model(flat)\n",
    "\n",
    "    def metrics(self):\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        emb = self.embedding.weight.data\n",
    "        return {'total_params': total, 'trainable_params': trainable,\n",
    "                'emb_mean': emb.mean().item(), 'emb_std': emb.std().item()}\n",
    "\n",
    "    def fit(self, train_df, val_df=None, batch_size=10000, epochs=10, device='cuda',\n",
    "            optimizer_type='adamw', momentum=0.9, weight_decay=1e-2,\n",
    "            scheduler_type='onecycle'):\n",
    "        \"\"\"Train with LR schedules and momentum-based optimizers.\"\"\"\n",
    "        self.to(device)\n",
    "        N = self.context_size\n",
    "        # compute steps\n",
    "        total_windows = sum(max(len(row['merged_tokenized']) - N, 0) for _, row in train_df.iterrows())\n",
    "        steps_per_epoch = math.ceil(total_windows / batch_size)\n",
    "        # dataloader\n",
    "        dataset = ContextWindowDataset(train_df, self.vocab, N)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, collate_fn=lambda b: (\n",
    "            torch.stack([x[0] for x in b]), torch.LongTensor([x[1] for x in b])\n",
    "        ))\n",
    "        # optimizer\n",
    "        if optimizer_type=='sgd':\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=self.lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        elif optimizer_type=='adamw':\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        # scheduler\n",
    "        scheduler=None; step_every=None\n",
    "        if scheduler_type=='onecycle':\n",
    "            scheduler = OneCycleLR(optimizer, max_lr=self.lr, steps_per_epoch=steps_per_epoch, epochs=epochs, pct_start=0.3, anneal_strategy='cos')\n",
    "            step_every='batch'\n",
    "        elif scheduler_type=='cosine':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "            step_every='epoch'\n",
    "        elif scheduler_type=='plateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "            step_every='val'\n",
    "        loss_fn=nn.CrossEntropyLoss()\n",
    "        # storage for logging\n",
    "        loss_values = []\n",
    "        # train loop\n",
    "        for epoch in range(1,epochs+1):\n",
    "            self.train(); total_loss=0.0; count=0\n",
    "            logging.info(f\"Epoch {epoch}/{epochs} start ({optimizer_type}+{scheduler_type})\")\n",
    "            running_loss_100 = 0.0\n",
    "            for batch_idx, (inputs, targets) in enumerate(loader, start=1):\n",
    "                inputs,targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = self(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "                loss.backward(); optimizer.step()\n",
    "                if step_every=='batch': scheduler.step()\n",
    "                # update losses\n",
    "                total_loss += loss.item(); count += 1\n",
    "                running_loss_100 += loss.item()\n",
    "                # log every 100 batches\n",
    "                if batch_idx % 100 == 0:\n",
    "                    avg100 = running_loss_100 / 100\n",
    "                    loss_values.append(avg100)\n",
    "                    logging.info(f\"Epoch {epoch}, Batch {batch_idx}: Avg100 Loss = {avg100:.4f}\")\n",
    "                    running_loss_100 = 0.0\n",
    "            avg=total_loss/count if count else float('nan')\n",
    "            msg=f\"Epoch {epoch} train loss: {avg:.4f}\"\n",
    "            if val_df is not None:\n",
    "                vppl=BenchmarkModel(val_df,self).perplexity()\n",
    "                msg+=f\", val ppl: {vppl:.2f}\"\n",
    "                if scheduler_type=='plateau': scheduler.step(vppl)\n",
    "            logging.info(msg)\n",
    "            if step_every=='epoch' and scheduler is not None: scheduler.step()\n",
    "        return loss_values\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 08:49:07,334 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-05-07 08:49:07,335 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "def create_pretrained_embedding_matrix(vocab, transformer_model, embedding_dim=384):\n",
    "    \"\"\"Create pretrained embeddings matrix with proper dimensionality.\"\"\"\n",
    "    # Get the transformer's output dimension (usually 768 for most models)\n",
    "    sample_embedding = transformer_model.encode(\"test\", show_progress_bar=False)\n",
    "    transformer_dim = len(sample_embedding)\n",
    "    \n",
    "    # Initialize an embedding matrix with the transformer's dimension first\n",
    "    temp_matrix = np.zeros((len(vocab), transformer_dim))\n",
    "    \n",
    "    # Fill the matrix with transformer embeddings\n",
    "    for token, idx in vocab.items():\n",
    "        try:\n",
    "            # Encode the token using the transformer\n",
    "            embedding = transformer_model.encode(token, show_progress_bar=False)\n",
    "            temp_matrix[idx] = embedding\n",
    "        except:\n",
    "            # If encoding fails, leave as zeros\n",
    "            continue\n",
    "    \n",
    "    # If needed, project down to desired embedding dimension\n",
    "    if transformer_dim != embedding_dim:\n",
    "        # Create a simple linear projection using SVD\n",
    "        U, _, _ = np.linalg.svd(temp_matrix, full_matrices=False)\n",
    "        embedding_matrix = U[:, :embedding_dim]  # Take first embedding_dim components\n",
    "    else:\n",
    "        embedding_matrix = temp_matrix\n",
    "        \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Create pretrained embeddings\n",
    "embedding_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')  # or your original transformer\n",
    "pretrained_embeddings = create_pretrained_embedding_matrix(tokenized_vocab, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HybridFFNN(\n",
    "    vocab=tokenized_vocab,\n",
    "    hidden_size=512,\n",
    "    dropout=0.5,\n",
    "    num_layers=2,\n",
    "    context_size=3,\n",
    "    lr=1e-4,\n",
    "    pretrained_embeddings=pretrained_embeddings\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 08:52:26,849 - INFO - Epoch 1/10 start (adamw+onecycle)\n",
      "2025-05-07 08:53:02,976 - INFO - Epoch 1, Batch 100: Avg100 Loss = 10.8137\n",
      "2025-05-07 08:53:38,730 - INFO - Epoch 1, Batch 200: Avg100 Loss = 10.8057\n",
      "2025-05-07 08:54:14,608 - INFO - Epoch 1, Batch 300: Avg100 Loss = 10.7939\n",
      "2025-05-07 08:54:50,682 - INFO - Epoch 1, Batch 400: Avg100 Loss = 10.7711\n",
      "2025-05-07 08:55:26,389 - INFO - Epoch 1, Batch 500: Avg100 Loss = 10.7224\n",
      "2025-05-07 08:56:02,150 - INFO - Epoch 1, Batch 600: Avg100 Loss = 10.6203\n",
      "2025-05-07 08:56:38,030 - INFO - Epoch 1, Batch 700: Avg100 Loss = 10.4221\n",
      "2025-05-07 08:57:14,831 - INFO - Epoch 1, Batch 800: Avg100 Loss = 10.0714\n",
      "2025-05-07 08:57:50,746 - INFO - Epoch 1, Batch 900: Avg100 Loss = 9.5309\n",
      "2025-05-07 08:58:26,763 - INFO - Epoch 1, Batch 1000: Avg100 Loss = 8.8120\n",
      "2025-05-07 08:59:02,658 - INFO - Epoch 1, Batch 1100: Avg100 Loss = 8.1321\n",
      "2025-05-07 08:59:38,459 - INFO - Epoch 1, Batch 1200: Avg100 Loss = 7.7579\n",
      "2025-05-07 09:00:14,332 - INFO - Epoch 1, Batch 1300: Avg100 Loss = 7.6312\n",
      "2025-05-07 09:00:50,110 - INFO - Epoch 1, Batch 1400: Avg100 Loss = 7.5843\n",
      "2025-05-07 09:01:26,130 - INFO - Epoch 1, Batch 1500: Avg100 Loss = 7.5712\n",
      "2025-05-07 09:02:02,994 - INFO - Epoch 1, Batch 1600: Avg100 Loss = 7.5566\n",
      "2025-05-07 09:02:38,953 - INFO - Epoch 1, Batch 1700: Avg100 Loss = 7.5371\n",
      "2025-05-07 09:03:14,918 - INFO - Epoch 1, Batch 1800: Avg100 Loss = 7.5276\n",
      "2025-05-07 09:03:50,718 - INFO - Epoch 1, Batch 1900: Avg100 Loss = 7.5088\n",
      "2025-05-07 09:04:26,604 - INFO - Epoch 1, Batch 2000: Avg100 Loss = 7.5093\n",
      "2025-05-07 09:05:02,431 - INFO - Epoch 1, Batch 2100: Avg100 Loss = 7.4923\n",
      "2025-05-07 09:05:38,267 - INFO - Epoch 1, Batch 2200: Avg100 Loss = 7.4951\n",
      "2025-05-07 09:06:14,034 - INFO - Epoch 1, Batch 2300: Avg100 Loss = 7.4906\n",
      "2025-05-07 09:06:50,022 - INFO - Epoch 1, Batch 2400: Avg100 Loss = 7.4677\n",
      "2025-05-07 09:07:24,864 - INFO - Epoch 1, Batch 2500: Avg100 Loss = 7.4689\n",
      "2025-05-07 09:07:59,541 - INFO - Epoch 1, Batch 2600: Avg100 Loss = 7.4549\n",
      "2025-05-07 09:08:35,417 - INFO - Epoch 1, Batch 2700: Avg100 Loss = 7.4466\n",
      "2025-05-07 09:09:11,235 - INFO - Epoch 1, Batch 2800: Avg100 Loss = 7.4378\n",
      "2025-05-07 09:09:46,920 - INFO - Epoch 1, Batch 2900: Avg100 Loss = 7.4321\n",
      "2025-05-07 09:10:22,728 - INFO - Epoch 1, Batch 3000: Avg100 Loss = 7.4227\n",
      "2025-05-07 09:10:58,550 - INFO - Epoch 1, Batch 3100: Avg100 Loss = 7.4162\n",
      "2025-05-07 09:11:34,328 - INFO - Epoch 1, Batch 3200: Avg100 Loss = 7.4177\n",
      "2025-05-07 09:12:11,046 - INFO - Epoch 1, Batch 3300: Avg100 Loss = 7.4004\n",
      "2025-05-07 09:12:46,864 - INFO - Epoch 1, Batch 3400: Avg100 Loss = 7.3878\n",
      "2025-05-07 09:13:22,636 - INFO - Epoch 1, Batch 3500: Avg100 Loss = 7.3957\n",
      "2025-05-07 09:13:58,488 - INFO - Epoch 1, Batch 3600: Avg100 Loss = 7.3922\n",
      "2025-05-07 09:14:34,217 - INFO - Epoch 1, Batch 3700: Avg100 Loss = 7.3744\n",
      "2025-05-07 09:15:10,001 - INFO - Epoch 1, Batch 3800: Avg100 Loss = 7.3664\n",
      "2025-05-07 09:15:45,827 - INFO - Epoch 1, Batch 3900: Avg100 Loss = 7.3614\n",
      "2025-05-07 09:16:22,584 - INFO - Epoch 1, Batch 4000: Avg100 Loss = 7.3488\n",
      "2025-05-07 09:16:57,951 - INFO - Epoch 1, Batch 4100: Avg100 Loss = 7.3318\n",
      "2025-05-07 09:18:51,813 - INFO - Epoch 1 train loss: 8.1746, val ppl: 1372.01\n",
      "2025-05-07 09:18:51,813 - INFO - Epoch 2/10 start (adamw+onecycle)\n",
      "2025-05-07 09:19:26,470 - INFO - Epoch 2, Batch 100: Avg100 Loss = 7.3129\n",
      "2025-05-07 09:20:01,203 - INFO - Epoch 2, Batch 200: Avg100 Loss = 7.2882\n",
      "2025-05-07 09:20:35,896 - INFO - Epoch 2, Batch 300: Avg100 Loss = 7.2863\n",
      "2025-05-07 09:21:10,484 - INFO - Epoch 2, Batch 400: Avg100 Loss = 7.2638\n",
      "2025-05-07 09:21:45,957 - INFO - Epoch 2, Batch 500: Avg100 Loss = 7.2639\n",
      "2025-05-07 09:22:20,631 - INFO - Epoch 2, Batch 600: Avg100 Loss = 7.2594\n",
      "2025-05-07 09:22:55,379 - INFO - Epoch 2, Batch 700: Avg100 Loss = 7.2323\n",
      "2025-05-07 09:23:30,080 - INFO - Epoch 2, Batch 800: Avg100 Loss = 7.2299\n",
      "2025-05-07 09:24:04,686 - INFO - Epoch 2, Batch 900: Avg100 Loss = 7.2122\n",
      "2025-05-07 09:24:39,367 - INFO - Epoch 2, Batch 1000: Avg100 Loss = 7.2084\n",
      "2025-05-07 09:25:14,058 - INFO - Epoch 2, Batch 1100: Avg100 Loss = 7.1856\n",
      "2025-05-07 09:25:48,689 - INFO - Epoch 2, Batch 1200: Avg100 Loss = 7.1582\n",
      "2025-05-07 09:26:24,097 - INFO - Epoch 2, Batch 1300: Avg100 Loss = 7.1103\n",
      "2025-05-07 09:26:58,760 - INFO - Epoch 2, Batch 1400: Avg100 Loss = 7.0894\n",
      "2025-05-07 09:27:33,498 - INFO - Epoch 2, Batch 1500: Avg100 Loss = 7.0742\n",
      "2025-05-07 09:28:08,164 - INFO - Epoch 2, Batch 1600: Avg100 Loss = 7.0685\n",
      "2025-05-07 09:28:42,765 - INFO - Epoch 2, Batch 1700: Avg100 Loss = 7.0596\n",
      "2025-05-07 09:29:17,397 - INFO - Epoch 2, Batch 1800: Avg100 Loss = 7.0223\n",
      "2025-05-07 09:29:51,996 - INFO - Epoch 2, Batch 1900: Avg100 Loss = 6.9757\n",
      "2025-05-07 09:30:27,605 - INFO - Epoch 2, Batch 2000: Avg100 Loss = 6.9804\n",
      "2025-05-07 09:31:03,405 - INFO - Epoch 2, Batch 2100: Avg100 Loss = 6.9384\n",
      "2025-05-07 09:31:40,118 - INFO - Epoch 2, Batch 2200: Avg100 Loss = 6.9087\n",
      "2025-05-07 09:32:15,945 - INFO - Epoch 2, Batch 2300: Avg100 Loss = 6.8775\n",
      "2025-05-07 09:32:51,718 - INFO - Epoch 2, Batch 2400: Avg100 Loss = 6.8442\n",
      "2025-05-07 09:33:27,602 - INFO - Epoch 2, Batch 2500: Avg100 Loss = 6.8232\n",
      "2025-05-07 09:34:03,486 - INFO - Epoch 2, Batch 2600: Avg100 Loss = 6.8003\n",
      "2025-05-07 09:34:39,311 - INFO - Epoch 2, Batch 2700: Avg100 Loss = 6.7495\n",
      "2025-05-07 09:35:15,114 - INFO - Epoch 2, Batch 2800: Avg100 Loss = 6.7332\n",
      "2025-05-07 09:35:51,650 - INFO - Epoch 2, Batch 2900: Avg100 Loss = 6.7296\n",
      "2025-05-07 09:36:27,433 - INFO - Epoch 2, Batch 3000: Avg100 Loss = 6.6891\n",
      "2025-05-07 09:37:03,212 - INFO - Epoch 2, Batch 3100: Avg100 Loss = 6.6557\n",
      "2025-05-07 09:37:39,080 - INFO - Epoch 2, Batch 3200: Avg100 Loss = 6.6282\n",
      "2025-05-07 09:38:14,970 - INFO - Epoch 2, Batch 3300: Avg100 Loss = 6.6165\n",
      "2025-05-07 09:38:50,711 - INFO - Epoch 2, Batch 3400: Avg100 Loss = 6.5746\n",
      "2025-05-07 09:39:26,532 - INFO - Epoch 2, Batch 3500: Avg100 Loss = 6.5453\n",
      "2025-05-07 09:40:02,234 - INFO - Epoch 2, Batch 3600: Avg100 Loss = 6.5405\n",
      "2025-05-07 09:40:38,069 - INFO - Epoch 2, Batch 3700: Avg100 Loss = 6.4925\n",
      "2025-05-07 09:41:14,764 - INFO - Epoch 2, Batch 3800: Avg100 Loss = 6.4797\n",
      "2025-05-07 09:41:50,699 - INFO - Epoch 2, Batch 3900: Avg100 Loss = 6.4796\n",
      "2025-05-07 09:42:26,401 - INFO - Epoch 2, Batch 4000: Avg100 Loss = 6.4451\n",
      "2025-05-07 09:43:02,016 - INFO - Epoch 2, Batch 4100: Avg100 Loss = 6.4151\n",
      "2025-05-07 09:44:58,366 - INFO - Epoch 2 train loss: 6.9037, val ppl: 521.01\n",
      "2025-05-07 09:44:58,367 - INFO - Epoch 3/10 start (adamw+onecycle)\n",
      "2025-05-07 09:45:34,068 - INFO - Epoch 3, Batch 100: Avg100 Loss = 6.3875\n",
      "2025-05-07 09:46:10,625 - INFO - Epoch 3, Batch 200: Avg100 Loss = 6.3691\n",
      "2025-05-07 09:46:46,354 - INFO - Epoch 3, Batch 300: Avg100 Loss = 6.3544\n",
      "2025-05-07 09:47:22,153 - INFO - Epoch 3, Batch 400: Avg100 Loss = 6.3161\n",
      "2025-05-07 09:47:57,931 - INFO - Epoch 3, Batch 500: Avg100 Loss = 6.3099\n",
      "2025-05-07 09:48:33,776 - INFO - Epoch 3, Batch 600: Avg100 Loss = 6.2853\n",
      "2025-05-07 09:49:09,575 - INFO - Epoch 3, Batch 700: Avg100 Loss = 6.2622\n",
      "2025-05-07 09:49:46,218 - INFO - Epoch 3, Batch 800: Avg100 Loss = 6.2396\n",
      "2025-05-07 09:50:22,031 - INFO - Epoch 3, Batch 900: Avg100 Loss = 6.2265\n",
      "2025-05-07 09:50:57,712 - INFO - Epoch 3, Batch 1000: Avg100 Loss = 6.2183\n",
      "2025-05-07 09:51:33,428 - INFO - Epoch 3, Batch 1100: Avg100 Loss = 6.2025\n",
      "2025-05-07 09:52:09,090 - INFO - Epoch 3, Batch 1200: Avg100 Loss = 6.1909\n",
      "2025-05-07 09:52:45,613 - INFO - Epoch 3, Batch 1300: Avg100 Loss = 6.1555\n",
      "2025-05-07 09:53:21,222 - INFO - Epoch 3, Batch 1400: Avg100 Loss = 6.1498\n",
      "2025-05-07 09:53:56,869 - INFO - Epoch 3, Batch 1500: Avg100 Loss = 6.1428\n",
      "2025-05-07 09:54:32,635 - INFO - Epoch 3, Batch 1600: Avg100 Loss = 6.1015\n",
      "2025-05-07 09:55:07,903 - INFO - Epoch 3, Batch 1700: Avg100 Loss = 6.1070\n",
      "2025-05-07 09:55:42,536 - INFO - Epoch 3, Batch 1800: Avg100 Loss = 6.0848\n",
      "2025-05-07 09:56:18,116 - INFO - Epoch 3, Batch 1900: Avg100 Loss = 6.0894\n",
      "2025-05-07 09:56:52,852 - INFO - Epoch 3, Batch 2000: Avg100 Loss = 6.0635\n",
      "2025-05-07 09:57:27,508 - INFO - Epoch 3, Batch 2100: Avg100 Loss = 6.0466\n",
      "2025-05-07 09:58:02,155 - INFO - Epoch 3, Batch 2200: Avg100 Loss = 6.0332\n",
      "2025-05-07 09:58:36,830 - INFO - Epoch 3, Batch 2300: Avg100 Loss = 6.0320\n",
      "2025-05-07 09:59:12,327 - INFO - Epoch 3, Batch 2400: Avg100 Loss = 6.0175\n",
      "2025-05-07 09:59:47,033 - INFO - Epoch 3, Batch 2500: Avg100 Loss = 5.9867\n",
      "2025-05-07 10:00:21,691 - INFO - Epoch 3, Batch 2600: Avg100 Loss = 5.9807\n",
      "2025-05-07 10:00:56,346 - INFO - Epoch 3, Batch 2700: Avg100 Loss = 5.9946\n",
      "2025-05-07 10:01:30,935 - INFO - Epoch 3, Batch 2800: Avg100 Loss = 5.9514\n",
      "2025-05-07 10:02:05,615 - INFO - Epoch 3, Batch 2900: Avg100 Loss = 5.9356\n",
      "2025-05-07 10:02:41,082 - INFO - Epoch 3, Batch 3000: Avg100 Loss = 5.9382\n",
      "2025-05-07 10:03:15,689 - INFO - Epoch 3, Batch 3100: Avg100 Loss = 5.9434\n",
      "2025-05-07 10:03:50,348 - INFO - Epoch 3, Batch 3200: Avg100 Loss = 5.9283\n",
      "2025-05-07 10:04:24,977 - INFO - Epoch 3, Batch 3300: Avg100 Loss = 5.9158\n",
      "2025-05-07 10:04:59,666 - INFO - Epoch 3, Batch 3400: Avg100 Loss = 5.8893\n",
      "2025-05-07 10:05:35,311 - INFO - Epoch 3, Batch 3500: Avg100 Loss = 5.8876\n",
      "2025-05-07 10:06:10,026 - INFO - Epoch 3, Batch 3600: Avg100 Loss = 5.8653\n",
      "2025-05-07 10:06:44,757 - INFO - Epoch 3, Batch 3700: Avg100 Loss = 5.8625\n",
      "2025-05-07 10:07:19,397 - INFO - Epoch 3, Batch 3800: Avg100 Loss = 5.8875\n",
      "2025-05-07 10:07:54,082 - INFO - Epoch 3, Batch 3900: Avg100 Loss = 5.8355\n",
      "2025-05-07 10:08:28,789 - INFO - Epoch 3, Batch 4000: Avg100 Loss = 5.8610\n",
      "2025-05-07 10:09:04,601 - INFO - Epoch 3, Batch 4100: Avg100 Loss = 5.7938\n",
      "2025-05-07 10:11:01,026 - INFO - Epoch 3 train loss: 6.0670, val ppl: 275.66\n",
      "2025-05-07 10:11:01,026 - INFO - Epoch 4/10 start (adamw+onecycle)\n",
      "2025-05-07 10:11:36,900 - INFO - Epoch 4, Batch 100: Avg100 Loss = 5.8026\n",
      "2025-05-07 10:12:12,650 - INFO - Epoch 4, Batch 200: Avg100 Loss = 5.7976\n",
      "2025-05-07 10:12:48,426 - INFO - Epoch 4, Batch 300: Avg100 Loss = 5.7862\n",
      "2025-05-07 10:13:24,190 - INFO - Epoch 4, Batch 400: Avg100 Loss = 5.7990\n",
      "2025-05-07 10:13:59,881 - INFO - Epoch 4, Batch 500: Avg100 Loss = 5.7830\n",
      "2025-05-07 10:14:36,430 - INFO - Epoch 4, Batch 600: Avg100 Loss = 5.7853\n",
      "2025-05-07 10:15:12,208 - INFO - Epoch 4, Batch 700: Avg100 Loss = 5.7621\n",
      "2025-05-07 10:15:48,020 - INFO - Epoch 4, Batch 800: Avg100 Loss = 5.7586\n",
      "2025-05-07 10:16:23,888 - INFO - Epoch 4, Batch 900: Avg100 Loss = 5.7522\n",
      "2025-05-07 10:16:59,671 - INFO - Epoch 4, Batch 1000: Avg100 Loss = 5.7410\n",
      "2025-05-07 10:17:35,321 - INFO - Epoch 4, Batch 1100: Avg100 Loss = 5.7076\n",
      "2025-05-07 10:18:11,028 - INFO - Epoch 4, Batch 1200: Avg100 Loss = 5.7151\n",
      "2025-05-07 10:18:47,318 - INFO - Epoch 4, Batch 1300: Avg100 Loss = 5.7222\n",
      "2025-05-07 10:19:21,978 - INFO - Epoch 4, Batch 1400: Avg100 Loss = 5.7061\n",
      "2025-05-07 10:19:56,619 - INFO - Epoch 4, Batch 1500: Avg100 Loss = 5.6942\n",
      "2025-05-07 10:20:31,317 - INFO - Epoch 4, Batch 1600: Avg100 Loss = 5.7033\n",
      "2025-05-07 10:21:05,942 - INFO - Epoch 4, Batch 1700: Avg100 Loss = 5.7086\n",
      "2025-05-07 10:21:40,516 - INFO - Epoch 4, Batch 1800: Avg100 Loss = 5.6887\n",
      "2025-05-07 10:22:15,105 - INFO - Epoch 4, Batch 1900: Avg100 Loss = 5.6805\n",
      "2025-05-07 10:22:49,850 - INFO - Epoch 4, Batch 2000: Avg100 Loss = 5.6514\n",
      "2025-05-07 10:23:24,547 - INFO - Epoch 4, Batch 2100: Avg100 Loss = 5.6437\n",
      "2025-05-07 10:24:00,009 - INFO - Epoch 4, Batch 2200: Avg100 Loss = 5.6661\n",
      "2025-05-07 10:24:34,648 - INFO - Epoch 4, Batch 2300: Avg100 Loss = 5.6535\n",
      "2025-05-07 10:25:09,245 - INFO - Epoch 4, Batch 2400: Avg100 Loss = 5.6484\n",
      "2025-05-07 10:25:43,869 - INFO - Epoch 4, Batch 2500: Avg100 Loss = 5.6337\n",
      "2025-05-07 10:26:18,590 - INFO - Epoch 4, Batch 2600: Avg100 Loss = 5.6319\n",
      "2025-05-07 10:26:53,232 - INFO - Epoch 4, Batch 2700: Avg100 Loss = 5.6464\n",
      "2025-05-07 10:27:27,816 - INFO - Epoch 4, Batch 2800: Avg100 Loss = 5.6182\n",
      "2025-05-07 10:28:02,398 - INFO - Epoch 4, Batch 2900: Avg100 Loss = 5.5903\n",
      "2025-05-07 10:28:38,652 - INFO - Epoch 4, Batch 3000: Avg100 Loss = 5.6172\n",
      "2025-05-07 10:29:14,515 - INFO - Epoch 4, Batch 3100: Avg100 Loss = 5.6240\n",
      "2025-05-07 10:29:50,468 - INFO - Epoch 4, Batch 3200: Avg100 Loss = 5.6186\n",
      "2025-05-07 10:30:26,242 - INFO - Epoch 4, Batch 3300: Avg100 Loss = 5.6118\n",
      "2025-05-07 10:31:02,237 - INFO - Epoch 4, Batch 3400: Avg100 Loss = 5.6107\n",
      "2025-05-07 10:31:38,111 - INFO - Epoch 4, Batch 3500: Avg100 Loss = 5.6044\n",
      "2025-05-07 10:32:13,792 - INFO - Epoch 4, Batch 3600: Avg100 Loss = 5.6071\n",
      "2025-05-07 10:32:49,399 - INFO - Epoch 4, Batch 3700: Avg100 Loss = 5.5539\n",
      "2025-05-07 10:33:25,913 - INFO - Epoch 4, Batch 3800: Avg100 Loss = 5.5844\n",
      "2025-05-07 10:34:01,651 - INFO - Epoch 4, Batch 3900: Avg100 Loss = 5.5678\n",
      "2025-05-07 10:34:37,311 - INFO - Epoch 4, Batch 4000: Avg100 Loss = 5.5739\n",
      "2025-05-07 10:35:13,049 - INFO - Epoch 4, Batch 4100: Avg100 Loss = 5.5172\n",
      "2025-05-07 10:37:09,336 - INFO - Epoch 4 train loss: 5.6717, val ppl: 203.70\n",
      "2025-05-07 10:37:09,337 - INFO - Epoch 5/10 start (adamw+onecycle)\n",
      "2025-05-07 10:37:45,086 - INFO - Epoch 5, Batch 100: Avg100 Loss = 5.5407\n",
      "2025-05-07 10:38:20,915 - INFO - Epoch 5, Batch 200: Avg100 Loss = 5.5603\n",
      "2025-05-07 10:38:57,449 - INFO - Epoch 5, Batch 300: Avg100 Loss = 5.5255\n",
      "2025-05-07 10:39:33,266 - INFO - Epoch 5, Batch 400: Avg100 Loss = 5.5297\n",
      "2025-05-07 10:40:09,018 - INFO - Epoch 5, Batch 500: Avg100 Loss = 5.5573\n",
      "2025-05-07 10:40:44,717 - INFO - Epoch 5, Batch 600: Avg100 Loss = 5.5183\n",
      "2025-05-07 10:41:20,274 - INFO - Epoch 5, Batch 700: Avg100 Loss = 5.5054\n",
      "2025-05-07 10:41:55,870 - INFO - Epoch 5, Batch 800: Avg100 Loss = 5.5170\n",
      "2025-05-07 10:42:31,624 - INFO - Epoch 5, Batch 900: Avg100 Loss = 5.5479\n",
      "2025-05-07 10:43:07,254 - INFO - Epoch 5, Batch 1000: Avg100 Loss = 5.5096\n",
      "2025-05-07 10:43:42,808 - INFO - Epoch 5, Batch 1100: Avg100 Loss = 5.4988\n",
      "2025-05-07 10:44:19,195 - INFO - Epoch 5, Batch 1200: Avg100 Loss = 5.4946\n",
      "2025-05-07 10:44:54,276 - INFO - Epoch 5, Batch 1300: Avg100 Loss = 5.5019\n",
      "2025-05-07 10:45:28,882 - INFO - Epoch 5, Batch 1400: Avg100 Loss = 5.4930\n",
      "2025-05-07 10:46:03,491 - INFO - Epoch 5, Batch 1500: Avg100 Loss = 5.4724\n",
      "2025-05-07 10:46:38,063 - INFO - Epoch 5, Batch 1600: Avg100 Loss = 5.4810\n",
      "2025-05-07 10:47:12,730 - INFO - Epoch 5, Batch 1700: Avg100 Loss = 5.4878\n",
      "2025-05-07 10:47:47,334 - INFO - Epoch 5, Batch 1800: Avg100 Loss = 5.4798\n",
      "2025-05-07 10:48:22,849 - INFO - Epoch 5, Batch 1900: Avg100 Loss = 5.4846\n",
      "2025-05-07 10:48:57,525 - INFO - Epoch 5, Batch 2000: Avg100 Loss = 5.4305\n",
      "2025-05-07 10:49:32,173 - INFO - Epoch 5, Batch 2100: Avg100 Loss = 5.4953\n",
      "2025-05-07 10:50:06,776 - INFO - Epoch 5, Batch 2200: Avg100 Loss = 5.4520\n",
      "2025-05-07 10:50:41,455 - INFO - Epoch 5, Batch 2300: Avg100 Loss = 5.4787\n",
      "2025-05-07 10:51:16,170 - INFO - Epoch 5, Batch 2400: Avg100 Loss = 5.4444\n",
      "2025-05-07 10:51:50,741 - INFO - Epoch 5, Batch 2500: Avg100 Loss = 5.4333\n",
      "2025-05-07 10:52:25,374 - INFO - Epoch 5, Batch 2600: Avg100 Loss = 5.4522\n",
      "2025-05-07 10:53:00,069 - INFO - Epoch 5, Batch 2700: Avg100 Loss = 5.4557\n",
      "2025-05-07 10:53:35,561 - INFO - Epoch 5, Batch 2800: Avg100 Loss = 5.4877\n",
      "2025-05-07 10:54:10,174 - INFO - Epoch 5, Batch 2900: Avg100 Loss = 5.4143\n",
      "2025-05-07 10:54:44,785 - INFO - Epoch 5, Batch 3000: Avg100 Loss = 5.4573\n",
      "2025-05-07 10:55:20,427 - INFO - Epoch 5, Batch 3100: Avg100 Loss = 5.4354\n",
      "2025-05-07 10:55:56,199 - INFO - Epoch 5, Batch 3200: Avg100 Loss = 5.4635\n",
      "2025-05-07 10:56:32,025 - INFO - Epoch 5, Batch 3300: Avg100 Loss = 5.4392\n",
      "2025-05-07 10:57:07,890 - INFO - Epoch 5, Batch 3400: Avg100 Loss = 5.4425\n",
      "2025-05-07 10:57:43,609 - INFO - Epoch 5, Batch 3500: Avg100 Loss = 5.4552\n",
      "2025-05-07 10:58:20,247 - INFO - Epoch 5, Batch 3600: Avg100 Loss = 5.4420\n",
      "2025-05-07 10:58:56,004 - INFO - Epoch 5, Batch 3700: Avg100 Loss = 5.4204\n",
      "2025-05-07 10:59:31,806 - INFO - Epoch 5, Batch 3800: Avg100 Loss = 5.4393\n",
      "2025-05-07 11:00:07,276 - INFO - Epoch 5, Batch 3900: Avg100 Loss = 5.4366\n",
      "2025-05-07 11:00:42,647 - INFO - Epoch 5, Batch 4000: Avg100 Loss = 5.4170\n",
      "2025-05-07 11:01:18,062 - INFO - Epoch 5, Batch 4100: Avg100 Loss = 5.4574\n",
      "2025-05-07 11:03:13,680 - INFO - Epoch 5 train loss: 5.4764, val ppl: 173.03\n",
      "2025-05-07 11:03:13,680 - INFO - Epoch 6/10 start (adamw+onecycle)\n",
      "2025-05-07 11:03:49,571 - INFO - Epoch 6, Batch 100: Avg100 Loss = 5.4178\n",
      "2025-05-07 11:04:24,530 - INFO - Epoch 6, Batch 200: Avg100 Loss = 5.3945\n",
      "2025-05-07 11:05:00,146 - INFO - Epoch 6, Batch 300: Avg100 Loss = 5.3896\n",
      "2025-05-07 11:05:35,968 - INFO - Epoch 6, Batch 400: Avg100 Loss = 5.3778\n",
      "2025-05-07 11:06:11,862 - INFO - Epoch 6, Batch 500: Avg100 Loss = 5.4039\n",
      "2025-05-07 11:06:48,418 - INFO - Epoch 6, Batch 600: Avg100 Loss = 5.4194\n",
      "2025-05-07 11:07:24,060 - INFO - Epoch 6, Batch 700: Avg100 Loss = 5.3959\n",
      "2025-05-07 11:07:59,734 - INFO - Epoch 6, Batch 800: Avg100 Loss = 5.4258\n",
      "2025-05-07 11:08:35,580 - INFO - Epoch 6, Batch 900: Avg100 Loss = 5.4102\n",
      "2025-05-07 11:09:11,409 - INFO - Epoch 6, Batch 1000: Avg100 Loss = 5.3651\n",
      "2025-05-07 11:09:47,056 - INFO - Epoch 6, Batch 1100: Avg100 Loss = 5.3619\n",
      "2025-05-07 11:10:23,228 - INFO - Epoch 6, Batch 1200: Avg100 Loss = 5.3790\n",
      "2025-05-07 11:10:57,802 - INFO - Epoch 6, Batch 1300: Avg100 Loss = 5.3598\n",
      "2025-05-07 11:11:32,506 - INFO - Epoch 6, Batch 1400: Avg100 Loss = 5.3550\n",
      "2025-05-07 11:12:07,196 - INFO - Epoch 6, Batch 1500: Avg100 Loss = 5.3843\n",
      "2025-05-07 11:12:41,885 - INFO - Epoch 6, Batch 1600: Avg100 Loss = 5.3665\n",
      "2025-05-07 11:13:17,357 - INFO - Epoch 6, Batch 1700: Avg100 Loss = 5.3809\n",
      "2025-05-07 11:13:52,060 - INFO - Epoch 6, Batch 1800: Avg100 Loss = 5.4219\n",
      "2025-05-07 11:14:26,685 - INFO - Epoch 6, Batch 1900: Avg100 Loss = 5.3659\n",
      "2025-05-07 11:15:01,341 - INFO - Epoch 6, Batch 2000: Avg100 Loss = 5.3550\n",
      "2025-05-07 11:15:35,973 - INFO - Epoch 6, Batch 2100: Avg100 Loss = 5.3547\n",
      "2025-05-07 11:16:10,563 - INFO - Epoch 6, Batch 2200: Avg100 Loss = 5.3766\n",
      "2025-05-07 11:16:46,116 - INFO - Epoch 6, Batch 2300: Avg100 Loss = 5.3739\n",
      "2025-05-07 11:17:20,701 - INFO - Epoch 6, Batch 2400: Avg100 Loss = 5.3740\n",
      "2025-05-07 11:17:55,358 - INFO - Epoch 6, Batch 2500: Avg100 Loss = 5.3345\n",
      "2025-05-07 11:18:29,943 - INFO - Epoch 6, Batch 2600: Avg100 Loss = 5.3419\n",
      "2025-05-07 11:19:04,557 - INFO - Epoch 6, Batch 2700: Avg100 Loss = 5.3680\n",
      "2025-05-07 11:19:39,956 - INFO - Epoch 6, Batch 2800: Avg100 Loss = 5.3591\n",
      "2025-05-07 11:20:14,607 - INFO - Epoch 6, Batch 2900: Avg100 Loss = 5.3462\n",
      "2025-05-07 11:20:49,203 - INFO - Epoch 6, Batch 3000: Avg100 Loss = 5.3319\n",
      "2025-05-07 11:21:23,840 - INFO - Epoch 6, Batch 3100: Avg100 Loss = 5.3493\n",
      "2025-05-07 11:21:58,347 - INFO - Epoch 6, Batch 3200: Avg100 Loss = 5.3098\n",
      "2025-05-07 11:22:32,930 - INFO - Epoch 6, Batch 3300: Avg100 Loss = 5.3360\n",
      "2025-05-07 11:23:08,465 - INFO - Epoch 6, Batch 3400: Avg100 Loss = 5.3220\n",
      "2025-05-07 11:23:43,087 - INFO - Epoch 6, Batch 3500: Avg100 Loss = 5.3693\n",
      "2025-05-07 11:24:17,649 - INFO - Epoch 6, Batch 3600: Avg100 Loss = 5.3417\n",
      "2025-05-07 11:24:52,282 - INFO - Epoch 6, Batch 3700: Avg100 Loss = 5.3641\n",
      "2025-05-07 11:25:26,916 - INFO - Epoch 6, Batch 3800: Avg100 Loss = 5.3386\n",
      "2025-05-07 11:26:01,438 - INFO - Epoch 6, Batch 3900: Avg100 Loss = 5.3035\n",
      "2025-05-07 11:26:36,935 - INFO - Epoch 6, Batch 4000: Avg100 Loss = 5.3228\n",
      "2025-05-07 11:27:11,518 - INFO - Epoch 6, Batch 4100: Avg100 Loss = 5.3272\n",
      "2025-05-07 11:29:05,062 - INFO - Epoch 6 train loss: 5.3648, val ppl: 156.76\n",
      "2025-05-07 11:29:05,062 - INFO - Epoch 7/10 start (adamw+onecycle)\n",
      "2025-05-07 11:29:39,715 - INFO - Epoch 7, Batch 100: Avg100 Loss = 5.3486\n",
      "2025-05-07 11:30:15,379 - INFO - Epoch 7, Batch 200: Avg100 Loss = 5.3096\n",
      "2025-05-07 11:30:51,187 - INFO - Epoch 7, Batch 300: Avg100 Loss = 5.3148\n",
      "2025-05-07 11:31:26,862 - INFO - Epoch 7, Batch 400: Avg100 Loss = 5.2962\n",
      "2025-05-07 11:32:02,432 - INFO - Epoch 7, Batch 500: Avg100 Loss = 5.3328\n",
      "2025-05-07 11:32:38,209 - INFO - Epoch 7, Batch 600: Avg100 Loss = 5.3093\n",
      "2025-05-07 11:33:14,067 - INFO - Epoch 7, Batch 700: Avg100 Loss = 5.3164\n",
      "2025-05-07 11:33:50,861 - INFO - Epoch 7, Batch 800: Avg100 Loss = 5.3214\n",
      "2025-05-07 11:34:26,583 - INFO - Epoch 7, Batch 900: Avg100 Loss = 5.3338\n",
      "2025-05-07 11:35:02,344 - INFO - Epoch 7, Batch 1000: Avg100 Loss = 5.2910\n",
      "2025-05-07 11:35:38,092 - INFO - Epoch 7, Batch 1100: Avg100 Loss = 5.3080\n",
      "2025-05-07 11:36:13,861 - INFO - Epoch 7, Batch 1200: Avg100 Loss = 5.3251\n",
      "2025-05-07 11:36:49,699 - INFO - Epoch 7, Batch 1300: Avg100 Loss = 5.3247\n",
      "2025-05-07 11:37:26,316 - INFO - Epoch 7, Batch 1400: Avg100 Loss = 5.3162\n",
      "2025-05-07 11:38:01,924 - INFO - Epoch 7, Batch 1500: Avg100 Loss = 5.2719\n",
      "2025-05-07 11:38:37,782 - INFO - Epoch 7, Batch 1600: Avg100 Loss = 5.2802\n",
      "2025-05-07 11:39:13,429 - INFO - Epoch 7, Batch 1700: Avg100 Loss = 5.2752\n",
      "2025-05-07 11:39:48,753 - INFO - Epoch 7, Batch 1800: Avg100 Loss = 5.2867\n",
      "2025-05-07 11:40:25,146 - INFO - Epoch 7, Batch 1900: Avg100 Loss = 5.2771\n",
      "2025-05-07 11:41:00,865 - INFO - Epoch 7, Batch 2000: Avg100 Loss = 5.2871\n",
      "2025-05-07 11:41:36,664 - INFO - Epoch 7, Batch 2100: Avg100 Loss = 5.2897\n",
      "2025-05-07 11:42:12,632 - INFO - Epoch 7, Batch 2200: Avg100 Loss = 5.2916\n",
      "2025-05-07 11:42:48,642 - INFO - Epoch 7, Batch 2300: Avg100 Loss = 5.2989\n",
      "2025-05-07 11:43:25,573 - INFO - Epoch 7, Batch 2400: Avg100 Loss = 5.3104\n",
      "2025-05-07 11:44:01,949 - INFO - Epoch 7, Batch 2500: Avg100 Loss = 5.2889\n",
      "2025-05-07 11:44:38,006 - INFO - Epoch 7, Batch 2600: Avg100 Loss = 5.2942\n",
      "2025-05-07 11:45:14,209 - INFO - Epoch 7, Batch 2700: Avg100 Loss = 5.2639\n",
      "2025-05-07 11:45:50,136 - INFO - Epoch 7, Batch 2800: Avg100 Loss = 5.2785\n",
      "2025-05-07 11:46:26,169 - INFO - Epoch 7, Batch 2900: Avg100 Loss = 5.3026\n",
      "2025-05-07 11:47:03,037 - INFO - Epoch 7, Batch 3000: Avg100 Loss = 5.2832\n",
      "2025-05-07 11:47:39,057 - INFO - Epoch 7, Batch 3100: Avg100 Loss = 5.2756\n",
      "2025-05-07 11:48:15,073 - INFO - Epoch 7, Batch 3200: Avg100 Loss = 5.2955\n",
      "2025-05-07 11:48:51,199 - INFO - Epoch 7, Batch 3300: Avg100 Loss = 5.2963\n",
      "2025-05-07 11:49:27,161 - INFO - Epoch 7, Batch 3400: Avg100 Loss = 5.3052\n",
      "2025-05-07 11:50:03,171 - INFO - Epoch 7, Batch 3500: Avg100 Loss = 5.2684\n",
      "2025-05-07 11:50:39,961 - INFO - Epoch 7, Batch 3600: Avg100 Loss = 5.2988\n",
      "2025-05-07 11:51:15,841 - INFO - Epoch 7, Batch 3700: Avg100 Loss = 5.3069\n",
      "2025-05-07 11:51:51,665 - INFO - Epoch 7, Batch 3800: Avg100 Loss = 5.2757\n",
      "2025-05-07 11:52:27,617 - INFO - Epoch 7, Batch 3900: Avg100 Loss = 5.2961\n",
      "2025-05-07 11:53:03,521 - INFO - Epoch 7, Batch 4000: Avg100 Loss = 5.3133\n",
      "2025-05-07 11:53:40,047 - INFO - Epoch 7, Batch 4100: Avg100 Loss = 5.2609\n",
      "2025-05-07 11:55:37,187 - INFO - Epoch 7 train loss: 5.2977, val ppl: 148.08\n",
      "2025-05-07 11:55:37,188 - INFO - Epoch 8/10 start (adamw+onecycle)\n",
      "2025-05-07 11:56:12,959 - INFO - Epoch 8, Batch 100: Avg100 Loss = 5.2678\n",
      "2025-05-07 11:56:48,432 - INFO - Epoch 8, Batch 200: Avg100 Loss = 5.2751\n",
      "2025-05-07 11:57:24,003 - INFO - Epoch 8, Batch 300: Avg100 Loss = 5.2388\n",
      "2025-05-07 11:58:00,482 - INFO - Epoch 8, Batch 400: Avg100 Loss = 5.2492\n",
      "2025-05-07 11:58:35,968 - INFO - Epoch 8, Batch 500: Avg100 Loss = 5.2950\n",
      "2025-05-07 11:59:11,006 - INFO - Epoch 8, Batch 600: Avg100 Loss = 5.2652\n",
      "2025-05-07 11:59:46,246 - INFO - Epoch 8, Batch 700: Avg100 Loss = 5.2638\n",
      "2025-05-07 12:00:21,966 - INFO - Epoch 8, Batch 800: Avg100 Loss = 5.2627\n",
      "2025-05-07 12:00:57,608 - INFO - Epoch 8, Batch 900: Avg100 Loss = 5.2757\n",
      "2025-05-07 12:01:34,028 - INFO - Epoch 8, Batch 1000: Avg100 Loss = 5.2582\n",
      "2025-05-07 12:02:09,791 - INFO - Epoch 8, Batch 1100: Avg100 Loss = 5.2698\n",
      "2025-05-07 12:02:45,463 - INFO - Epoch 8, Batch 1200: Avg100 Loss = 5.2673\n",
      "2025-05-07 12:03:21,107 - INFO - Epoch 8, Batch 1300: Avg100 Loss = 5.2649\n",
      "2025-05-07 12:03:56,760 - INFO - Epoch 8, Batch 1400: Avg100 Loss = 5.2678\n",
      "2025-05-07 12:04:33,274 - INFO - Epoch 8, Batch 1500: Avg100 Loss = 5.2800\n",
      "2025-05-07 12:05:08,739 - INFO - Epoch 8, Batch 1600: Avg100 Loss = 5.2596\n",
      "2025-05-07 12:05:44,276 - INFO - Epoch 8, Batch 1700: Avg100 Loss = 5.2718\n",
      "2025-05-07 12:06:19,135 - INFO - Epoch 8, Batch 1800: Avg100 Loss = 5.2636\n",
      "2025-05-07 12:06:53,691 - INFO - Epoch 8, Batch 1900: Avg100 Loss = 5.2784\n",
      "2025-05-07 12:07:28,422 - INFO - Epoch 8, Batch 2000: Avg100 Loss = 5.2667\n",
      "2025-05-07 12:08:03,922 - INFO - Epoch 8, Batch 2100: Avg100 Loss = 5.2365\n",
      "2025-05-07 12:08:39,591 - INFO - Epoch 8, Batch 2200: Avg100 Loss = 5.2706\n",
      "2025-05-07 12:09:15,298 - INFO - Epoch 8, Batch 2300: Avg100 Loss = 5.2751\n",
      "2025-05-07 12:09:50,739 - INFO - Epoch 8, Batch 2400: Avg100 Loss = 5.2532\n",
      "2025-05-07 12:10:26,466 - INFO - Epoch 8, Batch 2500: Avg100 Loss = 5.2624\n",
      "2025-05-07 12:11:01,959 - INFO - Epoch 8, Batch 2600: Avg100 Loss = 5.2396\n",
      "2025-05-07 12:11:38,480 - INFO - Epoch 8, Batch 2700: Avg100 Loss = 5.2446\n",
      "2025-05-07 12:12:13,984 - INFO - Epoch 8, Batch 2800: Avg100 Loss = 5.2529\n",
      "2025-05-07 12:12:49,559 - INFO - Epoch 8, Batch 2900: Avg100 Loss = 5.2675\n",
      "2025-05-07 12:13:25,144 - INFO - Epoch 8, Batch 3000: Avg100 Loss = 5.2636\n",
      "2025-05-07 12:14:00,698 - INFO - Epoch 8, Batch 3100: Avg100 Loss = 5.2295\n",
      "2025-05-07 12:14:37,010 - INFO - Epoch 8, Batch 3200: Avg100 Loss = 5.2094\n",
      "2025-05-07 12:15:12,618 - INFO - Epoch 8, Batch 3300: Avg100 Loss = 5.2467\n",
      "2025-05-07 12:15:48,003 - INFO - Epoch 8, Batch 3400: Avg100 Loss = 5.2852\n",
      "2025-05-07 12:16:23,564 - INFO - Epoch 8, Batch 3500: Avg100 Loss = 5.2542\n",
      "2025-05-07 12:16:59,243 - INFO - Epoch 8, Batch 3600: Avg100 Loss = 5.2311\n",
      "2025-05-07 12:17:34,730 - INFO - Epoch 8, Batch 3700: Avg100 Loss = 5.2511\n",
      "2025-05-07 12:18:11,224 - INFO - Epoch 8, Batch 3800: Avg100 Loss = 5.2872\n",
      "2025-05-07 12:18:46,734 - INFO - Epoch 8, Batch 3900: Avg100 Loss = 5.2380\n",
      "2025-05-07 12:19:22,159 - INFO - Epoch 8, Batch 4000: Avg100 Loss = 5.2351\n",
      "2025-05-07 12:19:56,838 - INFO - Epoch 8, Batch 4100: Avg100 Loss = 5.2413\n",
      "2025-05-07 12:21:50,342 - INFO - Epoch 8 train loss: 5.2590, val ppl: 144.08\n",
      "2025-05-07 12:21:50,348 - INFO - Epoch 9/10 start (adamw+onecycle)\n",
      "2025-05-07 12:22:25,718 - INFO - Epoch 9, Batch 100: Avg100 Loss = 5.2581\n",
      "2025-05-07 12:23:00,224 - INFO - Epoch 9, Batch 200: Avg100 Loss = 5.2412\n",
      "2025-05-07 12:23:34,708 - INFO - Epoch 9, Batch 300: Avg100 Loss = 5.2364\n",
      "2025-05-07 12:24:09,178 - INFO - Epoch 9, Batch 400: Avg100 Loss = 5.2633\n",
      "2025-05-07 12:24:43,677 - INFO - Epoch 9, Batch 500: Avg100 Loss = 5.2195\n",
      "2025-05-07 12:25:18,191 - INFO - Epoch 9, Batch 600: Avg100 Loss = 5.2325\n",
      "2025-05-07 12:25:52,614 - INFO - Epoch 9, Batch 700: Avg100 Loss = 5.2224\n",
      "2025-05-07 12:26:27,964 - INFO - Epoch 9, Batch 800: Avg100 Loss = 5.2469\n",
      "2025-05-07 12:27:02,434 - INFO - Epoch 9, Batch 900: Avg100 Loss = 5.2003\n",
      "2025-05-07 12:27:36,957 - INFO - Epoch 9, Batch 1000: Avg100 Loss = 5.2230\n",
      "2025-05-07 12:28:11,519 - INFO - Epoch 9, Batch 1100: Avg100 Loss = 5.2547\n",
      "2025-05-07 12:28:45,985 - INFO - Epoch 9, Batch 1200: Avg100 Loss = 5.2491\n",
      "2025-05-07 12:29:20,545 - INFO - Epoch 9, Batch 1300: Avg100 Loss = 5.2463\n",
      "2025-05-07 12:29:55,098 - INFO - Epoch 9, Batch 1400: Avg100 Loss = 5.2235\n",
      "2025-05-07 12:30:29,588 - INFO - Epoch 9, Batch 1500: Avg100 Loss = 5.2314\n",
      "2025-05-07 12:31:04,098 - INFO - Epoch 9, Batch 1600: Avg100 Loss = 5.2432\n",
      "2025-05-07 12:31:39,424 - INFO - Epoch 9, Batch 1700: Avg100 Loss = 5.2643\n",
      "2025-05-07 12:32:14,006 - INFO - Epoch 9, Batch 1800: Avg100 Loss = 5.2406\n",
      "2025-05-07 12:32:48,564 - INFO - Epoch 9, Batch 1900: Avg100 Loss = 5.2113\n",
      "2025-05-07 12:33:23,063 - INFO - Epoch 9, Batch 2000: Avg100 Loss = 5.2169\n",
      "2025-05-07 12:33:57,590 - INFO - Epoch 9, Batch 2100: Avg100 Loss = 5.2601\n",
      "2025-05-07 12:34:32,076 - INFO - Epoch 9, Batch 2200: Avg100 Loss = 5.2348\n",
      "2025-05-07 12:35:06,613 - INFO - Epoch 9, Batch 2300: Avg100 Loss = 5.2321\n",
      "2025-05-07 12:35:41,119 - INFO - Epoch 9, Batch 2400: Avg100 Loss = 5.2368\n",
      "2025-05-07 12:36:16,393 - INFO - Epoch 9, Batch 2500: Avg100 Loss = 5.2264\n",
      "2025-05-07 12:36:50,904 - INFO - Epoch 9, Batch 2600: Avg100 Loss = 5.2413\n",
      "2025-05-07 12:37:25,367 - INFO - Epoch 9, Batch 2700: Avg100 Loss = 5.2404\n",
      "2025-05-07 12:37:59,950 - INFO - Epoch 9, Batch 2800: Avg100 Loss = 5.2615\n",
      "2025-05-07 12:38:34,498 - INFO - Epoch 9, Batch 2900: Avg100 Loss = 5.2727\n",
      "2025-05-07 12:39:09,035 - INFO - Epoch 9, Batch 3000: Avg100 Loss = 5.2392\n",
      "2025-05-07 12:39:43,491 - INFO - Epoch 9, Batch 3100: Avg100 Loss = 5.2641\n",
      "2025-05-07 12:40:17,983 - INFO - Epoch 9, Batch 3200: Avg100 Loss = 5.2586\n",
      "2025-05-07 12:40:53,364 - INFO - Epoch 9, Batch 3300: Avg100 Loss = 5.2483\n",
      "2025-05-07 12:41:27,943 - INFO - Epoch 9, Batch 3400: Avg100 Loss = 5.2169\n",
      "2025-05-07 12:42:02,454 - INFO - Epoch 9, Batch 3500: Avg100 Loss = 5.2269\n",
      "2025-05-07 12:42:36,988 - INFO - Epoch 9, Batch 3600: Avg100 Loss = 5.2347\n",
      "2025-05-07 12:43:11,557 - INFO - Epoch 9, Batch 3700: Avg100 Loss = 5.2360\n",
      "2025-05-07 12:43:45,973 - INFO - Epoch 9, Batch 3800: Avg100 Loss = 5.2507\n",
      "2025-05-07 12:44:20,538 - INFO - Epoch 9, Batch 3900: Avg100 Loss = 5.2493\n",
      "2025-05-07 12:44:55,113 - INFO - Epoch 9, Batch 4000: Avg100 Loss = 5.2357\n",
      "2025-05-07 12:45:30,388 - INFO - Epoch 9, Batch 4100: Avg100 Loss = 5.2256\n",
      "2025-05-07 12:47:23,961 - INFO - Epoch 9 train loss: 5.2391, val ppl: 142.33\n",
      "2025-05-07 12:47:23,961 - INFO - Epoch 10/10 start (adamw+onecycle)\n",
      "2025-05-07 12:47:58,516 - INFO - Epoch 10, Batch 100: Avg100 Loss = 5.2160\n",
      "2025-05-07 12:48:33,048 - INFO - Epoch 10, Batch 200: Avg100 Loss = 5.2016\n",
      "2025-05-07 12:49:07,586 - INFO - Epoch 10, Batch 300: Avg100 Loss = 5.2645\n",
      "2025-05-07 12:49:42,117 - INFO - Epoch 10, Batch 400: Avg100 Loss = 5.2500\n",
      "2025-05-07 12:50:16,671 - INFO - Epoch 10, Batch 500: Avg100 Loss = 5.2302\n",
      "2025-05-07 12:50:52,035 - INFO - Epoch 10, Batch 600: Avg100 Loss = 5.2509\n",
      "2025-05-07 12:51:26,570 - INFO - Epoch 10, Batch 700: Avg100 Loss = 5.2467\n",
      "2025-05-07 12:52:01,022 - INFO - Epoch 10, Batch 800: Avg100 Loss = 5.2222\n",
      "2025-05-07 12:52:35,525 - INFO - Epoch 10, Batch 900: Avg100 Loss = 5.2380\n",
      "2025-05-07 12:53:09,990 - INFO - Epoch 10, Batch 1000: Avg100 Loss = 5.2423\n",
      "2025-05-07 12:53:44,485 - INFO - Epoch 10, Batch 1100: Avg100 Loss = 5.2459\n",
      "2025-05-07 12:54:18,971 - INFO - Epoch 10, Batch 1200: Avg100 Loss = 5.2242\n",
      "2025-05-07 12:54:53,341 - INFO - Epoch 10, Batch 1300: Avg100 Loss = 5.2308\n",
      "2025-05-07 12:55:28,546 - INFO - Epoch 10, Batch 1400: Avg100 Loss = 5.2548\n",
      "2025-05-07 12:56:02,993 - INFO - Epoch 10, Batch 1500: Avg100 Loss = 5.2410\n",
      "2025-05-07 12:56:37,452 - INFO - Epoch 10, Batch 1600: Avg100 Loss = 5.2540\n",
      "2025-05-07 12:57:11,882 - INFO - Epoch 10, Batch 1700: Avg100 Loss = 5.2143\n",
      "2025-05-07 12:57:46,303 - INFO - Epoch 10, Batch 1800: Avg100 Loss = 5.2464\n",
      "2025-05-07 12:58:20,836 - INFO - Epoch 10, Batch 1900: Avg100 Loss = 5.2267\n",
      "2025-05-07 12:58:56,185 - INFO - Epoch 10, Batch 2000: Avg100 Loss = 5.2339\n",
      "2025-05-07 12:59:31,659 - INFO - Epoch 10, Batch 2100: Avg100 Loss = 5.2551\n",
      "2025-05-07 13:00:08,056 - INFO - Epoch 10, Batch 2200: Avg100 Loss = 5.2245\n",
      "2025-05-07 13:00:43,672 - INFO - Epoch 10, Batch 2300: Avg100 Loss = 5.2287\n",
      "2025-05-07 13:01:19,155 - INFO - Epoch 10, Batch 2400: Avg100 Loss = 5.2238\n",
      "2025-05-07 13:01:54,786 - INFO - Epoch 10, Batch 2500: Avg100 Loss = 5.2728\n",
      "2025-05-07 13:02:30,405 - INFO - Epoch 10, Batch 2600: Avg100 Loss = 5.2155\n",
      "2025-05-07 13:03:05,983 - INFO - Epoch 10, Batch 2700: Avg100 Loss = 5.1987\n",
      "2025-05-07 13:03:41,619 - INFO - Epoch 10, Batch 2800: Avg100 Loss = 5.2255\n",
      "2025-05-07 13:04:17,150 - INFO - Epoch 10, Batch 2900: Avg100 Loss = 5.2199\n",
      "2025-05-07 13:04:53,488 - INFO - Epoch 10, Batch 3000: Avg100 Loss = 5.2477\n",
      "2025-05-07 13:05:29,017 - INFO - Epoch 10, Batch 3100: Avg100 Loss = 5.2000\n",
      "2025-05-07 13:06:04,599 - INFO - Epoch 10, Batch 3200: Avg100 Loss = 5.2293\n",
      "2025-05-07 13:06:40,139 - INFO - Epoch 10, Batch 3300: Avg100 Loss = 5.2351\n",
      "2025-05-07 13:07:15,686 - INFO - Epoch 10, Batch 3400: Avg100 Loss = 5.2165\n",
      "2025-05-07 13:07:50,548 - INFO - Epoch 10, Batch 3500: Avg100 Loss = 5.2095\n",
      "2025-05-07 13:08:25,080 - INFO - Epoch 10, Batch 3600: Avg100 Loss = 5.2183\n",
      "2025-05-07 13:08:59,655 - INFO - Epoch 10, Batch 3700: Avg100 Loss = 5.2308\n",
      "2025-05-07 13:09:34,204 - INFO - Epoch 10, Batch 3800: Avg100 Loss = 5.2198\n",
      "2025-05-07 13:10:09,691 - INFO - Epoch 10, Batch 3900: Avg100 Loss = 5.2264\n",
      "2025-05-07 13:10:44,215 - INFO - Epoch 10, Batch 4000: Avg100 Loss = 5.2270\n",
      "2025-05-07 13:11:18,742 - INFO - Epoch 10, Batch 4100: Avg100 Loss = 5.2461\n",
      "2025-05-07 13:13:12,536 - INFO - Epoch 10 train loss: 5.2321, val ppl: 142.07\n"
     ]
    }
   ],
   "source": [
    "# Train model - embeddings will be fine-tuned\n",
    "val_df = tokenized_df_train.sample(frac=0.1)\n",
    "loss_values = model.fit(\n",
    "  tokenized_df_train,\n",
    "  val_df=val_df,\n",
    "  epochs=10,\n",
    "  device=device,\n",
    "  optimizer_type='adamw',\n",
    "  weight_decay=1e-2,\n",
    "  scheduler_type='onecycle'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "save_model(model, \"model_cache/hybrid_ffnn_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "\n",
    "Looking at the value of the loss it seems like we are hitting local minimum it doest really go below 5 when trained for 5 epochs\n",
    "\n",
    "the difference of loss from the different epochs are not big which suggests again perhaps local minimum are being hit.\n",
    "\n",
    "I need to try to get out of this local minimum that is being hit\n",
    "\n",
    "looking at this article \n",
    "https://mohitmishra786687.medium.com/the-curse-of-local-minima-how-to-escape-and-find-the-global-minimum-fdabceb2cd6a\n",
    "\n",
    "SGD might be interesting to get out of a local minimum or any momentum based optimizers\n",
    "\n",
    "and learning rate schedlues\n",
    "\n",
    "\n",
    "https://medium.com/@shazanansar/escaping-the-trap-of-local-minima-why-optimization-algorithms-get-stuck-and-how-to-break-free-6e3042a6d9c9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import math\n",
    "class BenchmarkModel:\n",
    "    def __init__(self, test_df, model_to_test):\n",
    "        \"\"\"Initialize with test DataFrame and a trained model.\"\"\"\n",
    "        self.test_df = test_df\n",
    "        self.model = model_to_test.to(next(model_to_test.parameters()).device)\n",
    "        self.context_size = self.model.context_size\n",
    "        self.vocab = self.model.vocab\n",
    "        self.rev_vocab = {i: t for t, i in self.vocab.items()}\n",
    "\n",
    "    def predict_job_description(self, context_window, max_length=50, temperature=1.0):\n",
    "        \"\"\"Generate a sequence autoregressively from a starting context.\"\"\"\n",
    "        self.model.eval()\n",
    "        device = next(self.model.parameters()).device\n",
    "        tokens = context_window.copy()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            idxs = torch.LongTensor([\n",
    "                self.vocab.get(t, self.vocab.get(\"__UNK__\")) for t in tokens[-self.context_size:]\n",
    "            ]).unsqueeze(0).to(device)\n",
    "            \n",
    "            logits = self.model(idxs).squeeze(0) / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Set probability of UNK token to 0\n",
    "            unk_idx = self.vocab.get(\"__UNK__\", -1)\n",
    "            if unk_idx >= 0:\n",
    "                probs[unk_idx] = 0\n",
    "                # Renormalize if needed\n",
    "                if probs.sum() > 0:\n",
    "                    probs = probs / probs.sum()\n",
    "                else:\n",
    "                    # If all probs are zero, use uniform distribution\n",
    "                    probs = torch.ones_like(probs) / len(probs)\n",
    "            \n",
    "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_tok = self.rev_vocab.get(next_idx, \"<unk>\")\n",
    "            tokens.append(next_tok)\n",
    "            \n",
    "            # Optional: stop if end token is generated\n",
    "            if next_tok == \"__END__\" or next_tok == \"<END>\":\n",
    "                break\n",
    "                \n",
    "        return tokens\n",
    "\n",
    "    def perplexity(self):\n",
    "        \"\"\"Compute perplexity on the test set.\"\"\"\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        dataset = ContextWindowDataset(self.test_df, self.vocab, self.context_size)\n",
    "        \n",
    "        # Modified collate function to handle the dataset output correctly\n",
    "        def collate_fn(batch):\n",
    "            contexts = torch.stack([x[0] for x in batch])  # Get context_idxs\n",
    "            targets = torch.LongTensor([x[1] for x in batch])  # Get target_idx\n",
    "            return contexts, targets\n",
    "        \n",
    "        loader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=32, \n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        total_loss, count = 0.0, 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in loader:\n",
    "                inputs = inputs.to(next(self.model.parameters()).device)\n",
    "                targets = targets.to(next(self.model.parameters()).device)\n",
    "                logits = self.model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "                \n",
    "        avg_loss = total_loss / count if count else float('nan')\n",
    "        return math.exp(avg_loss)\n",
    "\n",
    "    def bleu_score(self):\n",
    "        \"\"\"Compute corpus BLEU over next-token predictions.\"\"\"\n",
    "        dataset = ContextWindowDataset(self.test_df, self.vocab, self.context_size)\n",
    "        refs, hyps = [], []\n",
    "        for ctx_idxs, tgt_idx in dataset:  # Only unpack the two values that are actually returned\n",
    "            pred_idx = self.model(ctx_idxs.unsqueeze(0).to(next(self.model.parameters()).device)).argmax(dim=-1).item()\n",
    "            pred_token = self.rev_vocab.get(pred_idx, \"<unk>\")\n",
    "            target_token = self.rev_vocab.get(tgt_idx, \"<unk>\")\n",
    "            hyps.append([pred_token])\n",
    "            refs.append([[target_token]])\n",
    "        return corpus_bleu(refs, hyps)\n",
    "\n",
    "    def rouge_score(self):\n",
    "        \"\"\"Compute average ROUGE-1 and ROUGE-L F1 over single-token predictions.\"\"\"\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)\n",
    "        total1, totalL, count = 0.0, 0.0, 0\n",
    "        dataset = ContextWindowDataset(self.test_df, self.vocab, self.context_size)\n",
    "        for ctx_idxs, tgt_idx in dataset:  # Only unpack the two values that are actually returned\n",
    "            pred_idx = self.model(ctx_idxs.unsqueeze(0).to(next(self.model.parameters()).device)).argmax(dim=-1).item()\n",
    "            pred_token = self.rev_vocab.get(pred_idx, \"<unk>\")\n",
    "            target_token = self.rev_vocab.get(tgt_idx, \"<unk>\")\n",
    "            scores = scorer.score(target_token, pred_token)\n",
    "            total1 += scores['rouge1'].fmeasure\n",
    "            totalL += scores['rougeL'].fmeasure\n",
    "            count += 1\n",
    "        return {'rouge1': total1/count if count else 0.0,\n",
    "                'rougeL': totalL/count if count else 0.0}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model = BenchmarkModel(tokenized_df_train, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "perplexity = benchmark_model.perplexity()\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  147.41035956339098\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity: \", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Engineer Internship ( Remote ) ( Remote ) Hybrid ) - Day Shift I - $ - $ 32-52 per hour - $ 18.00 $ per hour shift schedule , shift , weekends , holidays 9 pay period 2023 us . expertise , including limited , needed , manage firm business projects \n"
     ]
    }
   ],
   "source": [
    "generated_job_description = benchmark_model.predict_job_description([\"Software\", \"Engineer\", \"Internship\"], 50, 0.5)\n",
    "for token in generated_job_description:\n",
    "    print(token, end=\" \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Engineer Internship ( Industrial ) - REMOTE - Part Time - Full Time - $ 32-58 per hour - $ 20.00 $ per hour shift day day operations , including limited limited , limited , individual must able perform essential functions job . reasonable accommodations may made enable individuals disabilities perform essential \n"
     ]
    }
   ],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim, vocab, context_size=3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_dim)\n",
    "        )\n",
    "        # Add required attributes\n",
    "        self.vocab = vocab\n",
    "        self.context_size = context_size\n",
    "        self.rev_vocab = {i: t for t, i in vocab.items()}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create model with required attributes\n",
    "simple_model = SimpleModel(\n",
    "    input_dim=384, \n",
    "    hidden_size=1024, \n",
    "    output_dim=len(tokenized_vocab),\n",
    "    vocab=tokenized_vocab,\n",
    "    context_size=3  # Set to match your training\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "state_dict = torch.load(\"model_cache/network_deep_lr.pth\")\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_key = f\"model.{k}\"\n",
    "    new_state_dict[new_key] = v\n",
    "\n",
    "simple_model.load_state_dict(new_state_dict)\n",
    "benchmark_model_deep = BenchmarkModel(tokenized_df_train, simple_model)\n",
    "\n",
    "generated_job_description = benchmark_model.predict_job_description([\"Software\", \"Engineer\", \"Internship\"], 50, 0.5)\n",
    "for token in generated_job_description:\n",
    "    print(token, end=\" \")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m bleu_score = \u001b[43mbenchmark_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbleu_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m rouge_score = benchmark_model.rouge_score()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPerplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperplexity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mBenchmarkModel.bleu_score\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     63\u001b[39m refs, hyps = [], []\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ctx_idxs, tgt_idx \u001b[38;5;129;01min\u001b[39;00m dataset:  \u001b[38;5;66;03m# Only unpack the two values that are actually returned\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     pred_idx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx_idxs\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.item()\n\u001b[32m     66\u001b[39m     pred_token = \u001b[38;5;28mself\u001b[39m.rev_vocab.get(pred_idx, \u001b[33m\"\u001b[39m\u001b[33m<unk>\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m     target_token = \u001b[38;5;28mself\u001b[39m.rev_vocab.get(tgt_idx, \u001b[33m\"\u001b[39m\u001b[33m<unk>\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "bleu_score = benchmark_model.bleu_score()\n",
    "rouge_score = benchmark_model.rouge_score()\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "print(f\"ROUGE Score: {rouge_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REport\n",
    "\n",
    "After this attempt with a momentum based optimizer and a LR scheduler it still seems like we are hitting some sort of plateau this could just mean the FFNN are not very good at the task that is next token prediction the RNN might be a better approach\n",
    "\n",
    "\n",
    "A very counter intuitive thing is happening the architecture in which we start with pretrained  embeddings from the transformer is not performing as well with a model that doesnt fine tune anything but sticks to the static semantices generated by the transformer\n",
    "\n",
    "it looks like fine tuning on top of the rich semantics from the transformer is making the model deteriorate the semantics ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-linkedin-offers-oiOM3zqO-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
