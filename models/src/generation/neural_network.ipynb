{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "\n",
    "THis notebook will contain the Neural Network taht will do the text generation\n",
    "\n",
    "I will have a FFNN generate job descriptions, will I do so:\n",
    "\n",
    "1- First part is the tokenization part take the jb description and have the tokens\n",
    "2- Create my vocabulary:\n",
    "        - First I will try with every single toke,\n",
    "        - SecondI will try with only the top N most frequent token\n",
    "3- For the token representation I will use a Enbeddings, pre-trained embeddings wit h word2Vec\n",
    "4- How will I train the FFNN ?\n",
    "   - I will create Input, Output pairs\n",
    "        - The Input will be a context window of fixed size N and a Special token <START> and the output is the actual next token\n",
    "        it will be as followed\n",
    "        | Input (context window, N=3) | Output (next token) |\n",
    "        |--------------------------------------|---------------------|\n",
    "        | [\"<START>\", \"Analyze\", \"data\"]       |     \"and\"           |\n",
    "    \n",
    "\n",
    "\n",
    "Perhaps try dropping job descriptions that are over the thrshold to keep the repartition as gaussean bell (Normal law distribuion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../..')) # Adjust '..' if your notebook is deeper\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources checked/downloaded.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt' tokenizer...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'stopwords'...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Add this check and download for punkt_tab\n",
    "try:\n",
    "    # Check for the specific English directory within punkt_tab\n",
    "    nltk.data.find('tokenizers/punkt_tab/english/') \n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt_tab'...\")\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "print(\"NLTK resources checked/downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cedric/.cache/pypoetry/virtualenvs/nlp-linkedin-offers-oiOM3zqO-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/cedric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/cedric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 09:16:47.007320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746170207.047568   65226 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746170207.059654   65226 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746170207.145112   65226 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746170207.145121   65226 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746170207.145123   65226 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746170207.145124   65226 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-02 09:16:47.155880: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-02 09:16:49,372 - INFO - Use pytorch device_name: cpu\n",
      "2025-05-02 09:16:49,373 - INFO - Load pretrained SentenceTransformer: multi-qa-MiniLM-L6-cos-v1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from models.src.preprocessing.tokenizer import tokenize\n",
    "from data.process_data_modeling import get_processed_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import logging\n",
    "import nltk\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "embedding_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cedric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/cedric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 09:16:50,609 - INFO - Importing data from Kaggle\n",
      "2025-05-02 09:16:50,610 - INFO - Path to dataset files: /home/cedric/.cache/kagglehub/datasets/arshkon/linkedin-job-postings/versions/13\n",
      "2025-05-02 09:16:50,611 - INFO - List of files in the dataset: ['companies', 'postings.csv', 'mappings', 'jobs']\n",
      "2025-05-02 09:16:53,702 - INFO - Rows with at least one NaN value: 1725\n",
      "2025-05-02 09:16:53,703 - INFO - Number of rows before dropping NaN values: 123849\n",
      "2025-05-02 09:16:53,724 - INFO - Number of rows after dropping NaN values: 122124\n",
      "2025-05-02 09:17:35,933 - INFO - DataFrame saved to: /home/cedric/Desktop/MAJEUR/NLP/NLP_Linkedin_offers/models/src/generation/data/processed/cleaned_postings_modeling.parquet\n",
      "2025-05-02 09:17:35,933 - INFO - Data processing completed successfully !\n",
      "2025-05-02 09:17:35,934 - INFO - Returning the processed DataFrame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Coordinator</td>\n",
       "      <td>job description a leading real estate firm in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assitant Restaurant Manager</td>\n",
       "      <td>the national exemplar is accepting application...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Elder Law / Trusts and Estates Associat...</td>\n",
       "      <td>senior associate attorney elder law trusts and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Economic Development and Planning Intern</td>\n",
       "      <td>job summary the economic development planning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Producer</td>\n",
       "      <td>company description raw cereal is a creative d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                              Marketing Coordinator   \n",
       "2                        Assitant Restaurant Manager   \n",
       "3  Senior Elder Law / Trusts and Estates Associat...   \n",
       "5           Economic Development and Planning Intern   \n",
       "6                                           Producer   \n",
       "\n",
       "                                         description  \n",
       "0  job description a leading real estate firm in ...  \n",
       "2  the national exemplar is accepting application...  \n",
       "3  senior associate attorney elder law trusts and...  \n",
       "5  job summary the economic development planning ...  \n",
       "6  company description raw cereal is a creative d...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_processed_data()\n",
    "df.head()\n",
    "df.drop(columns=[\"company_name\", \"location\"], inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Coordinator &lt;START&gt; job description ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assitant Restaurant Manager &lt;START&gt; the nation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Elder Law / Trusts and Estates Associat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Economic Development and Planning Intern &lt;STAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Producer &lt;START&gt; company description raw cerea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              merged\n",
       "0  Marketing Coordinator <START> job description ...\n",
       "2  Assitant Restaurant Manager <START> the nation...\n",
       "3  Senior Elder Law / Trusts and Estates Associat...\n",
       "5  Economic Development and Planning Intern <STAR...\n",
       "6  Producer <START> company description raw cerea..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"merged\"] = df['title'] + \" <START> \" + df['description'] + \" <END>\"\n",
    "df.drop(columns=[\"title\", \"description\"], inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88431</th>\n",
       "      <td>Big Spring Market - Overnight Perishables Rep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94806</th>\n",
       "      <td>Front Desk Guest Services Representative &lt;STAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67484</th>\n",
       "      <td>AVP Head of National Markets &lt;START&gt; job descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29134</th>\n",
       "      <td>Air Export Operations Specialist &lt;START&gt; well ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24667</th>\n",
       "      <td>Outside Sales Representative - Medical Equipme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  merged\n",
       "88431  Big Spring Market - Overnight Perishables Rep ...\n",
       "94806  Front Desk Guest Services Representative <STAR...\n",
       "67484  AVP Head of National Markets <START> job descr...\n",
       "29134  Air Export Operations Specialist <START> well ...\n",
       "24667  Outside Sales Representative - Medical Equipme..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(df:pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Get the vocabulary from the dataframe and the size of the vocabulary\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for _, row in df.iterrows():\n",
    "        merged_string = row[\"merged\"]\n",
    "        tokens = merged_string.split(\" \")\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab, len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, df, vocab, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the FeedForwardNN\n",
    "        \"\"\"\n",
    "        logging.info(f\"Initializing FeedForwardNN with input_dim: {input_dim}, hidden_dim: {hidden_dim}, output_dim: {output_dim}\")\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.last_window_tokens_index = 0\n",
    "        self.embedding_transformer = embedding_model\n",
    "        self.model = self.init_model()\n",
    "        self.N = 4\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def _get_vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of the vocabulary\n",
    "        \"\"\"\n",
    "        if self.vocab is None:\n",
    "            raise ValueError(\"Vocabulary is not initialized\")\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def _get_context_windows_batched(self, training_set: pd.DataFrame, N: int, batch_size: int):\n",
    "        \"\"\"\n",
    "        Generator that yields batches of (context_window, next_token) pairs.\n",
    "        \"\"\"\n",
    "        logging.info(f\"..... Creating context windows .....\")\n",
    "        if len(training_set) < N:\n",
    "            raise ValueError(f\"Window size {N} is larger than the number of tokens {len(training_set)}\")\n",
    "        \n",
    "        batch_context_windows = []\n",
    "        for _, row in training_set.iterrows():\n",
    "            merged_string = row[\"merged\"]\n",
    "            tokens = merged_string.split(\" \")\n",
    "            for i in range(len(tokens) - N):\n",
    "                context_window = tokens[i:i+N]\n",
    "                next_token = tokens[i+N]\n",
    "                batch_context_windows.append((context_window, next_token))\n",
    "                if len(batch_context_windows) == batch_size:\n",
    "                    yield batch_context_windows\n",
    "                    batch_context_windows = []\n",
    "        if batch_context_windows:\n",
    "            yield batch_context_windows\n",
    "\n",
    "        logging.info(f\"..... Finished yielding context windows .....\")\n",
    "\n",
    "            \n",
    "    def init_model(self):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def _embed_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Embed the tokens using the pre-trained embeddings\n",
    "        \"\"\"\n",
    "        sentence_ = \" \".join(tokens)\n",
    "        return self.embedding_transformer.encode(sentence_, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "\n",
    "    def fit(self, training_set: pd.DataFrame, batch_size=32, epochs=10, device='cpu'):\n",
    "        \"\"\"\n",
    "        Train the FFNN using transformer embeddings as input, with batch context window generation for memory efficiency.\n",
    "        \"\"\"\n",
    "        logging.info(f\"..... Training started .....\")\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            logging.info(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            batch_gen = self._get_context_windows_batched(training_set, self.N, batch_size)\n",
    "            batch_idx = 0\n",
    "            for batch in batch_gen:\n",
    "                context_windows = [\" \".join(window) for window, _ in batch]\n",
    "                next_tokens = [token for _, token in batch]\n",
    "                y = torch.tensor([self.vocab.get(token, self.vocab.get('<UNK>', 0)) for token in next_tokens], dtype=torch.long, device=device)\n",
    "                X = self.embedding_transformer.encode(context_windows, batch_size=batch_size, show_progress_bar=False)\n",
    "                X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(X)\n",
    "                loss = loss_fn(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_idx += 1\n",
    "                if batch_idx % 10 == 0:\n",
    "                    logging.info(f\"Epoch {epoch+1}, Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "            logging.info(f\"Epoch {epoch+1} completed.\")\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "    def _generation_loop(self, context_window, max_length, temperature):\n",
    "        \"\"\"\n",
    "        Generate a job description using the model\n",
    "        \"\"\"\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 09:17:50,326 - INFO - Initializing FeedForwardNN with input_dim: 384, hidden_dim: 1024, output_dim: 457772\n",
      "2025-05-02 09:17:51,749 - INFO - ..... Training started .....\n",
      "2025-05-02 09:17:51,750 - INFO - Epoch 1/1\n",
      "2025-05-02 09:17:51,750 - INFO - ..... Creating context windows .....\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_train_sample = df_train.sample(n=1000, random_state=42)\n",
    "vocab, vocab_size = get_vocab(df_train_sample)\n",
    "network = FeedForwardNN(df_train_sample, vocab, embedding_dim, 1024, vocab_size)\n",
    "network.fit(df_train_sample, batch_size=8, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-linkedin-offers-oiOM3zqO-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
