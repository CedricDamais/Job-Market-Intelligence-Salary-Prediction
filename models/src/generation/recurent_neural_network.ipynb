{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "\n",
    "I wil try to reach a base line model first using an LSTM basic implemenation with the same data processing done on the Feed Forward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../..')) # Adjust '..' if your notebook is deeper\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cerdricdamais/Desktop/EPITA/MAJEUR/NLP-1/NLP_Linkedin_offers/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cerdricdamais/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cerdricdamais/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 20:59:44,040 - INFO - Use pytorch device_name: mps\n",
      "2025-05-07 20:59:44,041 - INFO - Load pretrained SentenceTransformer: multi-qa-MiniLM-L6-cos-v1\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-05-07 20:59:45,181 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from models.src.preprocessing.tokenizer import tokenize_data_frame\n",
    "from data.process_data_modeling import get_processed_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import logging\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "embedding_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "from torch.utils.data import IterableDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources checked/downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cerdricdamais/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cerdricdamais/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt' tokenizer...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'stopwords'...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Add this check and download for punkt_tab\n",
    "try:\n",
    "    # Check for the specific English directory within punkt_tab\n",
    "    nltk.data.find('tokenizers/punkt_tab/english/') \n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt_tab'...\")\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "print(\"NLTK resources checked/downloaded.\")\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 21:02:07,597 - INFO - Importing data from Kaggle\n",
      "2025-05-07 21:02:07,599 - INFO - Path to dataset files: /Users/cerdricdamais/.cache/kagglehub/datasets/arshkon/linkedin-job-postings/versions/13\n",
      "2025-05-07 21:02:07,600 - INFO - List of files in the dataset: ['postings.csv', 'mappings', 'jobs', 'companies']\n",
      "2025-05-07 21:02:11,739 - INFO - Rows with at least one NaN value: 1725\n",
      "2025-05-07 21:02:11,741 - INFO - Number of rows before dropping NaN values: 123849\n",
      "2025-05-07 21:02:11,772 - INFO - Number of rows after dropping NaN values: 122124\n",
      "2025-05-07 21:03:01,598 - INFO - DataFrame saved to: /Users/cerdricdamais/Desktop/EPITA/MAJEUR/NLP-1/NLP_Linkedin_offers/models/src/generation/data/processed/cleaned_postings_modeling.parquet\n",
      "2025-05-07 21:03:01,603 - INFO - Data processing completed successfully !\n",
      "2025-05-07 21:03:01,603 - INFO - Returning the processed DataFrame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Coordinator</td>\n",
       "      <td>job description a leading real estate firm in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assitant Restaurant Manager</td>\n",
       "      <td>the national exemplar is accepting application...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Elder Law / Trusts and Estates Associat...</td>\n",
       "      <td>senior associate attorney elder law trusts and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Economic Development and Planning Intern</td>\n",
       "      <td>job summary the economic development planning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Producer</td>\n",
       "      <td>company description raw cereal is a creative d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                              Marketing Coordinator   \n",
       "2                        Assitant Restaurant Manager   \n",
       "3  Senior Elder Law / Trusts and Estates Associat...   \n",
       "5           Economic Development and Planning Intern   \n",
       "6                                           Producer   \n",
       "\n",
       "                                         description  \n",
       "0  job description a leading real estate firm in ...  \n",
       "2  the national exemplar is accepting application...  \n",
       "3  senior associate attorney elder law trusts and...  \n",
       "5  job summary the economic development planning ...  \n",
       "6  company description raw cereal is a creative d...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_processed_data()\n",
    "df.head()\n",
    "df.drop(columns=[\"company_name\", \"location\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88431</th>\n",
       "      <td>Big Spring Market Overnight Perishables Rep Pa...</td>\n",
       "      <td>h e b needs energetic and motivated partners w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94806</th>\n",
       "      <td>Front Desk Guest Services Representative</td>\n",
       "      <td>driftwood hospitality management s company cul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67484</th>\n",
       "      <td>AVP Head of National Markets</td>\n",
       "      <td>job description providing for loved ones, plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29134</th>\n",
       "      <td>Air Export Operations Specialist</td>\n",
       "      <td>well know logistics company based on the nassa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24667</th>\n",
       "      <td>Outside Sales Representative Medical Equipment</td>\n",
       "      <td>as america s largest supplier of respiratory a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "88431  Big Spring Market Overnight Perishables Rep Pa...   \n",
       "94806           Front Desk Guest Services Representative   \n",
       "67484                       AVP Head of National Markets   \n",
       "29134                   Air Export Operations Specialist   \n",
       "24667     Outside Sales Representative Medical Equipment   \n",
       "\n",
       "                                             description  \n",
       "88431  h e b needs energetic and motivated partners w...  \n",
       "94806  driftwood hospitality management s company cul...  \n",
       "67484  job description providing for loved ones, plan...  \n",
       "29134  well know logistics company based on the nassa...  \n",
       "24667  as america s largest supplier of respiratory a...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Coordinator</td>\n",
       "      <td>job description a leading real estate firm in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assitant Restaurant Manager</td>\n",
       "      <td>the national exemplar is accepting application...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Elder Law Trusts and Estates Associate ...</td>\n",
       "      <td>senior associate attorney elder law trusts and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Economic Development and Planning Intern</td>\n",
       "      <td>job summary the economic development planning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Producer</td>\n",
       "      <td>company description raw cereal is a creative d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                              Marketing Coordinator   \n",
       "2                        Assitant Restaurant Manager   \n",
       "3  Senior Elder Law Trusts and Estates Associate ...   \n",
       "5           Economic Development and Planning Intern   \n",
       "6                                           Producer   \n",
       "\n",
       "                                         description  \n",
       "0  job description a leading real estate firm in ...  \n",
       "2  the national exemplar is accepting application...  \n",
       "3  senior associate attorney elder law trusts and...  \n",
       "5  job summary the economic development planning ...  \n",
       "6  company description raw cereal is a creative d...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stop_words(df, column_name='merged'):\n",
    "    \"\"\"\n",
    "    Removes predefined unwanted tokens from a specified text column in the DataFrame.\n",
    "    It splits the text by whitespace, filters out tokens present in the unwanted_set,\n",
    "    and then rejoins the tokens.\n",
    "    The modification is done in-place.\n",
    "    \"\"\"\n",
    "    unwanted_token = {\"-\", \".\", \"!\", \"?\", \"...\", \"....\", \".....\", \"/\", \"|\", \"~\", \"`\", \"=\", \"+\", \"_\", \"*\"}\n",
    "\n",
    "    def clean_text_series(text_series):\n",
    "        if not isinstance(text_series, str):\n",
    "            return text_series  # Return as is if not a string (e.g., NaN, numbers)\n",
    "        \n",
    "        tokens = text_series.split()\n",
    "        \n",
    "        # Filter out tokens that are exactly in unwanted_token\n",
    "        filtered_tokens = [token for token in tokens if token not in unwanted_token]\n",
    "        \n",
    "        return \" \".join(filtered_tokens)\n",
    "    df[column_name] = df[column_name].apply(clean_text_series)\n",
    "        \n",
    "\n",
    "remove_stop_words(df, column_name=\"title\")\n",
    "remove_stop_words(df, column_name=\"description\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Configure logging if not already configured\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# --- This is the function from the previous response, kept for completeness ---\n",
    "def create_all_title_embeddings_cache(list_of_all_titles, sentence_transformer_model, cache_file_path):\n",
    "    logging.info(f\"Generating embeddings for {len(list_of_all_titles)} unique titles...\")\n",
    "    title_embeddings_array = sentence_transformer_model.encode(\n",
    "        list_of_all_titles, batch_size=32, show_progress_bar=True, convert_to_tensor=False\n",
    "    )\n",
    "    with open(cache_file_path, 'wb') as f:\n",
    "        pickle.dump(title_embeddings_array, f)\n",
    "    logging.info(f\"Title embeddings for {len(list_of_all_titles)} titles saved to {cache_file_path}\")\n",
    "    return title_embeddings_array\n",
    "\n",
    "\n",
    "class OptionBDataLoader:\n",
    "    def __init__(self, dataframe,\n",
    "                 sentence_transformer_model,\n",
    "                 unique_titles_embedding_cache_path,\n",
    "                 description_tokenizer_func,\n",
    "                 description_word_to_idx,\n",
    "                 batch_size,\n",
    "                 device='cuda'):\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "        self.sentence_transformer_model = sentence_transformer_model\n",
    "        self.unique_titles_embedding_cache_path = unique_titles_embedding_cache_path\n",
    "        self.description_tokenizer_func = description_tokenizer_func\n",
    "        \n",
    "        # Ensure your vocabulary map is passed correctly\n",
    "        self.word_to_idx = description_word_to_idx\n",
    "        self.idx_to_word = {idx: word for word, idx in description_word_to_idx.items()}\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # Fetch IDs for special tokens from YOUR vocabulary map\n",
    "        # It's crucial that your `description_word_to_idx` contains these keys.\n",
    "        self.pad_idx = self.word_to_idx['<PAD>']\n",
    "        self.start_idx = self.word_to_idx['__START__'] # Using your specified token\n",
    "        self.end_idx = self.word_to_idx['__END__']     # Using your specified token\n",
    "        self.unk_idx = self.word_to_idx['<UNK>']\n",
    "\n",
    "        # --- Title Embeddings Handling (same as before) ---\n",
    "        self.unique_titles = sorted(self.dataframe['title'].unique().tolist())\n",
    "        self.title_to_idx_map = {title: i for i, title in enumerate(self.unique_titles)}\n",
    "        try:\n",
    "            with open(self.unique_titles_embedding_cache_path, 'rb') as f:\n",
    "                self.unique_title_embeddings_array = pickle.load(f)\n",
    "            logging.info(f\"Loaded pre-computed embeddings for {len(self.unique_title_embeddings_array)} unique titles from {self.unique_titles_embedding_cache_path}\")\n",
    "            if len(self.unique_title_embeddings_array) != len(self.unique_titles):\n",
    "                logging.warning(\"Mismatch in length of cached embeddings and unique titles. Recomputing.\")\n",
    "                raise FileNotFoundError\n",
    "        except FileNotFoundError:\n",
    "            logging.info(f\"Cache not found or mismatch. Creating embeddings for {len(self.unique_titles)} unique titles...\")\n",
    "            self.unique_title_embeddings_array = create_all_title_embeddings_cache(\n",
    "                self.unique_titles, self.sentence_transformer_model, self.unique_titles_embedding_cache_path\n",
    "            )\n",
    "        self.unique_title_embeddings_tensor = torch.tensor(self.unique_title_embeddings_array, dtype=torch.float)\n",
    "        self.title_indices_for_df = [self.title_to_idx_map[title] for title in self.dataframe['title']]\n",
    "        \n",
    "        # --- Prepare all description data for iteration ---\n",
    "        self.tokenized_descriptions = []\n",
    "        for desc_text in self.dataframe['description']:\n",
    "            # Tokenize the raw description text\n",
    "            tokens_from_desc = self.description_tokenizer_func(desc_text)\n",
    "            \n",
    "            # Convert tokens to IDs, using <UNK> for out-of-vocab tokens\n",
    "            numericalized_tokens = [self.word_to_idx.get(token, self.unk_idx) for token in tokens_from_desc]\n",
    "            \n",
    "            # Add __START__ at the beginning and __END__ at the end\n",
    "            final_sequence_ids = [self.start_idx] + numericalized_tokens + [self.end_idx]\n",
    "            self.tokenized_descriptions.append(torch.tensor(final_sequence_ids, dtype=torch.long))\n",
    "\n",
    "        self.dataset_indices = list(range(len(self.dataframe)))\n",
    "\n",
    "    def _pad_sequences(self, sequences_batch, max_len=None):\n",
    "        # (Same as before - pads sequences in a batch to the same length)\n",
    "        if max_len is None:\n",
    "            max_len = max(len(seq) for seq in sequences_batch)\n",
    "        \n",
    "        padded_batch = torch.full((len(sequences_batch), max_len), self.pad_idx, dtype=torch.long)\n",
    "        for i, seq in enumerate(sequences_batch):\n",
    "            length = min(len(seq), max_len)\n",
    "            padded_batch[i, :length] = seq[:length]\n",
    "        return padded_batch\n",
    "\n",
    "    def generate_batches(self):\n",
    "        # (Same as before - generates batches of title_embeddings, decoder_inputs, decoder_targets)\n",
    "        current_order_indices = list(self.dataset_indices)\n",
    "        np.random.shuffle(current_order_indices)\n",
    "\n",
    "        for i in range(0, len(current_order_indices), self.batch_size):\n",
    "            batch_df_indices = current_order_indices[i : i + self.batch_size]\n",
    "            \n",
    "            batch_title_unique_indices = [self.title_indices_for_df[df_idx] for df_idx in batch_df_indices]\n",
    "            batch_title_embeddings = self.unique_title_embeddings_tensor[batch_title_unique_indices].to(self.device)\n",
    "            \n",
    "            batch_desc_sequences = [self.tokenized_descriptions[df_idx] for df_idx in batch_df_indices]\n",
    "            \n",
    "            # Decoder input: e.g., [__START__, w1, w2]\n",
    "            # Decoder target: e.g., [w1, w2, __END__]\n",
    "            # These are created from the full sequence: [__START__, w1, w2, __END__]\n",
    "            decoder_input_batch = self._pad_sequences([seq[:-1] for seq in batch_desc_sequences])\n",
    "            decoder_target_batch = self._pad_sequences([seq[1:] for seq in batch_desc_sequences])\n",
    "            \n",
    "            yield batch_title_embeddings, decoder_input_batch.to(self.device), decoder_target_batch.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, encoder_out_dim):\n",
    "        self.encoder_out_dim = encoder_out_dim\n",
    "        self.decoder_in_dim = encoder_out_dim\n",
    "    \n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, lr, layers) -> None:\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lr = lr\n",
    "        self.rnn = self.init_model()\n",
    "        self.decoder = DecoderLSTM()\n",
    "        self.encoder = EncoderLSTM()\n",
    "    \n",
    "    def init_model(self):\n",
    "        pass    \n",
    "\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTMWithAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModelT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_model,                 # SentenceTransformer\n",
    "                 decoder_vocab_size,              # Size of your description vocabulary\n",
    "                 embedding_dim=384,               # Dimension of Transformer embeddings\n",
    "                 decoder_embed_dim=256,           # Dimension for your decoder token embeddings\n",
    "                 hidden_dim=512,                  # LSTM hidden dimension\n",
    "                 num_layers=2,                    # Number of LSTM layers\n",
    "                 dropout_p=0.5):                  # Dropout probability\n",
    "        super(LSTMLanguageModelT, self).__init__()  # Don't forget this!\n",
    "        \n",
    "        # Store the embedding model\n",
    "        self.title_embedding_transformer = embedding_model\n",
    "        # Freeze the transformer parameters\n",
    "        for param in self.title_embedding_transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Dimensions\n",
    "        self.embedding_dim = embedding_dim        # Dimension from the transformer\n",
    "        self.decoder_embed_dim = decoder_embed_dim  # Dimension for description tokens\n",
    "        self.hidden_dim = hidden_dim              # LSTM hidden state size\n",
    "        self.num_layers = num_layers\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        \n",
    "        # Linear projections for initializing LSTM states from title embedding\n",
    "        self.title_to_hidden = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.title_to_cell = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Embedding layer for decoder input tokens (descriptions)\n",
    "        self.decoder_embedding = nn.Embedding(decoder_vocab_size, decoder_embed_dim)\n",
    "        \n",
    "        # LSTM for generating the description token by token\n",
    "        self.lstm = nn.LSTM(input_size=decoder_embed_dim,\n",
    "                           hidden_size=hidden_dim,\n",
    "                           num_layers=num_layers,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout_p if num_layers > 1 else 0)\n",
    "        \n",
    "        # Final output projection\n",
    "        self.output_projection = nn.Linear(hidden_dim, decoder_vocab_size)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "    def _get_title_embeddings(self, title_texts):\n",
    "        \"\"\"Get embeddings for a batch of titles using the transformer\"\"\"\n",
    "        with torch.no_grad():  # No need to track gradients for the frozen transformer\n",
    "            # The .encode() method returns numpy arrays, convert to tensors\n",
    "            title_embeddings_numpy = self.title_embedding_transformer.encode(title_texts, convert_to_tensor=False)\n",
    "            title_embeddings = torch.tensor(title_embeddings_numpy, dtype=torch.float).to(next(self.parameters()).device)\n",
    "        return title_embeddings\n",
    "    \n",
    "    def forward(self, title_texts, target_tokens=None, max_len=100):\n",
    "        \"\"\"\n",
    "        Forward pass through the model\n",
    "        \n",
    "        Args:\n",
    "            title_texts: List of job title strings\n",
    "            target_tokens: Tensor of target description tokens for teacher forcing [batch_size, seq_len]\n",
    "            max_len: Maximum generation length for inference\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Either token predictions (for inference) or scores (for training)\n",
    "        \"\"\"\n",
    "        batch_size = len(title_texts)\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        # 1. Get title embeddings\n",
    "        title_embeddings = self._get_title_embeddings(title_texts)  # [batch_size, embedding_dim]\n",
    "        \n",
    "        # 2. Project to initialize LSTM states (hidden and cell)\n",
    "        h_0 = self.title_to_hidden(title_embeddings)  # [batch_size, hidden_dim]\n",
    "        c_0 = self.title_to_cell(title_embeddings)    # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Reshape for LSTM layers [num_layers, batch_size, hidden_dim]\n",
    "        h_0 = h_0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c_0 = c_0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        \n",
    "        # 3. Run decoder (either with teacher forcing for training or step-by-step for inference)\n",
    "        if target_tokens is not None:\n",
    "            # Training mode with teacher forcing\n",
    "            # Embed the target tokens\n",
    "            embedded_targets = self.decoder_embedding(target_tokens)  # [batch_size, seq_len, decoder_embed_dim]\n",
    "            embedded_targets = self.dropout(embedded_targets)\n",
    "            \n",
    "            # Pass through LSTM\n",
    "            lstm_out, _ = self.lstm(embedded_targets, (h_0, c_0))  # [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Project to vocabulary space\n",
    "            output_logits = self.output_projection(lstm_out)  # [batch_size, seq_len, decoder_vocab_size]\n",
    "            return output_logits\n",
    "            \n",
    "        else:\n",
    "            # Inference mode - generate token by token\n",
    "            outputs = []\n",
    "            \n",
    "            # Start with START token (assuming id=1, adjust as needed)\n",
    "            current_token = torch.full((batch_size, 1), 1, dtype=torch.long, device=device)\n",
    "            \n",
    "            # Hidden states from title embedding\n",
    "            hidden = (h_0, c_0)\n",
    "            \n",
    "            # Generate tokens step by step\n",
    "            for _ in range(max_len):\n",
    "                # Embed current token\n",
    "                token_embedding = self.decoder_embedding(current_token)  # [batch_size, 1, decoder_embed_dim]\n",
    "                \n",
    "                # Get LSTM output for this step\n",
    "                lstm_out, hidden = self.lstm(token_embedding, hidden)  # lstm_out: [batch_size, 1, hidden_dim]\n",
    "                \n",
    "                # Project to vocabulary\n",
    "                logits = self.output_projection(lstm_out.squeeze(1))  # [batch_size, decoder_vocab_size]\n",
    "                \n",
    "                # Get the most likely next token\n",
    "                _, predicted = torch.max(logits, dim=1)  # [batch_size]\n",
    "                outputs.append(predicted.unsqueeze(1))\n",
    "                \n",
    "                # Use predicted token as next input\n",
    "                current_token = predicted.unsqueeze(1)\n",
    "                \n",
    "                # Optional: stop if all sequences generate END token (id=2, adjust as needed)\n",
    "                if (predicted == 2).all():\n",
    "                    break\n",
    "                    \n",
    "            # Concatenate all predictions\n",
    "            outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_len]\n",
    "            return outputs\n",
    "    \n",
    "    def train_model(self, dataloader, optimizer, criterion, num_epochs=5, clip_grad=1.0):\n",
    "        \"\"\"\n",
    "        Train the model using the provided dataloader\n",
    "        \n",
    "        Args:\n",
    "            dataloader: DataLoader yielding (title_embeddings, decoder_inputs, decoder_targets)\n",
    "            optimizer: PyTorch optimizer\n",
    "            criterion: Loss function (typically nn.CrossEntropyLoss)\n",
    "            num_epochs: Number of training epochs\n",
    "            clip_grad: Gradient clipping value\n",
    "            \n",
    "        Returns:\n",
    "            losses: List of average losses per epoch\n",
    "        \"\"\"\n",
    "        # This is the proper training loop, replacing the incomplete train method\n",
    "        self.train()  # Set model to training mode\n",
    "        \n",
    "        device = next(self.parameters()).device\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_items = 0\n",
    "            \n",
    "            for batch_idx, (title_texts, decoder_inputs, decoder_targets) in enumerate(dataloader):\n",
    "                batch_size = decoder_inputs.size(0)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self(title_texts, decoder_inputs)  # [batch_size, seq_len, vocab_size]\n",
    "                \n",
    "                # Reshape outputs and targets for the loss function\n",
    "                outputs_flat = outputs.reshape(-1, self.decoder_vocab_size)  # [batch_size*seq_len, vocab_size]\n",
    "                targets_flat = decoder_targets.reshape(-1)  # [batch_size*seq_len]\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs_flat, targets_flat)\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), clip_grad)\n",
    "                \n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track statistics\n",
    "                items = batch_size * decoder_targets.size(1)\n",
    "                epoch_loss += loss.item() * items\n",
    "                epoch_items += items\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Calculate average loss for the epoch\n",
    "            avg_loss = epoch_loss / epoch_items\n",
    "            losses.append(avg_loss)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} complete, Avg Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pruned_vocab(df, column, top_k=50000):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary of the top_k most frequent tokens.\n",
    "    \"\"\"\n",
    "    token_counter = Counter()\n",
    "    for text in df[column]:\n",
    "        tokens = text.split(\" \")\n",
    "        token_counter.update(tokens)\n",
    "\n",
    "    most_common = token_counter.most_common(top_k)\n",
    "    vocab = {token: idx for idx, (token, _) in enumerate(most_common)}\n",
    "    vocab[\"__START__\"] = len(vocab)\n",
    "    vocab[\"__END__\"] = len(vocab)\n",
    "    vocab['<UNK>'] = len(vocab)\n",
    "    vocab['<PAD>'] = len(vocab)\n",
    "    return vocab, len(vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your special tokens and build vocabulary\n",
    "special_tokens = ['<PAD>', '__START__', '__END__', '<UNK>']\n",
    "# Build vocabulary from your dataset (count word frequencies, etc.)\n",
    "list_of_common_words = get_pruned_vocab(df_train, \"description\")\n",
    "word_to_idx = {token: idx for idx, token in enumerate(special_tokens + list_of_common_words)}\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "# 2. Create the dataloader (using your OptionBDataLoader class or similar)\n",
    "cache_path = \"unique_title_embeddings_lstm.pkl\"\n",
    "data_loader = OptionBDataLoader(\n",
    "    dataframe=df,\n",
    "    sentence_transformer_model=embedding_model,\n",
    "    unique_titles_embedding_cache_path=cache_path,\n",
    "    description_tokenizer_func=lambda x: x.lower().split(),  # Simple tokenizer\n",
    "    description_word_to_idx=word_to_idx,\n",
    "    batch_size=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 3. Create model instance\n",
    "model = LSTMLanguageModelT(\n",
    "    embedding_model=embedding_model,\n",
    "    decoder_vocab_size=vocab_size,\n",
    "    embedding_dim=384,  # For multi-qa-MiniLM-L6-cos-v1\n",
    "    decoder_embed_dim=256,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout_p=0.5\n",
    ").to(device)\n",
    "\n",
    "# 4. Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<PAD>'])  # Ignore padding tokens\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Train the model\n",
    "losses = model.train_model(\n",
    "    dataloader=data_loader.generate_batches(),\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "# 6. Generate a description from a title\n",
    "def generate_description(model, title, max_len=100):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model([title], target_tokens=None, max_len=max_len)\n",
    "        # Convert token IDs back to words\n",
    "        idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "        words = [idx_to_word.get(idx.item(), '<UNK>') for idx in outputs[0]]\n",
    "        # Remove any tokens after __END__\n",
    "        if '__END__' in words:\n",
    "            words = words[:words.index('__END__')]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Example: generate a description for a new title\n",
    "sample_title = \"Data Scientist\"\n",
    "generated_description = generate_description(model, sample_title)\n",
    "print(f\"Title: {sample_title}\")\n",
    "print(f\"Generated Description: {generated_description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-linkedin-offers-oiOM3zqO-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
